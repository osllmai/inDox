{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b93b1299207afd79"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T08:23:41.886334400Z",
     "start_time": "2024-05-06T08:23:41.877100700Z"
    }
   },
   "id": "ae288bea28cc20f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASHKAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASHKAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Indox import IndoxRetrievalAugmentation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T08:24:03.140890700Z",
     "start_time": "2024-05-06T08:23:41.889334600Z"
    }
   },
   "id": "6b1e15cbe842b5d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using for IndoxRetrievalAugmentation for pdf files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1554405d87959e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we initialize an instance of IndoxRetrievalAugmentation (IRA) to leverage its document processing capabilities:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b183b7df25503ed"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "IRA = IndoxRetrievalAugmentation()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T08:24:03.152395300Z",
     "start_time": "2024-05-06T08:24:03.145211900Z"
    }
   },
   "id": "704cef45a921f489"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next , let's take a look at the configuration options available for IRA:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6cb3dc207e107f2"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'clustering': {'dim': 10, 'threshold': 0.1},\n 'embedding_model': 'sbert',\n 'postgres': {'conn_string': 'postgresql+psycopg2://postgres:xxx@localhost:port/db_name'},\n 'prompts': {'document_relevancy_prompt': \"You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\\nGive a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\nProvide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\nHere is the retrieved document:\\n{document}\\nHere is the user question:\\n{question}\",\n  'summary_model': {'content': 'You are a helpful assistant. Give a detailed summary of the documentation provided'}},\n 'qa_model': {'name': 'mistral', 'temperature': 9e-05},\n 'splitter': 'raptor-text-splitter',\n 'summary_model': {'max_tokens': 100,\n  'min_len': 30,\n  'model_name': 'gpt-3.5-turbo-0125'},\n 'vector_store': 'chroma'}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRA.config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T08:24:03.161411900Z",
     "start_time": "2024-05-06T08:24:03.154727300Z"
    }
   },
   "id": "3d21dd44898babc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's what's happening:\n",
    "\n",
    "If unstructured is set to True, IRA will extract data from various document types such as PDF, text, HTML, Markdown, or LaTeX using the unstructured library.\n",
    "If unstructured is set to False, IRA expects the document to be either a PDF or a text file. Additionally, when unstructured is set to False, you have the option to add an extra clustering layer to the document processing.\n",
    "This flexibility allows users to handle a wide range of document types and tailor the processing approach based on their specific needs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba8490042ca0311a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IRA.initialize()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9161f170e518298"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "file_path = \"Benchmarking_large_language_models.pdf\" "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:36:50.240880800Z",
     "start_time": "2024-05-02T10:36:50.235372400Z"
    }
   },
   "id": "d652d88284ecf667"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 14:06:53,243 - INFO - Reading PDF for file: Benchmarking_large_language_models.pdf ...\n",
      "2024-05-02 14:06:54,255 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:06:55,475 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:06:56,678 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:06:57,982 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:06:59,175 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:00,577 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:02,188 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:03,852 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:05,483 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:07,221 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:08,804 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:10,379 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:11,901 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:13,474 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:14,973 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:16,543 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:18,090 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:19,581 - INFO - Detecting page elements ...\n",
      "2024-05-02 14:07:23,553 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:25,491 - INFO - Loading the Table agent ...\n",
      "2024-05-02 14:07:25,491 - INFO - Loading the table structure model ...\n",
      "2024-05-02 14:07:26,204 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "2024-05-02 14:07:26,506 - INFO - [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-05-02 14:07:26,521 - INFO - Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-05-02 14:07:26,641 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:28,698 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:30,751 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:32,018 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:33,845 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:35,597 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:37,281 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:39,042 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:40,960 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:42,351 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:43,724 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:47,193 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:47,594 - INFO - padding image by 20 for structure detection\n",
      "2024-05-02 14:07:48,329 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:50,426 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:52,471 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:53,726 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:54,195 - INFO - padding image by 20 for structure detection\n",
      "2024-05-02 14:07:55,026 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:57,144 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:57,642 - INFO - padding image by 20 for structure detection\n",
      "2024-05-02 14:07:58,494 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:07:59,089 - INFO - padding image by 20 for structure detection\n",
      "2024-05-02 14:07:59,884 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:08:00,455 - INFO - padding image by 20 for structure detection\n",
      "2024-05-02 14:08:01,267 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:08:02,808 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:08:03,323 - INFO - padding image by 20 for structure detection\n",
      "2024-05-02 14:08:04,188 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-05-02 14:08:04,729 - INFO - padding image by 20 for structure detection\n",
      "2024-05-02 14:08:05,496 - INFO - Processing entire page OCR with tesseract...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Chunking process\n"
     ]
    }
   ],
   "source": [
    "documents = IRA.create_chunks(file_path=file_path, unstructured=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:38:07.728696200Z",
     "start_time": "2024-05-02T10:36:50.235372400Z"
    }
   },
   "id": "d6b9675ec160dc3b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='Article Benchmarking large language models from open and closed source models to apply data annotation for free-text criteria in the healthcare\\n\\nAli Nemati1,†,‡ , Mohammad Assadi Shalmani1,†,‡ , Qiang Lu, and Jake Luo2,*\\n\\n1 Ali Nemati, nemati@uwm.edu; Tel.: +1-971-400-2132 2 Mohammad Assadi Shalmani, assadis2@uwm.edu; Tel.: +1-414-406-9052 3 Qiang Lu, luqiang@cup.edu.cn * † Current address: Affiliation. ‡\\n\\nThese authors contributed equally to this work.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='Abstract: LLMs can significantly improve data annotation for free-text healthcare records, but thorough evaluation is critical to ensure they meet strict accuracy and reliability requirements, particularly in understanding patient characteristics commonly used in clinical research. In this study, we aim to assess LLM performance in extracting real patient attributes from free-text data. We develop a novel LLM evaluation framework based on Multi-criteria Decision Analysis (MCDA) and the Order of Preference b', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='y Similarity to Ideal Solution (TOPSIS) technique, defining 10 metrics to assess annotation quality on criteria like age, gender, BMI, diseases, and white blood count and platelets. In the evaluation study, we found that top open-source and commercial LLMs achieve accuracy scores of 0.59, 1, 0.84, 0.56, and 0.92 in predicting gender, age, BMI, disease presence, and platelet count, respectively on our multi-faceted benchmark. Our work not only provides a comprehensive assessment of LLM capabilities for extra', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='cting patient characteristics from free-text, but also introduces an effective new methodology for evaluating LLMs to enhance responsible decision-making regarding their use in healthcare.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='Keywords: Large language models; Healthcare data annotation; Multi-criteria Decision Analysis; Closed-source and open-source models; Evaluation metrics; Human and LLM evaluation; Decision- making in healthcare\\n\\n0. Introduction\\n\\nCitation: Nemati, A.; Assadi Shalmani, M; Luo, J. Title. Journal Not Specified 2024, 1, 0. https://doi.org/\\n\\nReceived:\\n\\nRevised:\\n\\nAccepted:\\n\\nAccepted:', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='Published:\\n\\nCopyright: © 2024 by the authors. Submitted to Journal Not Specified for possible open access publication under the terms and conditions of the Creative Commons Attri- bution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/).', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='Large language models (LLMs) have emerged as powerful tools for natural language processing, driving innovation across various industries, including healthcare . The expan- sion of LLMs, both from open and closed source models, represents a significant shift in how data is handled, interpreted, and utilized, ushering in a new era of computational intelligence [1][2]. One area where LLMs have shown great promise is in the extraction of patient information from free-text clinical records [3]. Free-text patien', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='t characteristics descriptions found in clinical notes, discharge summaries, and medical reports constitute a significant segment of medical records and literature, offering indispensable insights for clinical decision-making, research, and education, underscoring the importance of precise information extraction for enhancing patient care, supporting clinical decisions, advancing research, and informing policy-making[4]. LLMs enable medical professionals to extract important information from lengthy and com', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='plex texts, facilitating precise and efficient data annotation [5]. This helps to uncover insights from unstructured data, which can lead to personalized medicine, faster diagnosis processes, and the automation of routine tasks such as data annotation and labeling. These capabilities not only enhance the quality of care but also streamline operations within healthcare systems, ultimately leading to better health outcomes and reduced costs [6]. However, the usage of LLMs in healthcare', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='Version April 9, 2024 submitted to Journal Not Specified\\n\\nhttps://www.mdpi.com/journal/notspecified\\n\\n17', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 1}),\n Document(page_content='presents unique challenges, particularly regarding the accuracy, reliability, and ethical use of AI in sensitive fields [7]. Evaluating the performance of LLMs in extracting patient information from free-text descriptions is critical to ensure they meet the strict requirements of healthcare applications [8]. Several studies have focused on benchmarking LLMs for various healthcare tasks, such as named entity recognition [9], relation extraction [10], and question answering [10]. However, there is a lack of c', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='omprehensive evaluation frameworks 40 specifically tailored to assess LLM performance in extracting patient characteristics from 41 free-text clinical records [11].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='Benchmarking involves comparing model performance against established standards and is essential to evaluate the effectiveness, fairness, and safety of LLMs [12]. It helps identify the most promising models for specific healthcare tasks, such as data annotation and information extraction from free-text criteria. This process ensures the responsible and beneficial deployment of LLMs in healthcare settings [10].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='As LLMs continue to develop and expand, their role in revolutionizing healthcare through improved data annotation and information extraction becomes increasingly im- portant [10]. In this transformation, benchmarking plays a crucial role by providing a framework for evaluating and choosing the most appropriate models for healthcare ap- plications [11]. By overcoming the challenges and leveraging the opportunities presented by LLMs, the healthcare industry can explore new horizons in patient care, research,', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='and operational efficiency [13].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='Hybrid Evaluation includes Human and LLMs evaluation incorporates both human oversight and automated process to assess context by initially using scripts [14] to interpret LLM-generated responses and then involving human evaluators when scripts alone do not suffice. Alongside this, our framework’s evaluation metrics include human validation, where LLM outputs are compared to human expert benchmarks on all criteria cover in our study, and computational methods such as the BERTScore methodology, which evaluat', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='es precision, F1 score, and recall by measuring the cosine similarity between LLM outputs and the ground truth. Additionally, cosine similarity is used to assess semantic alignment between outputs and ground truth, employing the Sentence Transformer library. These methods collectively ensure a comprehensive and nuanced evaluation of the LLM’s capa- bilities, from accuracy and completeness in data extraction to semantic coherence, setting a 65 robust benchmark for assessing LLM performance in extracting pati', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='ent characteristics from 66 textual data.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='In this study, we introduce a novel evaluation methodology tailored for assessing the effectiveness of large language models in extracting critical patient characteristics from unstructured clinical dataset. Leveraging the principles of Multi-criteria Decision Analysis (MCDA) and the TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) method, our framework outlines ten distinct metrics to evaluate the precision of LLMs in annotating key patient data such as age, gender, body mass inde', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='x (BMI), disease diagnoses, and relevant hematological parameters including white blood cell count and platelet levels. This refined assessment tool is applied to examine a variety of leading LLMs, encompassing both open-source and proprietary solutions, across a comprehensive selection of clinical records that span multiple medical disciplines and diverse patient demographics. Our analysis not only benchmarks the accuracy levels of 78 these models in identifying specific patient attributes—with accuracy sc', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='ores ranging from 79 0 to 1 across different criteria—but also introduces an innovative approach to evaluate LLMs in healthcare. Through our in-depth evaluation, we aim to shed light on the optimal application of LLMs in clinical settings, promoting enhanced patient care and informed decision-making in the deployment of these technologies for health data management.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='1. Methodology 1.1. Dataset\\n\\nThe data source for this study is patient characteristic descriptions, commonly known as patient eligibility criteria, which are used for screening patients for clinical research\\n\\n84\\n\\n85\\n\\n[15]. These descriptions are an essential part of clinical trial protocols and provide detailed information about the target patient population, including demographic, clinical, and laboratory parameters [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 2}),\n Document(page_content='The dataset used in this study was acquired from the Clinical Trials Transformation Initiative (CTTI) [16], a public-private partnership that aims to develop and drive the adoption of practices that will increase the quality and efficiency of clinical trials [17]. CTTI maintains a comprehensive database of clinical trial protocols, which includes patient eligibility criteria from a wide range of therapeutic areas and study designs.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='For our evaluation tests, we randomly selected a sample of patient eligibility criteria descriptions from the CTTI database. The sample size was determined based on statistical power calculations and considerations for the generalizability of the results [cite]. In total, we acquired 500 patient criteria descriptions for each of the evaluation tests, ensuring a diverse and representative dataset.\\n\\n100 We meticulously review and annotate every piece of context, then cross-check and 101', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='validate each text against specific categories for all criteria. This process is referred to as establishing the \"ground truth\" as we are confident that the results are accurate which have double-checked.\\n\\n104 The patient eligibility criteria descriptions in our dataset cover a broad spectrum 105', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='of medical conditions, including but not limited to cardiovascular diseases, oncology, neurological disorders, and infectious diseases. The descriptions also encompass various study types, such as randomized controlled trials, observational studies, and pragmatic 108 trials [cite]. This diversity in the dataset allows for a comprehensive evaluation of LLM 109 performance across different clinical domains and research settings.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='To ensure the quality and consistency of the dataset, we performed data preprocessing steps, including text cleaning, anonymization, and standardization [cite]. These steps were crucial to remove any personally identifiable information, resolve inconsistencies in the data format, and prepare the dataset for input into the LLMs [cite]. In our research, we categorize the dataset into two primary types: raw and clean data. Raw data refers to the dataset in its original form, without undergoing any preprocessin', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='g. On the other hand, clean data, as the term suggests, involves refining the text through various preprocessing steps to enhance its quality and consistency. These steps include removing stop words to reduce noise in the text, correcting encoding mismatches or corruptions (e.g., removing characters like \"ï¿½?ï¿½\"), and standardizing mathematical symbols for uniformity. For example, inconsistent representations such as \"16>=\" and \"=<16\" in different rows are unified to a single format, \"=<16\". Furthermore,', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='in addressing gender criteria, variations such as \"women,\" \"a pregnant person,\" or \"Female\" are standardized to \"female\" to maintain consistency across the dataset. Additionally, we employ stemming and lemmatization techniques to reduce the variability of tokens [18]. Stemming simplifies words to their root form, often leading to a reduction in the complexity of textual data. Lemmatization, on the other hand, involves converting words into their lemma or dictionary form. These processes are crucial for mini', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='mizing data sparsity and improving the outcomes of our analysis by ensuring that different forms of a word are interpreted as the same token. Together, these preprocessing measures are key to ensuring the data’s quality and reliability in our analysis.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='By using a large, diverse, and well-curated dataset from a reputable source like CTTI, our study aims to provide a robust and reliable evaluation of LLM performance in extracting 133 patient characteristics from free-text eligibility criteria descriptions. The insights gained 134 from this evaluation can inform the development and implementation of LLMs in clinical research and support the identification of eligible patients for clinical trials [cite].\\n\\n1.2. Designing Effective Prompts', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='Prompt design plays a crucial role in eliciting accurate and relevant responses from 138 language models [cite]. In this study, we propose a structured approach to prompt design, which we call \"Designing Effective Prompts\" (Figure 2). This methodology emphasizes the 139 140\\n\\n88\\n\\n89\\n\\n90\\n\\n91\\n\\n92\\n\\n93\\n\\n94\\n\\n95\\n\\n102\\n\\n103\\n\\n137', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 3}),\n Document(page_content='importance of three key elements: role, context, and task. While not all three elements are required in every prompt, their precise definition and thoughtful integration are essential for optimizing the performance of language models [cite].\\n\\n143 The role element defines the identity or function that the language model should 144', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 4}),\n Document(page_content='assume in a given scenario [cite]. For example, the language model could be prompted to act as a highly skilled data analyst and annotator, or as a hospital receptionist processing patient information. By specifying the role, we provide the language model with a clear understanding of the perspective it should adopt when responding to the prompt [cite].\\n\\n148 The context element provides the language model with the necessary background 149', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 4}),\n Document(page_content='information and guidelines to perform the task effectively [cite]. In the context of patient eligibility criteria extraction, the context element might include instructions on how to interpret and annotate specific patient characteristics, such as gender, age, or medical conditions. 150 151 152 153\\n\\nFor instance, the context element could specify the use of standardized markers like ’M’, ’F’, or ’ALL’ for gender, or provide guidance on assigning ’F’ when pregnancy is mentioned.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 4}),\n Document(page_content='The task element clearly outlines the specific objectives or actions the language model is expected to accomplish [cite]. In our study, the task element focuses on identifying and extracting patient characteristics relevant to eligibility criteria for clinical trials. This could include identifying patients with specific medical conditions (e.g., type 2 diabetes), demographic characteristics (e.g., age, gender), or clinical parameters (e.g., body mass index, HbA1c levels) [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 4}),\n Document(page_content='162 By combining these three elements, \"Designing Effective Prompts\" offers a structured 163\\n\\nand adaptable approach to evaluate the capabilities of language models across various tasks and scenarios [cite]. This methodology allows for the creation of targeted prompts that assess the language model’s ability to understand and extract patient characteristics from free-text eligibility criteria descriptions [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 4}),\n Document(page_content='To ensure the effectiveness and reliability of our prompts, we conducted a thorough literature review and consulted with domain experts, including clinicians, researchers, and data annotators [cite]. We also performed iterative testing and refinement of the prompts to optimize their clarity, specificity, and comprehensiveness [cite].\\n\\n145\\n\\n146\\n\\n147\\n\\n154\\n\\n155\\n\\n156\\n\\n157\\n\\n158\\n\\n159\\n\\n160\\n\\n161\\n\\n164\\n\\n165\\n\\n166\\n\\n167', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 4}),\n Document(page_content='Prompt < You are a highly skilled Al trained in data analysis and data annotation. | have a text which is about medical data. Assume you are a receptionist You have to find the following information from the text and fill the form. 1. Gender as example: it should be such as M, F, ALL, 2. It might be found pregnant or pregnancy or etc in the text, if it is found, you should fill the form as “F\" in a hospital and you have to fill a form for each patient. Role 3. Remember all above variable can be or any value', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 5}),\n Document(page_content=', replace with S Context 4. Please do it step by step. 5. Do not return anything else. Inclusion criteria: Type 2 diabetic male and female patients according to the following criteria: Based upon a complete medical history and clinical laboratory tests Age >= 18 and Age <= 65 years Body mass index >= 25 <= 35 kg/m2 HbAic <= 7.5% Treatment with metformin (=1500 mg daily) for <= 3 months Sigi accordance with GCP and the local legislation. For female patients of childbearing potential: Use of acceptable method', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 5}),\n Document(page_content='of contraception (Pearl-Index <1). Exclusion criteria: Treatment with any glucose-lowering drug except for metformin within prior 3 months. Any finding of the medical examination (including blood pressure, pulse rate and electrocardiogram) deviating from normal and of clinical relev and dated written informed consent prior to admission to the study in Task', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 5}),\n Document(page_content='Figure 1. A structured framework for prompt construction in language model benchmarking, highlighting the integration of role, context, and task elements to enhance model performance evaluation.\\n\\n1.3. LLM Evaluation Design', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 5}),\n Document(page_content='In order to assess the performance of LLMs in extracting patient characteristics from 173 free-text eligibility criteria descriptions, it is crucial to establish a comprehensive and multi- faceted evaluation framework [cite]. The evaluation of LLMs in this context is essential for several reasons: 174 175 176', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 5}),\n Document(page_content='Ensuring accuracy and reliability: Extracting patient characteristics from free-text descriptions requires a high level of accuracy to support clinical decision-making and 178 research. Evaluating LLMs helps ensure that the extracted information is reliable and 179 can be trusted for downstream applications.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 5}),\n Document(page_content='2. Assessing generalizability: LLMs should be able to perform well across a diverse range of eligibility criteria descriptions, covering various therapeutic areas, study designs, and patient populations [cite]. A comprehensive evaluation framework allows us to assess the generalizability of LLMs and identify potential limitations or biases [cite]. Facilitating model selection and improvement: By evaluating multiple LLMs using a standardized framework, we can compare their performance, identify strengths and', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 5}),\n Document(page_content='187 weaknesses, and select the most suitable models for specific tasks [cite]. Moreover, the evaluation results can guide the development and fine-tuning of LLMs to enhance their performance in extracting patient characteristics [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 5}),\n Document(page_content='172', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 5}),\n Document(page_content='To address these objectives, we propose an evaluation framework that incorporates multiple metrics, each capturing different aspects of LLM performance (Figure 2). The rationale for using multiple metrics is to provide a holistic assessment of LLMs, considering factors such as accuracy, relevance, consistency, and coherence [cite]. By combining these 194 metrics, we aim to obtain a more comprehensive understanding of LLM performance and 195 make informed decisions regarding their deployment in clinical rese', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content='arch settings [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content='1. Human Validation: This metric involves comparing the LLM output with ground 198 truth answers provided by human experts for specific criteria such as BMI, age, gender, platelet count, and white blood cell count [cite]. Human validation ensures that the LLM’s performance is benchmarked against the gold standard of human judgment [cite]. F1 Score, Precision, and Recall: These metrics, derived from the BERTScore methodol- ogy, assess the LLM’s ability to generate outputs that match the ground truth data [ci', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content='te]. By calculating the cosine similarity between the tokens of the LLM output and the ground truth, we can quantify the model’s accuracy and completeness in extracting relevant information [cite]. Cosine Similarity: Using the Sentence Transformer library and the ’all-mpnet-base-v2’ 208 model, we measure the semantic similarity between the ground truth data and LLM 209 output [cite]. Cosine similarity captures the semantic proximity of the sentences, providing insights into the LLM’s ability to generate coh', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content='erent and meaningful outputs [cite]. Factual Consistency: This metric evaluates the LLM’s ability to generate outputs that are factually consistent with the input data [cite]. Factual consistency is crucial in the clinical domain, where inaccuracies can have serious consequences for patient care and research integrity [cite]. Coherence, Consistency, Fluency, and Relevance: These qualitative metrics assess various aspects of the LLM’s output, such as logical coherence, consistency across different parts of t', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content='he output, fluency of the generated text, and relevance to the input data [cite]. These metrics provide a more nuanced evaluation of the LLM’s language generation capabilities [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content='To integrate these multiple metrics and identify the top-ranking LLMs, we employ Multi- criteria Decision Analysis (MCDA) using the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) [cite]. TOPSIS allows us to consider the relative importance of each metric and rank the LLMs based on their proximity to the ideal solution [cite]. By applying MCDA, we can make informed decisions about the most suitable LLMs for extracting patient characteristics from free-text eligibility criteria des', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content='criptions [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content=\"Evaluation Large Launage Models Metircs | \\\\ ne \\\\ oF Insprired From Insprired From Insprired From Towards a Unified BertScore DeepEval fremwork Multi-Dimensional Human Djugment Evaluator (UMDE) fi y 1 i ) I | Facual Consistency | 1 | I Fi Score | (between LLM output | Prelevance | | \\\\ Precision | cuelgresin)) pueney | Human Validation | i] Recall | Cosine ' Consistency | | | | Similarity(between | Coherance | | I | Answer and Context) | | | te Ao A A J Check and Validate Context Output From LLM Sanity Check\", metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content='Figure 2. Evaluation Large Launage Models Metircs.\\n\\n1.4. Human Validation\\n\\n228', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 6}),\n Document(page_content='To assess the performance of LLMs in extracting patient characteristics from free-text eligibility criteria descriptions, we incorporate a human validation process. This process involves comparing the LLM’s output with ground truth answers provided by human experts for specific criteria, including age, gender, BMI, platelet count, white blood cell 232 (WBC) count, and disease categories [cite]. Similar scoring systems have been employed 233 in other studies evaluating the performance of natural language pro', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 7}),\n Document(page_content='cessing models in clinical settings [cite1, cite2]. These studies have demonstrated the effectiveness of human validation in assessing the accuracy and reliability of automated information extraction from medical texts [cite3, cite4].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 7}),\n Document(page_content='For age, gender, BMI, platelet count, and WBC count, we employ a binary scoring system:\\n\\n238\\n\\n239\\n\\n1.\\n\\n2.\\n\\nScore of 1: Assigned if the LLM’s output accurately extracts the information for the specified criterion. Score of 0: Assigned if the LLM’s output fails to mention or incorrectly mentions the information for the specified criterion.\\n\\n—\\n\\n240\\n\\n241\\n\\n242\\n\\n243\\n\\nThis binary scoring system allows us to evaluate the LLM’s accuracy in extracting these key patient characteristics [cite].\\n\\n244\\n\\n245', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 7}),\n Document(page_content='For evaluating the LLM’s performance in identifying disease-related information, we consider five main disease categories: cancer, hepatitis, HIV, diabetes, and cardiovascular diseases. We use a proportional scoring system based on the number of correctly identified disease categories:\\n\\n1. 2. Maximum score of 1: Achieved if the LLM’s output accurately includes all five disease\\n\\ncategories.\\n\\n250\\n\\n251\\n\\n252', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 7}),\n Document(page_content='The proportional scoring system acknowledges the LLM’s ability to identify multiple disease categories and provides a more granular assessment of its performance [cite].\\n\\n253\\n\\n254\\n\\nTo calculate the overall human validation score for each LLM, we first compute the average score for age, gender, BMI, platelet count, and WBC count:\\n\\n255\\n\\n256\\n\\nAvg Score (Age, Gender, BMI, Platelet, WBC) = ∑n i=1 Scorei n (1)\\n\\nwhere n is the total number of criteria (i.e., 5) and Scorei is the binary score for each criterion.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 7}),\n Document(page_content='257\\n\\n258\\n\\nNext, we calculate the average score for the disease categories:\\n\\n259\\n\\n∑m\\n\\nj=1 Scorej m\\n\\nAvg Score (Disease Categories) = where m is the total number of disease categories (i.e., 5) and Scorej is the proportional\\n\\nscore for each category.\\n\\n260\\n\\n261\\n\\n262\\n\\nFinally, we compute the overall human validation score by taking the average of the two scores:\\n\\n263\\n\\n264\\n\\nHuman Validation Score = Avg Score (Age, Gender, BMI, Platelet, WBC) + Avg Score (Disease Categories) 2', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 7}),\n Document(page_content='(2) The human validation score ranges from 0 to 1, with higher scores indicating better per- formance in extracting patient characteristics from free-text eligibility criteria descriptions [cite].\\n\\n265\\n\\n266\\n\\n267', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 7}),\n Document(page_content='By using this clear and structured scoring system, we aim to provide a thorough and systematic assessment of LLM performance in the context of patient characteristic extraction. The human validation score serves as a crucial component of our evaluation framework, enabling the comparison of different LLMs and guiding their selection for clinical research applications [cite].\\n\\n268\\n\\n269\\n\\n270\\n\\n271\\n\\n272\\n\\n1.5. F1 Score , Precision and Recall', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 7}),\n Document(page_content='To evaluate the performance of LLMs in extracting patient characteristics, we employ the BERTScore methodology [cite]. BERTScore provides a comprehensive evaluation by comparing the tokens of the ground truth data (x) with the LLM’s output ( ˆx). This comparison is performed using a pre-trained BERT model, which generates contextualized embeddings for each token [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='The BERTScore calculation involves a greedy matching process to maximize the similarity score between the tokens of the ground truth and the LLM’s output [cite]. For each token in the ground truth, the most similar token in the LLM’s output is identified, and vice versa. This matching process allows for the computation of precision and recall scores.\\n\\nRecall (RBERT) measures the proportion of tokens in the ground truth that are correctly captured by the LLM’s output [cite]. It is calculated as follows:', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='RBERT = 1 |x| ∑ xi∈x max ˆxj∈ ˆx x⊤ i ˆxj (3)\\n\\nwhere xi represents a token in the ground truth, ˆxj represents a token in the LLM’s output, and x⊤ i ˆxj denotes the cosine similarity between their respective embeddings.\\n\\nPrecision (PBERT) measures the proportion of tokens in the LLM’s output that are relevant to the ground truth [cite]. It is calculated as follows:\\n\\nPBERT = 1 | ˆx| ∑ ˆxj∈ ˆx max xi ∈ xx⊤ i ˆxj (4)', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='The F1 score (FBERT) is the harmonic mean of precision and recall, providing a balanced measure of the LLM’s performance [cite]. It is calculated as follows:\\n\\nFBERT = 2 · PBERT · RBERT PBERT + RBERT (5)', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='In our study, we calculate the F1 score, precision, and recall by applying the BERTScore methodology to the LLM’s output and the corresponding ground truth data. Instead of using the original BERT model, we employ the T5 model [cite], which has been specifically designed for text-to-text tasks and has shown superior performance in various natural language processing applications [cite].\\n\\n1.6. Sementic Similarity', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='To further evaluate the semantic similarity between the ground truth data and the LLM’s output, we employ the Sentence Transformer library [cite]. Sentence Transformers are a set of pre-trained models that generate dense vector representations of sentences, capturing their semantic meaning [cite]. These models have been shown to outperform tra- ditional word embedding methods in various natural language processing tasks, including semantic textual similarity [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='In our study, we utilize the ’all-mpnet-base-v2’ model, a state-of-the-art transformer 304 model pre-trained on a large corpus of text data [cite]. This model has demonstrated 305 excellent performance in encoding sentences into semantically meaningful vectors [cite]. By leveraging the ’all-mpnet-base-v2’ model, we aim to capture the semantic proximity between the ground truth and the LLM’s output effectively.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='The process of calculating cosine similarity using Sentence Transformers involves the following steps [cite]:\\n\\n310 Encoding the ground truth and LLM output: The ’all-mpnet-base-v2’ model is used 311', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='to generate dense vector representations for each sentence in the ground truth and the corresponding LLM output. These vectors are obtained by passing the sentences through the pre-trained model, which learns to map semantically similar sentences to nearby points in the vector space [cite]. Computing cosine similarity: Once the vector representations 312 313 314 315\\n\\n273\\n\\n274\\n\\n275\\n\\n276\\n\\n277\\n\\n278\\n\\n279\\n\\n280\\n\\n281\\n\\n282\\n\\n283\\n\\n284\\n\\n285\\n\\n286\\n\\n287\\n\\n288\\n\\n289\\n\\n290\\n\\n291\\n\\n292\\n\\n293\\n\\n294\\n\\n295\\n\\n296\\n\\n297\\n\\n298\\n\\n299\\n\\n300', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='301\\n\\n302\\n\\n303\\n\\n309\\n\\nare obtained, we calculate the cosine similarity between the ground truth and LLM output vectors. Cosine similarity measures the cosine of the angle between two vectors in a multi-dimensional space [cite]. It ranges from -1 to 1, with higher values indicating greater semantic similarity. The cosine similarity between two vectors a and b is calculated as follows:\\n\\ncosine similarity = a · b |a||b| (6)', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 8}),\n Document(page_content='where a · b denotes the dot product of the vectors, and |a| and |b| represent their Euclidean norms [cite]. Interpreting cosine similarity scores: The resulting cosine similarity scores provide a quantitative measure of the semantic proximity between the ground truth and the LLM’s output. Scores closer to 1 indicate a high degree of semantic similarity, suggesting that the LLM has effectively captured the meaning and context of the ground truth [cite]. Lower scores, on the other hand, indicate a semantic di', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='vergence between the two texts.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='1.7. Factual Consistency\\n\\nFactual consistency is a crucial aspect of evaluating the performance of LLMs in extracting patient characteristics from free-text eligibility criteria descriptions [cite]. It measures the extent to which the information extracted by the LLM aligns with the factual details present in the ground truth data [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='To assess factual consistency, we employ a combination of automated and manual evaluation techniques [cite]. Automated methods involve comparing the extracted entities, such as numerical values, dates, and named entities, between the LLM’s output and the ground truth [cite]. This comparison can be performed using string matching algorithms or more advanced techniques like named entity recognition and normalization [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='Manual evaluation of factual consistency involves human experts reviewing the LLM’s output and comparing it against the ground truth [cite]. This process allows for a more nuanced assessment of the factual accuracy, taking into account the context and semantics of the extracted information [cite].\\n\\n340 The factual consistency score is calculated as the proportion of correctly extracted 341', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='factual details out of the total number of relevant facts present in the ground truth [cite]. 342 A higher factual consistency score indicates that the LLM is able to accurately extract and 343 represent the key information from the eligibility criteria descriptions [cite]. 344\\n\\nSoruce: Mohamd please consider it source: https://arxiv.org/pdf/2305.11171.pdf\\n\\n1.8. Coherence , Consistency ,Fluency and Relevance', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='In addition to the quantitative metrics discussed earlier, we also evaluate the qualita- tive aspects of the LLM’s output, including coherence, consistency, fluency, and relevance [cite]. These factors are essential for ensuring that the extracted patient characteristics are presented in a clear, understandable, and contextually appropriate manner [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='Coherence refers to the logical flow and structure of the LLM’s output [cite]. It assesses whether the extracted information is organized in a coherent and meaningful way, making it easy for readers to comprehend [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='Consistency measures the uniformity of the extracted information across different 354 parts of the LLM’s output [cite]. It ensures that the patient characteristics are represented 355 consistently throughout the generated text, without contradictions or discrepancies [cite]. Fluency evaluates the linguistic quality of the LLM’s output [cite]. It assesses whether the generated text follows the grammatical and syntactic rules of the language, resulting in smooth and natural-sounding sentences [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='Relevance gauges the extent to which the extracted patient characteristics are pertinent to the specific eligibility criteria and the overall context of the clinical study [cite]. It ensures that the LLM captures the most important and relevant information from the free-text descriptions [cite].\\n\\nTo evaluate these qualitative aspects, we employ a combination of human evaluation and automated metrics [cite]. Human experts assess the coherence, consistency, fluency, and\\n\\n316\\n\\n317\\n\\n318\\n\\n319\\n\\n320\\n\\n321\\n\\n322\\n\\n323', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='324\\n\\n325\\n\\n326\\n\\n327\\n\\n328\\n\\n329\\n\\n330\\n\\n331\\n\\n332\\n\\n333\\n\\n334\\n\\n335\\n\\n336\\n\\n345\\n\\n346\\n\\n351\\n\\n352\\n\\n353\\n\\n356\\n\\n360\\n\\n361\\n\\n362\\n\\n363\\n\\n364\\n\\n365\\n\\nrelevance of the LLM’s output using predefined rubrics or rating scales [cite]. Automated 366 methods, such as language models trained on coherence and fluency datasets, can provide complementary scores [cite].\\n\\n1.9. Multi-criteria Decision Analysis to select top rank', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 9}),\n Document(page_content='To determine the top-performing LLM for extracting patient characteristics from free- text eligibility criteria descriptions, we employ Multi-criteria Decision Analysis (MCDA) using the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) [cite]. TOPSIS is a well-established MCDA method that allows for the evaluation of alterna- tives based on multiple criteria [cite]. In our study, the alternatives are the different LLMs being evaluated, and the criteria include the various evaluation', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 10}),\n Document(page_content='metrics discussed earlier, such as human validation scores, F1 scores, cosine similarity, factual consistency, coherence, consistency, fluency, and relevance [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 10}),\n Document(page_content='The TOPSIS methodology involves the following steps [cite]:\\n\\nConstructing a decision matrix: The performance scores of each LLM across all evaluation metrics are organized into a decision matrix.\\n\\n2. Normalizing the decision matrix: The scores are normalized to ensure comparability across different criteria.\\n\\n382 3. Assigning weights to the criteria: Each evaluation metric is assigned a weight based 383', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 10}),\n Document(page_content='3. Assigning weights to the criteria: Each evaluation metric is assigned a weight based on its relative importance in assessing the overall performance of the LLMs.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 10}),\n Document(page_content='on its relative importance in assessing the overall performance of the LLMs. Identifying the Positive Ideal Solution (PIS) and Negative Ideal Solution (NIS): The PIS represents the best possible scores across all criteria, while the NIS represents the worst scores. Calculating the Euclidean distance: The distance of each LLM from the PIS and NIS is computed using the Euclidean distance formula. Computing the Relative Closeness to the Ideal Solution (RCIS): The RCIS for each LLM is calculated using the formu', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 10}),\n Document(page_content='la:', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 10}),\n Document(page_content='4.\\n\\n5.\\n\\n6.\\n\\nRCISi = D− i i + D− D+ i (7)\\n\\n7.\\n\\nwhere D+ i NIS. Ranking the LLMs: The LLMs are ranked based on their RCIS values, with the highest value indicating the best-performing model.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 10}),\n Document(page_content='By applying the TOPSIS methodology, we obtain a comprehensive evaluation of the LLMs, taking into account multiple criteria and their relative importance [cite]. This approach allows us to identify the top-performing LLM that excels across various dimensions, pro- viding a reliable and effective solution for extracting patient characteristics from free-text eligibility criteria descriptions [cite].\\n\\n2. Results\\n\\n369\\n\\n_\\n\\n370\\n\\n371\\n\\n372\\n\\n373', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 10}),\n Document(page_content='x — xs\\n\\n7\\n\\n377\\n\\n378\\n\\n379\\n\\n380\\n\\n381\\n\\n384\\n\\n385\\n\\n386\\n\\n387\\n\\n388\\n\\n389\\n\\n390\\n\\n391\\n\\n392\\n\\n393\\n\\n394\\n\\n395\\n\\n401\\n\\nFigure 3. Age All Models Radar. Figure 4. BMI All Models Radar.\\n\\nFigure 5. Disease All Models Radar.\\n\\nFigure 6. Gender All Models Radar. Figure 9. Overall caption for the included images.\\n\\nFigure 7. Platelets All Models Radar. Figure 8. WBC All Models Radar.\\n\\n1.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 10}),\n Document(page_content='categories—Age, BMI, Disease, Gender, Platelets, and WBC—reveals significant insights into the models’ performance with raw and clean datasets. This comprehensive evaluation employs the TOPSIS method alongside human validation scores, underscoring the varying impact of data quality on model accuracy and reliability. Each category—Age, BMI, Disease, Gender, WBC, and Platelets—underscores different facets of model performance, with detailed evaluations presented in the Appendix.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 11}),\n Document(page_content='GPT-4 emerges as a resourceful model, demonstrating superior capabilities in process- ing criteria related to Age, Gender, and Platelets with both raw and clean datasets. For 411 Age and Gender, GPT-4’s performance with raw data ranks top, indicating its advanced 412 linguistic analysis capabilities and a nuanced understanding of diverse expressions in patient data. Its standout performance in Platelets count assessment with clean data em- phasizes the importance of data quality in achieving accurate health', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 11}),\n Document(page_content='assessments, especially in predicting clotting disorders or bleeding conditions. This balance between processing capabilities with raw data and the enhancement provided by clean datasets is a recurrent theme, notably improving the model’s performance in disease pattern identification as well.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 11}),\n Document(page_content='419 In contrast, Bard exhibits a slight edge over GPT-4 in the BMI category with raw 420', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 11}),\n Document(page_content='data, highlighting its robustness in handling complex nutritional indicators. This suggests Bard’s potential utility in environments with limited preprocessing capabilities. However, a comprehensive analysis incorporating Platelets criteria reveals that GPT-4’s clean data processing in this area achieves a high human validation score, contrasting with other models and underlining the critical role of clean data for accurate health assessments.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 11}),\n Document(page_content='425 The evaluation further highlights discrepancies between algorithmic efficiency and 426\\n\\nhuman judgment, particularly in the WBC count assessment where Bard leads in the TOPSIS ranking for raw data processing, yet llama_2 is preferred according to human validation scores. This indicates llama_2’s nuanced understanding of subtle patterns in 427 428 429\\n\\n402\\n\\nWBC data, aligning more closely with expert judgments—crucial for diagnosing infections or blood disorders.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 11}),\n Document(page_content='Detailed evaluations for these categories are presented in Tables A1, A2, A3, A4, A5 and A6 in the Appendix.\\n\\nTable 1. Comparison of Model Performance Across Various Health-Related Criteria. Models are evaluated through the TOPSIS method and human validation scores for processing datasets related to Age, BMI, Disease, Gender, WBC, and Platelets.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 12}),\n Document(page_content='Criteria Age Age BMI BMI Disease Disease Gender Gender Gender WBC Human Validation GPT-4 Clean WBC Platelets GPT-4 Clean Platelets Human Validation GPT-4 Clean *Low validation score for Bard Raw in WBC category indicates potential model improvement or data preprocessing areas.', metadata={'detection_class_prob': 0.9201639890670776, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '<table><thead><th>Criteria Age</th><th>Top Rank Measure MCDA</th><th>Model GPT-4 Raw</th><th>Human Validation Score 0.93</th></thead><tr><td>Age</td><td>Human Validation</td><td>GPT-4 Clean</td><td>1.00</td></tr><tr><td>BMI</td><td>MCDA</td><td>Bard Raw</td><td>0.80</td></tr><tr><td>BMI</td><td>Human Validation</td><td>GPT-4 Clean</td><td>0.84</td></tr><tr><td>Disease</td><td>MCDA</td><td>GPT-4 Raw</td><td>0.51</td></tr><tr><td>Disease</td><td>Human Validation</td><td>GPT-4 Clean</td><td>0.56</td></tr><tr><', 'filetype': 'application/pdf', 'page_number': 12, 'parent_id': '76b50808e788ce76746a209ef61943ac', 'filename': 'Benchmarking_large_language_models.pdf'}),\n Document(page_content='', metadata={'detection_class_prob': 0.9201639890670776, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'td>Gender</td><td>MCDA</td><td>GPT-4 Raw</td><td>0.59</td></tr><tr><td>Gender</td><td>Human Validation</td><td>GPT-3.5 Raw</td><td>0.59</td></tr><tr><td>Gender</td><td>Human Validation</td><td>GPT-4 Clean</td><td>0.59</td></tr><tr><td>WBC</td><td>MCDA</td><td>Bard Raw</td><td>0.05\"</td></tr><tr><td>WBC</td><td>Human Validation</td><td>GPT-4 Clean</td><td>0.99</td></tr><tr><td>Platelets</td><td>MCDA</td><td>GPT-4 Clean</td><td>0.92</td></tr><tr><td>Platelets</td><td>Human Validation</td><td>GPT-4 Clean</td><', 'filetype': 'application/pdf', 'page_number': 12, 'parent_id': '76b50808e788ce76746a209ef61943ac', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.9201639890670776, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'td>0.92</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 12, 'parent_id': '76b50808e788ce76746a209ef61943ac', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='Age Criteria: GPT-4 stands out, with its performance on raw data receiving a high MCDA score and an almost perfect human validation score of 0.93. When clean data is used, GPT-4 achieves a human validation score of 1.00, indicating flawless accuracy. BMI Criteria: Bard’s analysis of raw data is top-ranked by the MCDA method with a human validation score of 0.80, showcasing its robustness in interpreting complex nutritional health indicators. GPT-4 with clean data achieves a slightly higher human validation', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 12}),\n Document(page_content='score of 0.84. Disease Criteria: GPT-4’s raw data approach initially ranks highest by MCDA, with a human validation score of 0.51, improving to 0.56 with clean data, emphasizing the impact of data quality. Gender Criteria: GPT-4’s raw data processing ranks highest by MCDA for gender 444 identification, with a human validation score of 0.59, matched by GPT-3.5 Raw and 445 GPT-4 Clean, showcasing superior linguistic analysis capabilities. WBC Criteria: Bard’s raw data processing leads in MCDA ranking but rece', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 12}),\n Document(page_content='ives a notably low human validation score of 0.05, suggesting areas for model improvement. Conversely, GPT-4 with clean data secures a near-perfect human validation score of 0.99, indicating its precision in WBC count assessment. Platelets Criteria: GPT-4’s processing of clean data excels in both MCDA and hu- man validation scores, achieving 0.92, highlighting the significance of data quality for accurate assessments.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 12}),\n Document(page_content='This comprehensive analysis elucidates the strengths and weaknesses of different 454 models across multiple health-related criteria, revealing how some models excel with raw 455 data while others achieve remarkable accuracy with clean datasets, as reflected in their hu- man validation scores. These insights serve as a crucial reference for understanding model performance in health data analysis, emphasizing the importance of human validation in verifying and contextualizing algorithmic assessments. This com', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 12}),\n Document(page_content='prehensive evaluation not only illustrates the nuanced capabilities of large language models in healthcare data analysis but also emphasizes the critical role of data quality. It highlights the necessity for balanced consideration between algorithmic predictions and human evaluation in health- care applications. The insights drawn from this analysis underscore the importance of', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 12}),\n Document(page_content='430\\n\\n431\\n\\n432\\n\\n433\\n\\n441\\n\\n442\\n\\n443\\n\\n446\\n\\n451\\n\\n452\\n\\n453', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 12}),\n Document(page_content='model selection tailored to specific healthcare tasks, guided by both data quality and the intrinsic capabilities of each model. Future research directions should focus on enhancing 465 models’ data processing capabilities, exploring automated preprocessing techniques, and 466 broadening the scope of evaluation to include more diverse datasets and health conditions, aiming to maximize the reliability and applicability of these models in real-world healthcare settings.\\n\\n3. Discussion', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='The present study aimed to evaluate the performance of LLMs in extracting patient characteristics from free-text clinical records, addressing a critical gap in the existing literature [cite]. Accurate and efficient extraction of patient characteristics is crucial for various healthcare applications, including clinical trial recruitment, personalized medicine, and epidemiological research [cite]. Our comprehensive evaluation framework, which assessed LLM performance across various dimensions, including Cosin', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='e Similarity, Factual Consistency, Coherence, Prevalence, Fluency, Consistency, provides valuable insights into the strengths and limitations of different models and their potential to advance the field of patient characteristics extraction.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='One of the key contributions of our work is the focus on a diverse set of patient characteristics, including age with 336 patient and in range of 0 to 90 years old, BMI with 43 patients among 3 categories, disease with 280 patients, gender with 341 patients, WBC count with 37 patients, and platelet count with 61 patients. Previous studies have often focused on a limited set of characteristics or have not provided detailed evaluations across multiple dimensions [19]. By assessing LLM performance on this dive', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='rse set of 485 characteristics, we provide a more comprehensive understanding of their capabilities and 486 limitations in extracting clinically relevant information from free-text records. This is particularly important given the heterogeneity of clinical data and the need for models that can handle a wide range of patient characteristics [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='Our results demonstrate that GPT-4, particularly when feed on clean datasets, exhibits superior performance in accurately extracting patient characteristics compared to other models like GPT-3.5, llama2, and Bard. This finding builds upon previous work that has highlighted the potential of LLMs for clinical information extraction [cite] and extends it by providing a more nuanced evaluation of model performance across different characteristics and dataset conditions. The strong performance of GPT-4 suggests', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='that it could be a valuable tool for automating patient characteristics extraction in clinical settings, potentially saving time and resources compared to manual chart review [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='However, our study also highlights the challenges and considerations involved in applying LLMs to patient characteristics extraction. We found that data quality, as reflected in the use of clean versus raw datasets, had a significant impact on model performance, particularly for complex characteristics like disease and platelet count. This echoes previous findings on the importance of data preprocessing and quality control in clinical natural lan- guage processing [cite] and underscores the need for careful', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='data curation when applying LLMs to patient characteristics extraction. Our work contributes to the growing body of literature on best practices for data preparation in clinical AI applications [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='Another important contribution of our study is the incorporation of human validation alongside algorithmic evaluation. While previous studies have often relied solely on automated metrics like F1 scores or accuracy [cite], we found that human validation provided additional insights and sometimes diverged from the algorithmic rankings. This highlights the importance of involving domain experts in the evaluation process and not relying solely on automated metrics to assess model performance [cite]. Our findin', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='gs suggest that a hybrid approach, combining algorithmic efficiency with human judgment, may be the most effective strategy for ensuring the accuracy and reliability of patient characteristics extraction in clinical settings.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='In terms of clinical implications, our work demonstrates the potential of LLMs to automate and streamline the process of patient characteristics extraction from free-text\\n\\n470\\n\\n490\\n\\n491\\n\\n492\\n\\n493\\n\\n494\\n\\n495\\n\\n496\\n\\n497', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 13}),\n Document(page_content='records. This could have significant benefits for clinical trial recruitment, allowing re- searchers to quickly identify eligible patients based on specific inclusion and exclusion criteria [cite]. It could also support the development of personalized treatment plans by 519 providing clinicians with a more comprehensive view of a patient’s medical history and 520 risk factors [cite]. However, our study also highlights the need for rigorous evaluation and 521 validation of LLMs before deploying them in clini', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='cal settings, to ensure their safety and 522 effectiveness.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='In addressing the limitations of our study on benchmarking, one significant constraint was the context window size in open-source models, which did not permit the addition of more than 4096 tokens. This limitation contrasts sharply with the capabilities of GPT-4 and GPT-3.5, where such restrictions were not present, allowing for much larger inputs. Furthermore, we encountered challenges related to content and bias guardrail mechanisms, specifically with Bard. These mechanisms, designed to mitigate biases, p', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='revented us 529 from feeding certain prompts to the models. Bard exhibited restrictions on inputs related 530 to gender, blocking prompts that mentioned more than 8 gender references, and age, particularly sensitive to mentions that could infer more than 2 age categories. These guardrails, while aimed at reducing bias, significantly limited the scope of prompts we could test, highlighting a trade-off between ethical considerations and the breadth of conversational topics available for benchmarking [20].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='One of the main limitations of our study is the focus on a specific set of patient characteristics and clinical settings. Future research should explore the generalizability of our findings to other healthcare domains and patient populations [cite]. Additionally, while our evaluation framework provides a useful starting point, there is a need for standardized benchmarks and metrics to facilitate comparisons between different LLMs and evaluation approaches [cite]. The development of such benchmarks could hel', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='p accelerate progress in the field of patient characteristics extraction and support the responsible deployment of LLMs in healthcare [cite].', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='4. Conclusions', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='In conclusion, our study makes important contributions to the field of patient charac- teristics extraction by providing a comprehensive evaluation of LLM performance across a diverse set of characteristics and dataset conditions. We demonstrate the potential of models like GPT-4 to automate and streamline this process, while also highlighting the challenges and considerations involved in their deployment. By proposing a hybrid evalu- ation approach that combines algorithmic efficiency with human judgment,', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='we provide a framework for ensuring the accuracy and reliability of LLMs in clinical settings. Ultimately, our work lays the foundation for future research and development in this area, with the goal of leveraging LLMs to improve patient care and advance precision medicine.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='5. Patents\\n\\nAcknowledgments: In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments).', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='Conflicts of Interest: Declare conflicts of interest or state “The authors declare no conflicts of interest.” Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results. Any role of the funders in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript; or in the decision to publish the results must be declared in this s', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='ection. If there is no role, please state “The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results”.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='Abbreviations\\n\\n544\\n\\n554\\n\\n555\\n\\n556\\n\\n557\\n\\n566\\n\\nThe following abbreviations are used in this manuscript:\\n\\nMDPI Multidisciplinary Digital Publishing Institute DOAJ Directory of open access journals TLA LD\\n\\nThree letter acronym Linear dichroism', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 14}),\n Document(page_content='Appendix A\\n\\nAppendix A.1\\n\\nIn evaluating language models, various metrics are employed to measure their perfor- mance and alignment with human expectations. Below is a list of abbreviations for these metrics:\\n\\n• HV: Human Validation - assesses how well the model’s outputs align with human', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 15}),\n Document(page_content='judgment. F1: F1 Score - measures the balance between precision and recall, indicating the model’s accuracy. Prec.: Precision - gauges the correctness of the model’s positive predictions. Cos. Sim.: Cosine Similarity - calculates the similarity between two vectorized texts. Fact. Cons.: Factual Consistency - verifies the accuracy of the information provided 581 by the model. Coh.: Coherence - evaluates the logical and smooth flow of text. Cons.: Consistency - checks for uniform responses to similar inputs.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 15}),\n Document(page_content='Flu.: Fluency - assesses the smoothness and naturalness of the text. Relev.: Relevance - measures how well the model’s outputs correspond to the input prompts.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 15}),\n Document(page_content='• •\\n\\n• • •\\n\\nTable A1. Evaluation Metrics for Different Models for Gender criteria.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 15}),\n Document(page_content='Rank HV F1 Prec. Recall Cos. Sim. Fact. Cons. Coh. Cons. Flu. Relev. 1 2 3 4 5 6 7 8 9 10 0.59 0.51 0.49 0.42 0.59 0.51 0.54 0.43 0.07 0.08 0.95 0.93 0.94 0.93 0.95 0.95 0.94 0.93 0.86 0.86 0.95 0.93 0.94 0.92 0.95 0.95 0.93 0.92 0.85 0.84 0.95 0.94 0.95 0.93 0.95 0.95 0.95 0.94 0.87 0.87 0.87 0.69 0.71 0.57 0.79 0.76 0.71 0.61 0.31 0.28 0.70 0.54 0.54 0.41 0.62 0.59 0.55 0.45 0.25 0.23 0.41 0.37 0.35 0.30 0.14 0.14 0.14 0.12 0.38 0.21 0.63 0.55 0.57 0.52 0.21 0.21 0.21 0.18 0.56 0.28 0.43 0.50 0.41 0.40 0.', metadata={'detection_class_prob': 0.916113018989563, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '<table><thead><th>Aodel</th><th>Rank</th><th>HV</th><th>Fl</th><th>Prec.</th><th>Recall</th><th>Cos. Sim.</th><th>Fact. Cons.</th><th>Coh.</th><th>Cons.</th><th>Flu.</th><th>Relev.</th></thead><tr><td>3PT4 Raw *</td><td>1</td><td>0.59</td><td>0.95</td><td>0.95</td><td>0.95</td><td>0.87</td><td>0.70</td><td>041</td><td>0.63</td><td>043</td><td>041</td></tr><tr><td>3PT3_5 Raw</td><td>2</td><td>0.51</td><td>0.93</td><td>0.93</td><td>0.94</td><td>069</td><td>054</td><td>037</td><td>0.55</td><td>0.50</td><td>0.3', 'filetype': 'application/pdf', 'page_number': 15, 'parent_id': '4f9b8c84445dc484acdba802c885fc5a', 'filename': 'Benchmarking_large_language_models.pdf'}),\n Document(page_content='46 0.41 0.49 0.40 0.39 0.38 0.41 0.32 0.33 0.27 0.35 0.36 0.32 0.28 0.19 0.18', metadata={'detection_class_prob': 0.916113018989563, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '2</td></tr><tr><td>Jama_2 Raw</td><td>3</td><td>0.49</td><td>0.94</td><td>0.94</td><td>0.95</td><td>0.71</td><td>054</td><td>035</td><td>057</td><td>041</td><td>0.33</td></tr><tr><td>Aistral Raw</td><td>4</td><td>0.42</td><td>0.93</td><td>0.92</td><td>0.93</td><td>057</td><td>041</td><td>0.30</td><td>0.52</td><td>0.40</td><td>0.27</td></tr><tr><td>3PT4 Clean *</td><td>5</td><td>0.59</td><td>0.95</td><td>0.95</td><td>0.95</td><td>0.79</td><td>062</td><td>014</td><td>0.21</td><td>046</td><td>0.35</td></tr><tr', 'filetype': 'application/pdf', 'page_number': 15, 'parent_id': '4f9b8c84445dc484acdba802c885fc5a', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.916113018989563, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '><td>Jama_2 Clean</td><td>6</td><td>0.51</td><td>0.95</td><td>0.95</td><td>0.95</td><td>0.76</td><td>059</td><td>014</td><td>0.21</td><td>041</td><td>0.36</td></tr><tr><td>3PT3_5 Clean</td><td>7</td><td>0.54</td><td>0.94</td><td>0.93</td><td>0.95</td><td>0.71</td><td>055</td><td>014</td><td>0.21</td><td>049</td><td>0.32</td></tr><tr><td>Aistral Clean</td><td>8</td><td>0.43</td><td>0.93</td><td>0.92</td><td>0.94</td><td>061</td><td>045</td><td>012</td><td>018</td><td>040</td><td>0.28</td></tr><tr><td>ard Raw', 'filetype': 'application/pdf', 'page_number': 15, 'parent_id': '4f9b8c84445dc484acdba802c885fc5a', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.916113018989563, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '</td><td>9</td><td>0.07</td><td>086</td><td>0.85</td><td>0.87</td><td>031</td><td>0.25</td><td>0.38</td><td>0.56</td><td>0.39</td><td>0.19</td></tr><tr><td>ard Clean</td><td>BR o</td><td>0.08</td><td>0.86</td><td>084</td><td>087</td><td>0.28</td><td>0.23</td><td>0.21</td><td>0.28</td><td>0.38</td><td>0.18</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 15, 'parent_id': '4f9b8c84445dc484acdba802c885fc5a', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='* Based on the above table, it is noted that the GPT4 model for raw data is the top-ranked model for the Gender criteria based on MCDA metric, but the GPT4 model for clean data is the highest Human Validation score of 0.59.\\n\\n567\\n\\n568\\n\\n569\\n\\n570\\n\\n571\\n\\n575\\n\\n576\\n\\n579\\n\\n580\\n\\n583\\n\\n584\\n\\n585\\n\\n586\\n\\n587\\n\\nTable A2. Evaluation Metrics for Different Models for Age criteria.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 15}),\n Document(page_content='Rank HV F1 Prec. Recall Cos. Sim. Fact. Cons. Coh. Cons. Flu. Relev. 1 2 3 4 5 6 7 8 9 10 0.93 0.80 1.00 0.86 0.77 0.16 0.08 0.11 0.38 0.12 0.97 0.96 0.97 0.96 0.84 0.87 0.85 0.89 0.88 0.86 0.97 0.96 0.97 0.96 0.83 0.88 0.85 0.89 0.89 0.87 0.97 0.95 0.97 0.96 0.85 0.86 0.85 0.89 0.88 0.85 0.92 0.85 0.92 0.88 0.49 0.40 0.46 0.57 0.55 0.36 0.80 0.72 0.80 0.76 0.37 0.24 0.30 0.28 0.25 0.19 0.78 0.71 0.37 0.35 0.40 0.28 0.25 0.33 0.19 0.15 0.87 0.82 0.47 0.45 0.53 0.49 0.33 0.45 0.25 0.21 0.39 0.36 0.39 0.37 0.', metadata={'detection_class_prob': 0.9203130602836609, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '<table><thead><th>Model</th><th>Rank</th><th>HV</th><th>Fl</th><th>Prec.</th><th>Recall</th><th>Cos. Sim.</th><th>Fact, Cons.</th><th>Coh.</th><th>Cons.</th><th>Flu.</th><th>Relev.</th></thead><tr><td>sPT4 Raw *</td><td>1</td><td>0.93</td><td>0.97</td><td>097</td><td>0.97</td><td>0.92</td><td>0.80</td><td>0.78</td><td>0.87</td><td>0.39</td><td>0.75</td></tr><tr><td>3PT3_5 Raw</td><td>2</td><td>0.80</td><td>0.96</td><td>096</td><td>095</td><td>0.85</td><td>0.72</td><td>0.71</td><td>0.82</td><td>0.36</td><td>', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf'}),\n Document(page_content='29 0.34 0.38 0.18 0.18 0.32 0.75 0.69 0.76 0.72 0.31 0.30 0.31 0.27 0.25 0.26', metadata={'detection_class_prob': 0.9203130602836609, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '0.69</td></tr><tr><td>3PT4 Clean</td><td>3</td><td>1.00</td><td>0.97</td><td>0.97</td><td>0.97</td><td>0.92</td><td>080</td><td>0.37</td><td>047</td><td>0.39</td><td>0.76</td></tr><tr><td>3PT3_5 Clean</td><td>4</td><td>0.86</td><td>096</td><td>0.96</td><td>0.96</td><td>0.88</td><td>0.76</td><td>0.35</td><td>045</td><td>0.37</td><td>0.72</td></tr><tr><td>3ard Raw</td><td>5</td><td>0.77</td><td>0.84</td><td>083</td><td>085</td><td>049</td><td>037</td><td>040</td><td>0.53</td><td>0.29</td><td>0.31</td></tr><tr', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.9203130602836609, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '><td>Mistral Raw</td><td>6</td><td>0.16</td><td>087</td><td>088</td><td>086</td><td>040</td><td>0.24</td><td>0.28</td><td>049</td><td>0.34</td><td>0.30</td></tr><tr><td>3ard Clean</td><td>7</td><td>0.08</td><td>085</td><td>085</td><td>085</td><td>046</td><td>0.30</td><td>0.25</td><td>0.33</td><td>0.38</td><td>0.31</td></tr><tr><td>slama_2 Raw</td><td>8</td><td>0.11</td><td>089</td><td>089</td><td>089</td><td>0.57</td><td>0.28</td><td>033</td><td>045</td><td>0.18</td><td>0.27</td></tr><tr><td>slama_2 Clean</', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.9203130602836609, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'td><td>9</td><td>0.38</td><td>«#4088</td><td>0.89</td><td>088</td><td>0.55</td><td>0.25</td><td>0.19</td><td>0.25</td><td>0.18</td><td>0.25</td></tr><tr><td>Mistral Clean</td><td>BR o</td><td>0.12</td><td>086</td><td>087</td><td>085</td><td>0.36</td><td>019</td><td>015</td><td>0.21</td><td>0.32</td><td>0.26</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='* Based on the above table, it is noted that the GPT4 model for raw data is the top-ranked model for the Age criteria based on MCDA metric and the highest Human Validation score of 0.93.\\n\\nTable A3. Evaluation Metrics for Different Models for BMI criteria.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 16}),\n Document(page_content='Rank HV F1 Prec. Recall Cos. Sim. Fact. Cons. Coh. Cons. Flu. Relev. 1 2 3 4 5 6 7 8 9 10 0.80 0.81 0.84 0.77 0.74 0.47 0.07 0.07 0.16 0.09 0.93 0.91 0.93 0.93 0.90 0.87 0.84 0.85 0.85 0.83 0.92 0.89 0.93 0.92 0.89 0.87 0.87 0.87 0.88 0.85 0.94 0.92 0.94 0.93 0.91 0.87 0.81 0.83 0.83 0.81 0.76 0.68 0.76 0.75 0.63 0.47 0.26 0.47 0.43 0.23 0.55 0.59 0.62 0.62 0.58 0.35 0.15 0.17 0.22 0.16 0.65 0.62 0.45 0.39 0.42 0.48 0.23 0.20 0.18 0.13 0.77 0.78 0.52 0.46 0.48 0.67 0.46 0.35 0.25 0.20 0.51 0.34 0.32 0.52 0.', metadata={'detection_class_prob': 0.9177797436714172, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '<table><thead><th>Model</th><th>Rank</th><th>HV</th><th>Fl</th><th>Prec.</th><th>Recall</th><th>Cos. Sim.</th><th>Fact, Cons.</th><th>Coh.</th><th>Cons.</th><th>Flu.</th><th>Relev.</th></thead><tr><td>Bard Raw *</td><td>1</td><td>080</td><td>093</td><td>092</td><td>094</td><td>0.76</td><td>0.55</td><td>0.65</td><td>0.77</td><td>0.51</td><td>0.60</td></tr><tr><td>SPT4 Raw</td><td>2</td><td>0.81</td><td>091</td><td>089</td><td>0.92</td><td>0.68</td><td>0.59</td><td>0.62</td><td>0.78</td><td>0.34</td><td>0.59<', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf'}),\n Document(page_content='41 0.43 0.35 0.29 0.29 0.36 0.60 0.59 0.66 0.64 0.62 0.47 0.24 0.19 0.23 0.27', metadata={'detection_class_prob': 0.9177797436714172, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '/td></tr><tr><td>SPT4 Clean*</td><td>3</td><td>084</td><td>093</td><td>093</td><td>094</td><td>0.76</td><td>062</td><td>045</td><td>0.52</td><td>0.32</td><td>0.66</td></tr><tr><td>Bard Clean</td><td>4</td><td>0.77</td><td>0.93</td><td>092</td><td>093</td><td>0.75</td><td>0.62</td><td>0.39</td><td>046</td><td>0.52</td><td>0.64</td></tr><tr><td>SPT3_5 Clean</td><td>5</td><td>0.74</td><td>0.90</td><td>089</td><td>091</td><td>0.63</td><td>0.58</td><td>042</td><td>048</td><td>041</td><td>0.62</td></tr><tr><td>SP', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.9177797436714172, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'T3_5 Raw</td><td>6</td><td>0.47</td><td>0.87</td><td>0.87</td><td>0.87</td><td>047</td><td>0.35</td><td>048</td><td>0.67</td><td>0.43</td><td>0.47</td></tr><tr><td>Mistral Raw</td><td>7</td><td>0.07</td><td>0.84</td><td>087</td><td>081</td><td>0.26</td><td>015</td><td>0.23</td><td>046</td><td>0.35</td><td>0.24</td></tr><tr><td>Llama_2 Raw</td><td>8</td><td>0.07.</td><td>0.85</td><td>087</td><td>083</td><td>047</td><td>0.17</td><td>0.20</td><td>0.35</td><td>0.29</td><td>0.19</td></tr><tr><td>Llama_2 Clean</t', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.9177797436714172, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'd><td>9</td><td>016</td><td>O85</td><td>088</td><td>083</td><td>043</td><td>0.22</td><td>018</td><td>0.25</td><td>0.29</td><td>0.23</td></tr><tr><td>Mistral Clean</td><td>BR o</td><td>0.09</td><td>083</td><td>085</td><td>081</td><td>0.23</td><td>016</td><td>013</td><td>0.20</td><td>0.36</td><td>0.27</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='* Based on the above table, it is noted that the Bard model for raw data is the top-ranked model for the BMI criteria based on MCDA metric, but the GPT4 model for clean data is the highest Human Validation score of 0.84.\\n\\nTable A4. Evaluation Metrics for Different Models for Disease criteria.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 16}),\n Document(page_content='Rank HV F1 Prec. Recall Cos. Sim. Fact. Cons. Coh. Cons. Flu. Relev. 1 2 3 4 5 6 7 8 9 10 0.51 0.56 0.47 0.38 0.02 0.03 0.02 0.03 0.02 0.04 0.93 0.94 0.92 0.91 0.86 0.87 0.88 0.88 0.83 0.82 0.93 0.94 0.93 0.92 0.85 0.85 0.86 0.86 0.85 0.84 0.93 0.94 0.91 0.91 0.88 0.89 0.91 0.90 0.81 0.81 0.86 0.88 0.79 0.76 0.58 0.61 0.66 0.62 0.40 0.41 0.48 0.53 0.47 0.41 0.13 0.15 0.16 0.13 0.24 0.25 0.58 0.44 0.39 0.52 0.43 0.39 0.35 0.36 0.26 0.20 0.73 0.55 0.50 0.69 0.59 0.55 0.42 0.44 0.44 0.25 0.87 0.87 0.79 0.78 0.', metadata={'detection_class_prob': 0.9142560362815857, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '<table><thead><th>Model</th><th>Rank</th><th>HV</th><th>Fl</th><th>Prec.</th><th>Recall</th><th>Cos. Sim.</th><th>Fact, Cons.</th><th>Coh.</th><th>Cons.</th><th>Flu.</th><th>Relev.</th></thead><tr><td>3PT4 Raw *</td><td>1</td><td>0.51</td><td>0.93</td><td>0.93</td><td>0.93</td><td>0.86</td><td>048</td><td>058</td><td>0.73</td><td>0.87</td><td>0.63</td></tr><tr><td>3PT4 Clean*</td><td>2</td><td>056</td><td>094</td><td>094</td><td>094</td><td>0.88</td><td>0.53</td><td>044</td><td>0.55</td><td>0.87</td><td>0.6', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf'}),\n Document(page_content='80 0.83 0.89 0.85 0.27 0.30 0.63 0.67 0.59 0.54 0.38 0.38 0.40 0.41 0.30 0.31', metadata={'detection_class_prob': 0.9142560362815857, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '7</td></tr><tr><td>3PT3_5 Clean</td><td>3</td><td>0.47</td><td>0.92</td><td>093</td><td>0.91</td><td>0.79</td><td>047</td><td>0.39</td><td>0.50</td><td>0.79</td><td>0.59</td></tr><tr><td>3PT3_5 Raw</td><td>4</td><td>0.38</td><td>091</td><td>092</td><td>091</td><td>0.76</td><td>041</td><td>0.52</td><td>0.69</td><td>0.78</td><td>0.54</td></tr><tr><td>lama_2 Raw</td><td>5</td><td>0.02</td><td>086</td><td>085</td><td>088</td><td>0.58</td><td>0.13</td><td>043</td><td>059</td><td>0.80</td><td>0.38</td></tr><tr><t', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.9142560362815857, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'd>Mistral Raw</td><td>6</td><td>0.03</td><td>0.87</td><td>085</td><td>089</td><td>061</td><td>0.15</td><td>039</td><td>055</td><td>0.83</td><td>0.38</td></tr><tr><td>Mistral Clean</td><td>7</td><td>0.02</td><td>088</td><td>086</td><td>091</td><td>0.66</td><td>0.16</td><td>035</td><td>042</td><td>0.89</td><td>0.40</td></tr><tr><td>lama_2 Clean</td><td>8</td><td>0.03</td><td>4088</td><td>086</td><td>0.90</td><td>062</td><td>0.13</td><td>0.36</td><td>0.</td><td>0.85</td><td>0.41</td></tr><tr><td>sard Raw</td><', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.9142560362815857, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'td>9</td><td>0.02</td><td>083</td><td>085</td><td>0.81</td><td>040</td><td>0.24</td><td>0.26</td><td>0O.</td><td>0.27</td><td>0.30</td></tr><tr><td>ard Clean</td><td>BR o</td><td>0.04</td><td>082</td><td>084</td><td>081</td><td>041</td><td>025</td><td>0.20</td><td>0.25</td><td>0.30</td><td>0.31</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 16, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='* Based on the above table, it is noted that the GPT4 model for raw data is the top-ranked model for the Disease criteria on MCDA metric and the GPT4 model for clean data is the highest Human Validation score of 0.56.\\n\\nTable A5. Evaluation Metrics for Different Models for Platelets criteria.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 16}),\n Document(page_content='Rank HV F1 Prec. Recall Cos. Sim. Fact. Cons. Coh. Cons. Flu. Relev. 1 2 3 4 5 6 7 8 9 10 0.92 0.82 0.52 0.44 0.44 0.36 0.23 0.16 0.21 0.10 0.93 0.91 0.88 0.87 0.87 0.87 0.83 0.83 0.86 0.82 0.92 0.89 0.90 0.90 0.90 0.89 0.81 0.82 0.86 0.83 0.94 0.93 0.85 0.85 0.84 0.84 0.84 0.85 0.86 0.82 0.78 0.80 0.58 0.55 0.56 0.51 0.25 0.30 0.51 0.24 0.70 0.74 0.59 0.57 0.54 0.51 0.23 0.24 0.29 0.14 0.45 0.55 0.43 0.41 0.27 0.23 0.53 0.33 0.21 0.25 0.54 0.77 0.69 0.70 0.37 0.36 0.63 0.38 0.32 0.50 0.41 0.24 0.57 0.48 0.', metadata={'detection_class_prob': 0.9139719605445862, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '<table><thead><th>Aodel</th><th>Rank</th><th>HV</th><th>F1</th><th>Prec.</th><th>Recall</th><th>Cos. Sim.</th><th>Fact. Cons.</th><th>Coh.</th><th>Cons.</th><th>Flu.</th><th>Relev.</th></thead><tr><td>3PT4 Clean *</td><td>1</td><td>0.92</td><td>0.93</td><td>0.92</td><td>0.94</td><td>0.78</td><td>0.70</td><td>045</td><td>0.54</td><td>041</td><td>0.62</td></tr><tr><td>3PT4 Raw</td><td>2</td><td>0.82</td><td>0.91</td><td>089</td><td>0.93</td><td>080</td><td>0.74</td><td>0.55</td><td>0.77</td><td>0.24</td><td>0', 'filetype': 'application/pdf', 'page_number': 17, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf'}),\n Document(page_content='54 0.49 0.62 0.56 0.31 0.33 0.62 0.56 0.48 0.46 0.44 0.40 0.37 0.36 0.29 0.23', metadata={'detection_class_prob': 0.9139719605445862, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '.56</td></tr><tr><td>Aistral Raw</td><td>3</td><td>0.52</td><td>0.88</td><td>0.90</td><td>085</td><td>058</td><td>059</td><td>043</td><td>0.69</td><td>0.57</td><td>0.48</td></tr><tr><td>Jama_2 Raw</td><td>4</td><td>044</td><td>087</td><td>090</td><td>085</td><td>0.55</td><td>0.57</td><td>041</td><td>0.70</td><td>048</td><td>0.46</td></tr><tr><td>Aistral Clean</td><td>5</td><td>044</td><td>087</td><td>090</td><td>084</td><td>0.56</td><td>054</td><td>0.27</td><td>0.37</td><td>0.54</td><td>0.44</td></tr><tr><t', 'filetype': 'application/pdf', 'page_number': 17, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.9139719605445862, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'd>Jama_2 Clean</td><td>6</td><td>0.36</td><td>0.87</td><td>089</td><td>084</td><td>0.51</td><td>051</td><td>0.23</td><td>0.36</td><td>0.49</td><td>0.40</td></tr><tr><td>ard Raw</td><td>7</td><td>0.23</td><td>0.83</td><td>0.81</td><td>084</td><td>0.25</td><td>023</td><td>053</td><td>0.63</td><td>0.62</td><td>0.37</td></tr><tr><td>ard Clean</td><td>8</td><td>0.16</td><td>0.83</td><td>082</td><td>085</td><td>030</td><td>024</td><td>0.33</td><td>0.38</td><td>0.56</td><td>0.36</td></tr><tr><td>3PT3_5 Clean</td><', 'filetype': 'application/pdf', 'page_number': 17, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.9139719605445862, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'td>9</td><td>0.21</td><td>086</td><td>086</td><td>086</td><td>051</td><td>029</td><td>0.21</td><td>032</td><td>0.31</td><td>0.29</td></tr><tr><td>3PT3_5 Raw</td><td>10</td><td>0.10</td><td>082</td><td>083</td><td>082</td><td>024</td><td>O14</td><td>0.25</td><td>0.50</td><td>0.33</td><td>0.23</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 17, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='* Based on the provided table, it is noted that the GPT4 Clean model is the top-ranked model for the Platelets criteria, with the highest Human Validation score of 0.92.\\n\\nTable A6. Evaluation Metrics for Different Models for WBC criteria.', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 17}),\n Document(page_content='Rank HV F1 Prec. Recall Cos. Sim. Fact. Cons. Coh. Cons. Flu. Relev. 1 2 3 4 5 6 7 8 9 10 0.05 0.76 0.94 0.95 0.93 0.99 0.95 0.94 0.03 0.16 0.80 0.87 0.99 0.99 0.99 0.99 0.99 0.99 0.81 0.83 0.79 0.85 0.99 0.99 0.99 0.99 0.99 0.99 0.82 0.83 0.82 0.89 0.99 0.99 0.99 0.99 0.99 0.99 0.81 0.83 0.17 0.58 0.95 0.95 0.95 0.97 0.95 0.95 0.24 0.30 0.20 0.41 0.74 0.74 0.73 0.75 0.74 0.74 0.20 0.20 0.58 0.29 0.16 0.15 0.14 0.11 0.11 0.10 0.20 0.15 0.60 0.47 0.39 0.38 0.38 0.15 0.16 0.15 0.24 0.22 0.88 0.64 0.34 0.36 0.', metadata={'detection_class_prob': 0.911581814289093, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '<table><thead><th>Aodel</th><th>Rank</th><th>HV</th><th>Fl</th><th>Prec.</th><th>Recall</th><th>Cos. Sim.</th><th>Fact, Cons.</th><th>Coh.</th><th>Cons.</th><th>Flu.</th><th>Relev.</th></thead><tr><td>ard Raw</td><td>1</td><td>0.05</td><td>0.80</td><td>0.79</td><td>0.82</td><td>0.17</td><td>0.20</td><td>0.58</td><td>0.60</td><td>0.88</td><td>0.46</td></tr><tr><td>3PT4 Raw</td><td>2</td><td>0.76</td><td>0.87</td><td>085</td><td>089</td><td>058</td><td>041</td><td>0.29</td><td>O47</td><td>0.64</td><td>0.54</t', 'filetype': 'application/pdf', 'page_number': 17, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf'}),\n Document(page_content='35 0.38 0.35 0.36 0.66 0.44 0.46 0.54 0.64 0.64 0.64 0.66 0.64 0.64 0.34 0.26', metadata={'detection_class_prob': 0.911581814289093, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'd></tr><tr><td>Jama_2 Raw *</td><td>3</td><td>0.94</td><td>099</td><td>0.99</td><td>0.99</td><td>0.95</td><td>0.74</td><td>0.16</td><td>039</td><td>0.34</td><td>0.64</td></tr><tr><td>Aistral Raw</td><td>4</td><td>0.95</td><td>099</td><td>0.99</td><td>0.99</td><td>0.95</td><td>0.74</td><td>0.15</td><td>038</td><td>0.36</td><td>0.64</td></tr><tr><td>3PT3_5 Raw</td><td>5</td><td>0.93</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.95</td><td>0.73</td><td>014</td><td>038</td><td>0.35</td><td>0.64</td></tr><tr>', 'filetype': 'application/pdf', 'page_number': 17, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.911581814289093, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': '<td>3PT4 Clean</td><td>6</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.99</td><td>0.97</td><td>0.75</td><td>0.11</td><td>015</td><td>0.38</td><td>0.66</td></tr><tr><td>Jama_2 Clean</td><td>7</td><td>0.95</td><td>099</td><td>0.99</td><td>0.99</td><td>0.95</td><td>0.74</td><td>011</td><td>016</td><td>0.35</td><td>0.64</td></tr><tr><td>Aistral Clean</td><td>8</td><td>0.94</td><td>099</td><td>0.99</td><td>0.99</td><td>0.95</td><td>0.74</td><td>0.10</td><td>015</td><td>0.36</td><td>0.64</td></tr><tr><td>ard C', 'filetype': 'application/pdf', 'page_number': 17, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='', metadata={'detection_class_prob': 0.911581814289093, 'last_modified': '2024-04-10T18:40:10', 'text_as_html': 'lean</td><td>9</td><td>0.03</td><td>081</td><td>082</td><td>081</td><td>024</td><td>0.20</td><td>0.20</td><td>024</td><td>066</td><td>0.34</td></tr><tr><td>3PT3_5 Clean</td><td>an o</td><td>0.16</td><td>0.83</td><td>0.83</td><td>0.83</td><td>0.30</td><td>0.20</td><td>0.15</td><td>0.22</td><td>0.44</td><td>0.26</td></tr></table>', 'filetype': 'application/pdf', 'page_number': 17, 'parent_id': '372e3a2cde4d4593c9fc602cadee9f99', 'filename': 'Benchmarking_large_language_models.pdf', 'is_continuation': True}),\n Document(page_content='* Based on the above table, it is noted that the GPT4 model for clean data is the top-ranked model for the BMI(Body Mass Index) criteria, but the GPT4 model for raw data is the highest Human Validation score of 0.84.\\n\\nReferences\\n\\n588\\n\\n589\\n\\n590\\n\\n591\\n\\n592\\n\\n593\\n\\n594\\n\\n595\\n\\n596\\n\\n597\\n\\n598\\n\\n599\\n\\n600\\n\\n601\\n\\n602\\n\\n603\\n\\n604\\n\\n605\\n\\n606\\n\\n607\\n\\n608\\n\\n611\\n\\n612\\n\\n615\\n\\n616\\n\\n617\\n\\n618\\n\\n619\\n\\n620\\n\\n621\\n\\n622\\n\\n623\\n\\n624\\n\\n625\\n\\n626\\n\\n627\\n\\n628\\n\\n629\\n\\n630\\n\\n631\\n\\n632\\n\\n633', metadata={'filename': 'Benchmarking_large_language_models.pdf', 'filetype': 'application/pdf', 'last_modified': '2024-04-10T18:40:10', 'page_number': 17})]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:38:07.751019Z",
     "start_time": "2024-05-02T10:38:07.735500400Z"
    }
   },
   "id": "ee9d9d6085e86ce2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connecting to the Vector Database and Storing Data\n",
    "\n",
    "Step 1: Connect to the Vector Database:\n",
    "Start by extracting the connection settings from your configuration file. These settings should include the database connection string and any other necessary parameters.Just pass the collection name.\n",
    "\n",
    "Step 2: Store Chunks:\n",
    "Use the store_in_vectorstore method of your database client to store the prepared chunks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75547e03898eb7cc"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 14:08:08,211 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully.\n"
     ]
    }
   ],
   "source": [
    "IRA.connect_to_vectorstore(collection_name=\"sample_pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:38:08.344080300Z",
     "start_time": "2024-05-02T10:38:07.743499700Z"
    }
   },
   "id": "6e81c157b93b4501"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 14:08:11,664 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:15,281 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:15,897 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:16,401 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:16,856 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:17,283 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:17,664 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:18,070 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:18,498 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:18,958 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:19,336 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:19,776 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:20,177 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:20,652 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:21,497 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:21,607 - INFO - Document added successfully to the vector store.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Indox.vectorstore.ChromaVectorStore at 0x1f754cd96a0>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRA.store_in_vectorstore(chunks=documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:38:21.617702900Z",
     "start_time": "2024-05-02T10:38:08.347601200Z"
    }
   },
   "id": "cd489a60f983a3be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute a query and retrieve the responses, along with the scores of the retrieved chunks and the context that was sent to the language learning model (LLM)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bdc5ff7ddbcf75b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "query = \"which model's result is better for gender criteria \""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:38:21.626509400Z",
     "start_time": "2024-05-02T10:38:21.620891500Z"
    }
   },
   "id": "ef7a12aa8c46e592"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 14:08:22,188 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:08:25,266 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = IRA.answer_question(query=query, top_k=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:38:25.278309700Z",
     "start_time": "2024-05-02T10:38:21.624406800Z"
    }
   },
   "id": "f73a511991d9f4b4"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'The GPT4 model for clean data is the better model for the Gender criteria, as it has the highest Human Validation score of 0.59 compared to the GPT4 model for raw data.'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:38:54.442207400Z",
     "start_time": "2024-05-02T10:38:54.417970700Z"
    }
   },
   "id": "3f7e6ea6220a268d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4b935470f097412e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4898c9fdee9332b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indox for html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b083f04823b38f78"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "IRA_2 = IndoxRetrievalAugmentation()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:39:16.245748900Z",
     "start_time": "2024-05-02T10:39:14.787707800Z"
    }
   },
   "id": "3c23e90dd585dd70"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{'clustering': {'dim': 10, 'threshold': 0.1},\n 'embedding_model': 'openai',\n 'postgres': {'conn_string': 'postgresql+psycopg2://postgres:xxx@localhost:port/db_name'},\n 'prompts': {'summary_model': {'content': 'You are a helpful assistant. Give a detailed summary of the documentation provided'}},\n 'qa_model': {'temperature': 0},\n 'splitter': 'semantic-text-splitter',\n 'summary_model': {'max_tokens': 100,\n  'min_len': 30,\n  'model_name': 'gpt-3.5-turbo-0125'},\n 'vector_store': 'chroma'}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRA_2.config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:39:16.258193Z",
     "start_time": "2024-05-02T10:39:16.250585300Z"
    }
   },
   "id": "458742741e229796"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "html = \"https://www.python.org/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:19.466314500Z",
     "start_time": "2024-05-02T11:40:19.458076300Z"
    }
   },
   "id": "54387baca31854e8"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 14:09:39,971 - INFO - Reading document from string ...\n",
      "2024-05-02 14:09:40,053 - INFO - Reading document ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Chunking process\n"
     ]
    }
   ],
   "source": [
    "chunks = IRA_2.create_chunks(file_path=html, unstructured=True, content_type=\"html\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:39:40.562054300Z",
     "start_time": "2024-05-02T10:39:38.474325400Z"
    }
   },
   "id": "e4d64ea06cdf68d7"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience.\\n\\nSkip to content\\n\\n▼ Close\\n\\nPython\\n\\nPSF\\n\\nDocs\\n\\nPyPI\\n\\nJobs\\n\\nCommunity\\n\\n▲ The Python Network\\n\\nDonate', metadata={'emphasized_text_contents': \"['Notice:', '▼', '▼', '▲', '▲']\", 'emphasized_text_tags': \"['strong', 'span', 'span', 'span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['Skip to content', '\\\\n                    ', 'Python', 'PSF', 'Docs', 'PyPI', 'Jobs', 'Community', '\\\\n                    ', 'Donate']\", 'link_urls': \"['#content', '#python-network', '/', 'https://www.python.org/psf/', 'https://docs.python.org', 'https://pypi.org/', '/jobs/', '/community-landing/', '#top', 'https://psfmember.org/civicrm/contribute/transact?reset=1&id=2']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='≡ Menu\\n\\nA A\\n                                    \\n                                        Smaller\\n                                        Larger\\n                                        Reset\\n\\nSocialize\\n                                    \\n                                        LinkedIn\\n                                        Mastodon\\n                                        Chat on IRC\\n                                        Twitter', metadata={'emphasized_text_contents': \"['≡', 'A A']\", 'emphasized_text_tags': \"['span', 'strong']\", 'filetype': 'text/html', 'link_texts': \"[None, None, 'Smaller', 'Larger', 'Reset', 'Socialize', None, None, None, None]\", 'link_urls': \"['#site-map', '#', 'javascript:;', 'javascript:;', 'javascript:;', '#', 'https://www.linkedin.com/company/python-software-foundation/', 'https://fosstodon.org/@ThePSF', '/community/irc/', 'https://twitter.com/ThePSF']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='About\\n        \\n            \\n\\n\\n    \\n        Applications\\n    \\n        Quotes\\n    \\n        Getting Started\\n    \\n        Help\\n    \\n        Python Brochure\\n\\nDownloads\\n        \\n            \\n\\n\\n    \\n        All releases\\n    \\n        Source code\\n    \\n        Windows\\n    \\n        macOS\\n    \\n        Other Platforms\\n    \\n        License\\n    \\n        Alternative Implementations', metadata={'filetype': 'text/html', 'link_texts': \"['About', 'Applications', 'Quotes', 'Getting Started', 'Help', 'Python Brochure', 'Downloads', 'All releases', 'Source code', 'Windows', 'macOS', 'Other Platforms', 'License', 'Alternative Implementations']\", 'link_urls': \"['/about/', '/about/apps/', '/about/quotes/', '/about/gettingstarted/', '/about/help/', 'http://brochure.getpython.info/', '/downloads/', '/downloads/', '/downloads/source/', '/downloads/windows/', '/downloads/macos/', '/download/other/', 'https://docs.python.org/3/license.html', '/download/alternatives']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content=\"Documentation\\n        \\n            \\n\\n\\n    \\n        Docs\\n    \\n        Audio/Visual Talks\\n    \\n        Beginner's Guide\\n    \\n        Developer's Guide\\n    \\n        FAQ\\n    \\n        Non-English Docs\\n    \\n        PEP Index\\n    \\n        Python Books\\n    \\n        Python Essays\", metadata={'filetype': 'text/html', 'link_texts': '[\\'Documentation\\', \\'Docs\\', \\'Audio/Visual Talks\\', \"Beginner\\'s Guide\", \"Developer\\'s Guide\", \\'FAQ\\', \\'Non-English Docs\\', \\'PEP Index\\', \\'Python Books\\', \\'Python Essays\\']', 'link_urls': \"['/doc/', '/doc/', '/doc/av', 'https://wiki.python.org/moin/BeginnersGuide', 'https://devguide.python.org/', 'https://docs.python.org/faq/', 'http://wiki.python.org/moin/Languages', 'https://peps.python.org', 'https://wiki.python.org/moin/PythonBooks', '/doc/essays/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='Community\\n        \\n            \\n\\n\\n    \\n        Diversity\\n    \\n        Mailing Lists\\n    \\n        IRC\\n    \\n        Forums\\n    \\n        PSF Annual Impact Report\\n    \\n        Python Conferences\\n    \\n        Special Interest Groups\\n    \\n        Python Logo\\n    \\n        Python Wiki\\n    \\n        Code of Conduct\\n    \\n        Community Awards\\n    \\n        Get Involved\\n    \\n        Shared Stories', metadata={'filetype': 'text/html', 'link_texts': \"['Community', 'Diversity', 'Mailing Lists', 'IRC', 'Forums', 'PSF Annual Impact Report', 'Python Conferences', 'Special Interest Groups', 'Python Logo', 'Python Wiki', 'Code of Conduct', 'Community Awards', 'Get Involved', 'Shared Stories']\", 'link_urls': \"['/community/', '/community/diversity/', '/community/lists/', '/community/irc/', '/community/forums/', '/psf/annual-report/2021/', '/community/workshops/', '/community/sigs/', '/community/logos/', 'https://wiki.python.org/moin/', '/psf/conduct/', '/community/awards', '/psf/get-involved/', '/psf/community-stories/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='Success Stories\\n        \\n            \\n\\n\\n    \\n        Arts\\n    \\n        Business\\n    \\n        Education\\n    \\n        Engineering\\n    \\n        Government\\n    \\n        Scientific\\n    \\n        Software Development\\n\\nNews\\n        \\n            \\n\\n\\n    \\n        Python News\\n    \\n        PSF Newsletter\\n    \\n        PSF News\\n    \\n        PyCon US News\\n    \\n        News from the Community', metadata={'filetype': 'text/html', 'link_texts': \"['Success Stories', 'Arts', 'Business', 'Education', 'Engineering', 'Government', 'Scientific', 'Software Development', 'News', 'Python News', 'PSF Newsletter', 'PSF News', 'PyCon US News', 'News from the Community']\", 'link_urls': \"['/success-stories/', '/success-stories/category/arts/', '/success-stories/category/business/', '/success-stories/category/education/', '/success-stories/category/engineering/', '/success-stories/category/government/', '/success-stories/category/scientific/', '/success-stories/category/software-development/', '/blogs/', '/blogs/', '/psf/newsletter/', 'http://pyfound.blogspot.com/', 'http://pycon.blogspot.com/', 'http://planetpython.org/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='Events\\n        \\n            \\n\\n\\n    \\n        Python Events\\n    \\n        User Group Events\\n    \\n        Python Events Archive\\n    \\n        User Group Events Archive\\n    \\n        Submit an Event\\n\\n>_\\n                        Launch Interactive Shell', metadata={'emphasized_text_contents': \"['Launch Interactive Shell']\", 'emphasized_text_tags': \"['span']\", 'filetype': 'text/html', 'link_texts': \"['Events', 'Python Events', 'User Group Events', 'Python Events Archive', 'User Group Events Archive', 'Submit an Event', '>_\\\\n                        ']\", 'link_urls': \"['/events/', '/events/python-events/', '/events/python-user-group/', '/events/python-events/past/', '/events/python-user-group/past/', 'https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event', '/shell/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content=\"# Python 3: Fibonacci series up to n\\r\\n>>> def fib(n):\\r\\n>>>     a, b = 0, 1\\r\\n>>>     while a < n:\\r\\n>>>         print(a, end=' ')\\r\\n>>>         a, b = b, a+b\\r\\n>>>     print()\\r\\n>>> fib(1000)\\r\\n0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987\\n                    Functions Defined\\r\\nThe core of extensible programming is defining functions. Python allows mandatory and optional arguments, keyword arguments, and even arbitrary argument lists. More about defining functions in Python\\xa03\", metadata={'emphasized_text_contents': \"['# Python 3: Fibonacci series up to n', '0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987']\", 'emphasized_text_tags': \"['span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['More about defining functions in Python\\\\xa03']\", 'link_urls': \"['//docs.python.org/3/tutorial/controlflow.html#defining-functions']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content=\"# Python 3: List comprehensions\\r\\n>>> fruits = ['Banana', 'Apple', 'Lime']\\r\\n>>> loud_fruits = [fruit.upper() for fruit in fruits]\\r\\n>>> print(loud_fruits)\\r\\n['BANANA', 'APPLE', 'LIME']\\r\\n\\r\\n# List and the enumerate function\\r\\n>>> list(enumerate(fruits))\\r\\n[(0, 'Banana'), (1, 'Apple'), (2, 'Lime')]\\n                    Compound Data Types\\r\\nLists (known as arrays in other languages) are one of the compound data types that Python understands. Lists can be indexed, sliced and manipulated with other built-in functions.\", metadata={'emphasized_text_contents': '[\\'# Python 3: List comprehensions\\', \"[\\'BANANA\\', \\'APPLE\\', \\'LIME\\']\", \\'# List and the enumerate function\\', \"[(0, \\'Banana\\'), (1, \\'Apple\\'), (2, \\'Lime\\')]\"]', 'emphasized_text_tags': \"['span', 'span', 'span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['More about lists in Python\\\\xa03']\", 'link_urls': \"['//docs.python.org/3/tutorial/introduction.html#lists']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='More about lists in Python\\xa03', metadata={'emphasized_text_contents': '[\\'# Python 3: List comprehensions\\', \"[\\'BANANA\\', \\'APPLE\\', \\'LIME\\']\", \\'# List and the enumerate function\\', \"[(0, \\'Banana\\'), (1, \\'Apple\\'), (2, \\'Lime\\')]\"]', 'emphasized_text_tags': \"['span', 'span', 'span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['More about lists in Python\\\\xa03']\", 'link_urls': \"['//docs.python.org/3/tutorial/introduction.html#lists']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='# Python 3: Simple arithmetic\\r\\n>>> 1 / 2\\r\\n0.5\\r\\n>>> 2 ** 3\\r\\n8\\r\\n>>> 17 / 3  # classic division returns a float\\r\\n5.666666666666667\\r\\n>>> 17 // 3  # floor division\\r\\n5\\n                    Intuitive Interpretation\\r\\nCalculations are simple with Python, and expression syntax is straightforward: the operators +, -, * and / work as expected; parentheses () can be used for grouping. More about simple math functions in Python\\xa03.', metadata={'emphasized_text_contents': \"['# Python 3: Simple arithmetic', '0.5', '8', '# classic division returns a float', '5.666666666666667', '# floor division', '5']\", 'emphasized_text_tags': \"['span', 'span', 'span', 'span', 'span', 'span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['More about simple math functions in Python\\\\xa03']\", 'link_urls': \"['http://docs.python.org/3/tutorial/introduction.html#using-python-as-a-calculator']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content=\"# For loop on a list\\r\\n>>> numbers = [2, 4, 6, 8]\\r\\n>>> product = 1\\r\\n>>> for number in numbers:\\r\\n...    product = product * number\\r\\n... \\r\\n>>> print('The product is:', product)\\r\\nThe product is: 384\\n                    All the Flow You’d Expect\\r\\nPython knows the usual control flow statements that other languages speak — if, for, while and range — with some of its own twists, of course. More control flow tools in Python\\xa03\", metadata={'emphasized_text_contents': \"['# For loop on a list', 'The product is: 384']\", 'emphasized_text_tags': \"['span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['More control flow tools in Python\\\\xa03']\", 'link_urls': \"['//docs.python.org/3/tutorial/controlflow.html']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='# Simple output (with Unicode)\\r\\n>>> print(\"Hello, I\\'m Python!\")\\r\\nHello, I\\'m Python!\\r\\n# Input, assignment\\r\\n>>> name = input(\\'What is your name?\\\\n\\')\\r\\nWhat is your name?\\r\\nPython\\r\\n>>> print(f\\'Hi, {name}.\\')\\r\\nHi, Python.\\r\\n\\n                    Quick & Easy to Learn\\r\\nExperienced programmers in any other language can pick up Python very quickly, and beginners find the clean syntax and indentation structure easy to learn. Whet your appetite with our Python\\xa03 overview.', metadata={'emphasized_text_contents': '[\\'# Simple output (with Unicode)\\', \"Hello, I\\'m Python!\", \\'# Input, assignment\\', \\'What is your name?\\\\r\\\\nPython\\', \\'Hi, Python.\\']', 'emphasized_text_tags': \"['span', 'span', 'span', 'span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['Whet your appetite']\", 'link_urls': \"['//docs.python.org/3/tutorial/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content=\"Python is a programming language that lets you work quickly and integrate systems more effectively. Learn More\\n\\nGet Started\\n\\nWhether you're new to programming or an experienced developer, it's easy to learn and use Python.\\n\\nStart with our Beginner’s Guide\\n\\nDownload\\n\\nPython source code and installers are available for download for all versions!\\n\\nLatest: Python 3.12.3\\n\\nDocs\\n\\nDocumentation for Python's standard library, along with tutorials and guides, are available online.\\n\\ndocs.python.org\", metadata={'filetype': 'text/html', 'link_texts': \"['Learn More', 'Start with our Beginner’s Guide', 'Python 3.12.3', 'docs.python.org']\", 'link_urls': \"['/doc/', '/about/gettingstarted/', '/downloads/release/python-3123/', 'https://docs.python.org']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content=\"Jobs\\n\\nLooking for work or have a Python related position that you're trying to hire for? Our relaunched community-run job board is the place to go.\\n\\njobs.python.org\\n\\nLatest News\\n\\nMore\\n\\n2024-04-09\\n Announcing Python Software Foundation Fellow Members for Q4 2023! 🎉\\n\\n2024-04-09\\n Python 3.12.3 and 3.13.0a6 released\\n\\n2024-04-08\\n Python 3.11.9 is now available\\n\\n2024-04-04\\n Python Software Foundation - April 2024 Newsletter\\n\\n2024-04-02\\n New Open Initiative for Cybersecurity Standards\\n\\nUpcoming Events\", metadata={'emphasized_text_contents': \"['relaunched community-run job board', '2024-', '2024-', '2024-', '2024-', '2024-']\", 'emphasized_text_tags': \"['strong', 'span', 'span', 'span', 'span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['jobs.python.org', 'More', 'Announcing Python Software Foundation Fellow Members for Q4 2023! 🎉', 'Python 3.12.3 and 3.13.0a6 released', 'Python 3.11.9 is now available', 'Python Software Foundation - April 2024 Newsletter', 'New Open Initiative for Cybersecurity Standards']\", 'link_urls': \"['//jobs.python.org', 'https://blog.python.org', 'https://pyfound.blogspot.com/2024/04/announcing-python-software-foundation.html', 'https://pythoninsider.blogspot.com/2024/04/python-3123-and-3130a6-released.html', 'https://pythoninsider.blogspot.com/2024/04/python-3119-is-now-available.html', 'https://mailchi.mp/python/python-software-foundation-december-2023-newsletter-15957293', 'https://pyfound.blogspot.com/2024/04/new-open-initiative-for-cybersecurity.html']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='More\\n\\n2024-05-04\\n TOUFU - Parent-Child Python Programming Workshop\\n\\n2024-05-05\\n TOUFU - Parent-Child Python Programming Workshop\\n\\n2024-05-06\\n TOUFU - Parent-Child Python Programming Workshop\\n\\n2024-05-07\\n May Helsinki Python meetup\\n\\n2024-05-15\\n PyCon US 2024\\n\\nSuccess Stories', metadata={'emphasized_text_contents': \"['2024-', '2024-', '2024-', '2024-', '2024-']\", 'emphasized_text_tags': \"['span', 'span', 'span', 'span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['More', 'TOUFU - Parent-Child Python Programming Workshop', 'TOUFU - Parent-Child Python Programming Workshop', 'TOUFU - Parent-Child Python Programming Workshop', 'May Helsinki Python meetup', 'PyCon US 2024']\", 'link_urls': \"['/events/calendars/', '/events/python-user-group/1758/', '/events/python-user-group/1759/', '/events/python-user-group/1761/', '/events/python-user-group/1762/', '/events/python-events/1508/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='More\\n\\nGenerating realistic location data for users for testing or modeling simulations is a hard problem. Current approaches just create random locations inside a box, placing users in waterways or on top of buildings. This inability to make accurate, synthetic location data stifles a lot of innovative projects that require diverse and complex datasets to fuel their work.', metadata={'filetype': 'text/html', 'link_texts': \"['More', 'Generating realistic location data for users for testing or modeling simulations is a hard problem. Current approaches just create random locations inside a box, placing users in waterways or on top of buildings. This inability to make accurate, synthetic location data stifles a lot of innovative projects that require diverse and complex datasets to fuel their work.']\", 'link_urls': \"['/success-stories/', '/success-stories/using-python-with-gretelai-to-generate-synthetic-location-data/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='Using Python with Gretel.ai to Generate Synthetic Location Data   by Alex Watson, co-founder and CPO, Gretel.ai', metadata={'text_as_html': '<table><tr><td><br/>                                        <br/></td><td>Using Python with Gretel.ai to Generate Synthetic Location Data</td><td></td><td>by Alex Watson, co-founder and CPO, Gretel.ai</td><td><br/></td></tr></table>', 'page_number': 1, 'parent_id': 'd47d7cb0e4f8fd2be5ee07826694c189', 'url': 'https://www.python.org/', 'filetype': 'text/html'}),\n Document(page_content='Use Python for…\\n\\nMore\\n\\nWeb Development:\\r\\n        Django, Pyramid, Bottle, Tornado, Flask, web2py\\n\\nGUI Development:\\r\\n        tkInter, PyGObject, PyQt, PySide, Kivy, wxPython, DearPyGui\\n\\nScientific and Numeric:\\r\\n        \\r\\nSciPy, Pandas, IPython\\n\\nSoftware Development:\\r\\n        Buildbot, Trac, Roundup\\n\\nSystem Administration:\\r\\n        Ansible, Salt, OpenStack, xonsh\\n\\n>>> Python Enhancement Proposals (PEPs): The future of Python is discussed here.\\n                         RSS', metadata={'emphasized_text_contents': \"['Web Development', 'Django, Pyramid, Bottle, Tornado, Flask, web2py', 'GUI Development', 'tkInter, PyGObject, PyQt, PySide, Kivy, wxPython, DearPyGui', 'Scientific and Numeric', 'SciPy, Pandas, IPython', 'Software Development', 'Buildbot, Trac, Roundup', 'System Administration', 'Ansible, Salt, OpenStack, xonsh', '>>>', '(PEPs)', 'is discussed here.']\", 'emphasized_text_tags': \"['b', 'span', 'b', 'span', 'b', 'span', 'b', 'span', 'b', 'span', 'span', 'span', 'span']\", 'filetype': 'text/html', 'link_texts': \"['More', 'Django', 'Pyramid', 'Bottle', 'Tornado', 'Flask', 'web2py', 'tkInter', 'PyGObject', 'PyQt', 'PySide', 'Kivy', 'wxPython', 'DearPyGui', 'SciPy', 'Pandas', 'IPython', 'Buildbot', 'Trac', 'Roundup', 'Ansible', 'Salt', 'OpenStack', 'xonsh', 'Python Enhancement Proposals', None]\", 'link_urls': \"['/about/apps', 'http://www.djangoproject.com/', 'http://www.pylonsproject.org/', 'http://bottlepy.org', 'http://tornadoweb.org', 'http://flask.pocoo.org/', 'http://www.web2py.com/', 'http://wiki.python.org/moin/TkInter', 'https://wiki.gnome.org/Projects/PyGObject', 'http://www.riverbankcomputing.co.uk/software/pyqt/intro', 'https://wiki.qt.io/PySide', 'https://kivy.org/', 'http://www.wxpython.org/', 'https://dearpygui.readthedocs.io/en/latest/', 'http://www.scipy.org', 'http://pandas.pydata.org/', 'http://ipython.org', 'http://buildbot.net/', 'http://trac.edgewall.org/', 'http://roundup.sourceforge.net/', 'http://www.ansible.com', 'https://saltproject.io', 'https://www.openstack.org', 'https://xon.sh', '/dev/peps/', '/dev/peps/peps.rss']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='>>> Python Software Foundation\\n\\nThe mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers. Learn more\\n\\nBecome a Member\\r\\n    Donate to the PSF', metadata={'emphasized_text_contents': \"['>>>']\", 'emphasized_text_tags': \"['span']\", 'filetype': 'text/html', 'link_texts': \"['Python Software Foundation', 'Learn more', 'Become a Member', 'Donate to the PSF']\", 'link_urls': \"['/psf/', '/psf/', '/users/membership/', '/psf/donations/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='▲ Back to Top\\n\\nAbout\\n        \\n            \\n\\n\\n    \\n        Applications\\n    \\n        Quotes\\n    \\n        Getting Started\\n    \\n        Help\\n    \\n        Python Brochure\\n\\nDownloads\\n        \\n            \\n\\n\\n    \\n        All releases\\n    \\n        Source code\\n    \\n        Windows\\n    \\n        macOS\\n    \\n        Other Platforms\\n    \\n        License\\n    \\n        Alternative Implementations', metadata={'emphasized_text_contents': \"['▲', '▲']\", 'emphasized_text_tags': \"['span', 'span']\", 'filetype': 'text/html', 'link_texts': \"[None, 'About', 'Applications', 'Quotes', 'Getting Started', 'Help', 'Python Brochure', 'Downloads', 'All releases', 'Source code', 'Windows', 'macOS', 'Other Platforms', 'License', 'Alternative Implementations']\", 'link_urls': \"['#python-network', '/about/', '/about/apps/', '/about/quotes/', '/about/gettingstarted/', '/about/help/', 'http://brochure.getpython.info/', '/downloads/', '/downloads/', '/downloads/source/', '/downloads/windows/', '/downloads/macos/', '/download/other/', 'https://docs.python.org/3/license.html', '/download/alternatives']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content=\"Documentation\\n        \\n            \\n\\n\\n    \\n        Docs\\n    \\n        Audio/Visual Talks\\n    \\n        Beginner's Guide\\n    \\n        Developer's Guide\\n    \\n        FAQ\\n    \\n        Non-English Docs\\n    \\n        PEP Index\\n    \\n        Python Books\\n    \\n        Python Essays\", metadata={'filetype': 'text/html', 'link_texts': '[\\'Documentation\\', \\'Docs\\', \\'Audio/Visual Talks\\', \"Beginner\\'s Guide\", \"Developer\\'s Guide\", \\'FAQ\\', \\'Non-English Docs\\', \\'PEP Index\\', \\'Python Books\\', \\'Python Essays\\']', 'link_urls': \"['/doc/', '/doc/', '/doc/av', 'https://wiki.python.org/moin/BeginnersGuide', 'https://devguide.python.org/', 'https://docs.python.org/faq/', 'http://wiki.python.org/moin/Languages', 'https://peps.python.org', 'https://wiki.python.org/moin/PythonBooks', '/doc/essays/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='Community\\n        \\n            \\n\\n\\n    \\n        Diversity\\n    \\n        Mailing Lists\\n    \\n        IRC\\n    \\n        Forums\\n    \\n        PSF Annual Impact Report\\n    \\n        Python Conferences\\n    \\n        Special Interest Groups\\n    \\n        Python Logo\\n    \\n        Python Wiki\\n    \\n        Code of Conduct\\n    \\n        Community Awards\\n    \\n        Get Involved\\n    \\n        Shared Stories', metadata={'filetype': 'text/html', 'link_texts': \"['Community', 'Diversity', 'Mailing Lists', 'IRC', 'Forums', 'PSF Annual Impact Report', 'Python Conferences', 'Special Interest Groups', 'Python Logo', 'Python Wiki', 'Code of Conduct', 'Community Awards', 'Get Involved', 'Shared Stories']\", 'link_urls': \"['/community/', '/community/diversity/', '/community/lists/', '/community/irc/', '/community/forums/', '/psf/annual-report/2021/', '/community/workshops/', '/community/sigs/', '/community/logos/', 'https://wiki.python.org/moin/', '/psf/conduct/', '/community/awards', '/psf/get-involved/', '/psf/community-stories/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='Success Stories\\n        \\n            \\n\\n\\n    \\n        Arts\\n    \\n        Business\\n    \\n        Education\\n    \\n        Engineering\\n    \\n        Government\\n    \\n        Scientific\\n    \\n        Software Development\\n\\nNews\\n        \\n            \\n\\n\\n    \\n        Python News\\n    \\n        PSF Newsletter\\n    \\n        PSF News\\n    \\n        PyCon US News\\n    \\n        News from the Community', metadata={'filetype': 'text/html', 'link_texts': \"['Success Stories', 'Arts', 'Business', 'Education', 'Engineering', 'Government', 'Scientific', 'Software Development', 'News', 'Python News', 'PSF Newsletter', 'PSF News', 'PyCon US News', 'News from the Community']\", 'link_urls': \"['/success-stories/', '/success-stories/category/arts/', '/success-stories/category/business/', '/success-stories/category/education/', '/success-stories/category/engineering/', '/success-stories/category/government/', '/success-stories/category/scientific/', '/success-stories/category/software-development/', '/blogs/', '/blogs/', '/psf/newsletter/', 'http://pyfound.blogspot.com/', 'http://pycon.blogspot.com/', 'http://planetpython.org/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content=\"Events\\n        \\n            \\n\\n\\n    \\n        Python Events\\n    \\n        User Group Events\\n    \\n        Python Events Archive\\n    \\n        User Group Events Archive\\n    \\n        Submit an Event\\n\\nContributing\\n        \\n            \\n\\n\\n    \\n        Developer's Guide\\n    \\n        Issue Tracker\\n    \\n        python-dev list\\n    \\n        Core Mentorship\\n    \\n        Report a Security Issue\", metadata={'filetype': 'text/html', 'link_texts': '[\\'Events\\', \\'Python Events\\', \\'User Group Events\\', \\'Python Events Archive\\', \\'User Group Events Archive\\', \\'Submit an Event\\', \\'Contributing\\', \"Developer\\'s Guide\", \\'Issue Tracker\\', \\'python-dev list\\', \\'Core Mentorship\\', \\'Report a Security Issue\\']', 'link_urls': \"['/events/', '/events/python-events/', '/events/python-user-group/', '/events/python-events/past/', '/events/python-user-group/past/', 'https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event', '/dev/', 'https://devguide.python.org/', 'https://github.com/python/cpython/issues', 'https://mail.python.org/mailman/listinfo/python-dev', '/dev/core-mentorship/', '/dev/security/']\", 'page_number': 1, 'url': 'https://www.python.org/'}),\n Document(page_content='▲ Back to Top\\n\\nHelp & General Contact\\n\\nDiversity Initiatives\\n\\nSubmit Website Bug\\n\\nStatus\\n\\nCopyright ©2001-2024.\\n                            \\xa0Python Software Foundation\\n                            \\xa0Legal Statements\\n                            \\xa0Privacy Policy', metadata={'emphasized_text_contents': \"['▲', '▲', 'General', 'Initiatives', 'Copyright ©2001-2024.', 'Python Software Foundation', 'Legal Statements', 'Privacy Policy']\", 'emphasized_text_tags': \"['span', 'span', 'span', 'span', 'span', 'span', 'span', 'span']\", 'filetype': 'text/html', 'link_texts': \"[None, 'Help & ', 'Diversity ', 'Submit Website Bug', 'Status ', 'Python Software Foundation', 'Legal Statements', 'Privacy Policy']\", 'link_urls': \"['#python-network', '/about/help/', '/community/diversity/', 'https://github.com/python/pythondotorg/issues', 'https://status.python.org/', '/psf-landing/', '/about/legal/', '/privacy/']\", 'page_number': 1, 'url': 'https://www.python.org/'})]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:39:41.490392400Z",
     "start_time": "2024-05-02T10:39:41.473730Z"
    }
   },
   "id": "8a8084fef0508a61"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully.\n"
     ]
    }
   ],
   "source": [
    "IRA_2.connect_to_vectorstore(collection_name=\"sample_html\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:39:43.437314600Z",
     "start_time": "2024-05-02T10:39:43.421564500Z"
    }
   },
   "id": "e9dd43a7dfeebdb8"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 14:09:46,856 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:09:47,394 - INFO - Document added successfully to the vector store.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Indox.vectorstore.ChromaVectorStore at 0x1f71ca9c380>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRA_2.store_in_vectorstore(chunks=chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:39:47.404484500Z",
     "start_time": "2024-05-02T10:39:45.452534800Z"
    }
   },
   "id": "8036ff896fd6b3be"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "query = \"what is python?\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:39:48.468824700Z",
     "start_time": "2024-05-02T10:39:48.457700500Z"
    }
   },
   "id": "6de42126f942ba40"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 14:09:55,891 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 14:09:59,735 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = IRA_2.answer_question(query=query,top_k=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:39:59.738070100Z",
     "start_time": "2024-05-02T10:39:54.729359900Z"
    }
   },
   "id": "12570df96332e80e"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Python is a programming language that allows users to work quickly and efficiently integrate systems. It is easy to learn and use, making it suitable for both beginners and experienced developers. Python offers a variety of applications, including web development with frameworks like Django and Flask, GUI development with libraries such as tkInter and PyQt, scientific and numeric computing with tools like SciPy and Pandas, software development with platforms like Buildbot and Trac, and system administration with tools like Ansible and OpenStack. Additionally, Python Enhancement Proposals (PEPs) discuss the future of Python. Python's clean syntax and indentation structure make it quick and easy to learn, appealing to both experienced programmers and beginners in other languages.\""
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:40:00.752332500Z",
     "start_time": "2024-05-02T10:40:00.722298400Z"
    }
   },
   "id": "39388e5021019c8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indox for markdown"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27029a75e214583a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "IRA_3 = IndoxRetrievalAugmentation()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:39:20.445995100Z",
     "start_time": "2024-05-02T11:39:18.796918900Z"
    }
   },
   "id": "823521947a1ed1c2"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'clustering': {'dim': 10, 'threshold': 0.1},\n 'embedding_model': 'openai',\n 'postgres': {'conn_string': 'postgresql+psycopg2://postgres:xxx@localhost:port/db_name'},\n 'prompts': {'summary_model': {'content': 'You are a helpful assistant. Give a detailed summary of the documentation provided'}},\n 'qa_model': {'temperature': 0},\n 'splitter': 'semantic-text-splitter',\n 'summary_model': {'max_tokens': 100,\n  'min_len': 30,\n  'model_name': 'gpt-3.5-turbo-0125'},\n 'vector_store': 'chroma'}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRA_3.config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:39:20.454470200Z",
     "start_time": "2024-05-02T11:39:20.445995100Z"
    }
   },
   "id": "c071a84b0dc2f825"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:10:30,891 - INFO - Reading document from string ...\n",
      "2024-05-02 15:10:30,891 - INFO - Reading document ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,043 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,051 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,059 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,059 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,059 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,059 - INFO - HTML element instance has no attribute type\n",
      "2024-05-02 15:10:31,059 - INFO - HTML element instance has no attribute type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Chunking process\n"
     ]
    }
   ],
   "source": [
    "md = \"README.md\"\n",
    "chunks_md = IRA_3.create_chunks(file_path=md, unstructured=True, content_type=\"md\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:31.929250200Z",
     "start_time": "2024-05-02T11:40:30.725041900Z"
    }
   },
   "id": "8ab65268032ed010"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='inDox : Advance Search and Retrieval Augmentation Generative\\n\\nOverview\\n\\nThis project combines advanced clustering techniques provided by Raptor with the efficient retrieval capabilities of pgvector and other vectorstores. It allows users to interact with and visualize their data in a PostgreSQL database. The solution involves segmenting text data into manageable chunks, enhancing retrieval through a custom model, and providing an interface for querying and retrieving relevant information.', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='Prerequisites\\n\\nBefore you can run this project, you need the following installed:\\n- Python 3.8+\\n- PostgreSQL (if you want to store your data on postgres)\\n- OpenAI API Key (if using OpenAI embedding model)\\n\\nEnsure your system also meets the following requirements:\\n- Access to environmental variables for sensitive information (API keys).\\n- Suitable hardware to support intensive computational tasks.\\n\\nInstallation\\n\\nClone the repository and navigate to the directory:', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='bash\\ngit clone https://github.com/osllmai/inDox.git\\ncd inDox\\n\\nInstall the required Python packages:\\n\\nbash\\npip install -r requirements.txt\\n\\nConfiguration\\n\\nEnvironment Variables\\n\\nSet your OPENAI_API_KEY in your environment variables for secure access.\\n\\nDatabase Setup\\n\\nEnsure your PostgreSQL database is up and running, and accessible from your application. (if you are going to use pgvector as your vectorstore)\\n\\nUsage', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content=\"Preparing Your Data\\n\\nDefine the File Path: Specify the path to your text or PDF file.\\n\\nLoad Embedding Models: Initialize your embedding model from OpenAI's selection of pre-trained models.\\n\\nClustering and Retrieval\\n\\nInitialize the Retrieval System\", metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='python\\nfrom Indox import IndoxRetrievalAugmentation\\nIRA = IndoxRetrievalAugmentation(re_chunk=False)\\n\\nin the above code, re_chunk is an argument of class IndoxRetrievalAugmentation that specifies if you want to perform re chunking or not. if you enable it, chunking will happen after each summarization, if not, the ckunking will happen only in start of the process. so you also can initialize the code this way:\\n\\npython\\nIRA = IndoxRetrievalAugmentation(re_chunk=True)', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='Initialize from your own configuration:\\n\\n```python\\nconfig = {\"clustering\": {\"dim\": 10, \"threshold\": 0.1},\\n\"postgres\": {\"conn_string\": \\'postgresql+psycopg2://postgres:xxx@localhost:port/da_name\\'},\\n          \"qa_model\": {\"temperature\": 0}, \"summary_model\": {\"max_tokens\": 100,\\n\"min_len\": 30, \"model_name\": \"gpt-3.5-turbo-0125\"}, \"vector_store\": \"pgvector\", \"embedding_model\": \"openai\",\\n\"splitter\": \"semantic-text-splitter\"}', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content=\"IRA = IndoxRetrievalAugmentation.from_config(config=config, re_chunk=False)\\n```\\nNote: You need to change postgres config to your postgres credentials if you set vector_store to pgvector\\n\\nlet's examine the config dictionary and its properties:\\n\\nclustering\\n\\ndim: Specifies dimension of clustering\", metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='threshold: Specifies threshold of clustering. if this number is low, more samples will be clustered together. in other words, if you increase this parameter, the number of clusters will increase but size of them will decrease.\\n\\npostgres\\n\\nconn_string: Credentials of your postgres database\\n\\nqa_model', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='temperature: The temperature of Question Answering model. if this parameter is high, the diversity of the would be high but probability of nonsense and hallucinations would incearse, and if this parameter is low, the diversity of the output would be low but also probability of hallucinations would decrease.\\n\\nsummary_model\\n\\nmax_tokens: Specifies max number of tokens that summary model could generate. more tokens means more cost and also potentially more quality.', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='min_len: Minimum number of tokens that summary model generates.\\n\\nmodel_name: The model you want to use as your summary model. the defualt is gpt-3.5-turbo-0125. you can use huggingface models that support summarize pipeline, for example, you can use:\\n\\npython\\n  {\"summary_model\": {\"max_tokens\":   100, \"min_len\": 30, \"model_name\": \"Falconsai/medical_summarization\"}}\\n- vector_store: Specify which vectorstore you want to use. defualt is pgvector, but you also can use \"chroma\" and \"faiss\" instead:', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='```python\\n{\"vector_store\": \"chroma\"}\\n\\n```\\n-  embedding_model: The model that is used to embed text. the defualt is openai embeddings, but you can also use \"SBert\" instead:\\n\\npython\\n{\"embedding_model\": \"SBert\"}\\n- splitter: The algorithm used for splitting the text. options are raptor-text-splitter and semantic-text-splitter.', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='Generate Chunks\\n\\npython\\nall_chunks = indox.create_chunks_from_document(docs=\\'./sample.txt\\', max_chunk_size=100)\\nprint(\"Chunks:\", all_chunks)\\n- The max_chunk_size parameter specifies max number of tokens in each chunk.\\n\\nPostgreSQL Setup with pgvector\\n\\nInstall pgvector: To install pgvector on your PostgreSQL server, follow the detailed installation instructions available on the official pgvector GitHub repository:', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content=\"pgvector Installation Instructions\\n\\nAdd Vector Extension:\\n   Connect to your PostgreSQL database and run the following SQL command to create the pgvector extension:\\n\\n```sql\\n-- Connect to your database\\npsql -U username -d database_name\\n\\n-- Run inside your psql terminal\\nCREATE EXTENSION vector;\\n\\nReplace the placeholders with your actual PostgreSQL credentials and details\\n\\n```\\n\\nFirst, you need to connect to the vectorstore\\n\\npython\\nindox.connect_to_vectorstore(collection_name='sample_c')\\n\\nStore in PostgreSQL\", metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='```python\\n\\nyou need to set your database credentials in th config.yaml file\\n\\nindox.store_in_vectorstore(all_chunks)\\n```', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='Querying\\n\\nLastly, we can use the IRA and asnwer to queries using answer_question function from IRA object.\\n\\npython\\nresponse, scores, context = IRA.answer_question(query=\"How did Cinderella reach her happy ending?\", top_k=5)\\nprint(\"Responses:\", response)\\nprint(\"Retrieve chunks:\", context)\\nprint(\"Scores:\", scores)\\n- the top_k argument speficies how many similar documents will be returned from vectorstore.', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1}),\n Document(page_content='Roadmap\\n\\n[x] vector stores\\n\\n[x] pgvector\\n\\n[x] chromadb\\n\\n[x] faiss\\n\\n[x] summary models\\n\\n[x] openai chatgpt\\n\\n[x] huggingface models\\n\\n[x] embedding models\\n\\n[x] openai embeddings\\n\\n[x] sentence transformer embeddings\\n\\n[x] chunking strategies\\n\\n[x] semantic chunking\\n\\n[x] add unstructured support\\n\\n[x] add simple RAG support\\n\\n[ ] cleaning pipeline', metadata={'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2024-04-30T15:43:37', 'page_number': 1})]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_md"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:32.926573800Z",
     "start_time": "2024-05-02T11:40:32.910854700Z"
    }
   },
   "id": "30e497ea950aa46b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:10:35,355 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully.\n"
     ]
    }
   ],
   "source": [
    "IRA_3.connect_to_vectorstore(collection_name=\"sample_md\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:35.574331800Z",
     "start_time": "2024-05-02T11:40:34.016890700Z"
    }
   },
   "id": "19b660ea90bbe8d5"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:10:37,479 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 15:10:37,970 - INFO - Document added successfully to the vector store.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Indox.vectorstore.ChromaVectorStore at 0x1de74bb42f0>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRA_3.store_in_vectorstore(chunks=chunks_md)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:37.974684800Z",
     "start_time": "2024-05-02T11:40:35.574331800Z"
    }
   },
   "id": "e4492b7602ac40cd"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "query = \"how summary model works?\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:37.982845200Z",
     "start_time": "2024-05-02T11:40:37.974684800Z"
    }
   },
   "id": "3d0954bdd89bc53f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:10:38,384 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-05-02 15:10:41,781 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = IRA_3.answer_question(query=query,top_k=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:41.790995700Z",
     "start_time": "2024-05-02T11:40:37.978192400Z"
    }
   },
   "id": "9b12c5553012754d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'The summary model works by generating a concise summary of a given input text based on specified parameters such as minimum and maximum number of tokens, model name, and temperature. The model condenses the input text into a shorter version while maintaining the key information. The summary model can be customized by adjusting parameters such as min_len, max_tokens, and model_name to control the length and quality of the generated summary. The temperature parameter influences the diversity of the output, with higher values leading to more diverse but potentially nonsensical summaries, and lower values resulting in more focused but less diverse summaries.'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:41.790995700Z",
     "start_time": "2024-05-02T11:40:41.786717400Z"
    }
   },
   "id": "af853ed367efd94b"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(['min_len: Minimum number of tokens that summary model generates.\\n\\nmodel_name: The model you want to use as your summary model. the defualt is gpt-3.5-turbo-0125. you can use huggingface models that support summarize pipeline, for example, you can use:\\n\\npython\\n  {\"summary_model\": {\"max_tokens\":   100, \"min_len\": 30, \"model_name\": \"Falconsai/medical_summarization\"}}\\n- vector_store: Specify which vectorstore you want to use. defualt is pgvector, but you also can use \"chroma\" and \"faiss\" instead:',\n  'temperature: The temperature of Question Answering model. if this parameter is high, the diversity of the would be high but probability of nonsense and hallucinations would incearse, and if this parameter is low, the diversity of the output would be low but also probability of hallucinations would decrease.\\n\\nsummary_model\\n\\nmax_tokens: Specifies max number of tokens that summary model could generate. more tokens means more cost and also potentially more quality.',\n  'Roadmap\\n\\n[x] vector stores\\n\\n[x] pgvector\\n\\n[x] chromadb\\n\\n[x] faiss\\n\\n[x] summary models\\n\\n[x] openai chatgpt\\n\\n[x] huggingface models\\n\\n[x] embedding models\\n\\n[x] openai embeddings\\n\\n[x] sentence transformer embeddings\\n\\n[x] chunking strategies\\n\\n[x] semantic chunking\\n\\n[x] add unstructured support\\n\\n[x] add simple RAG support\\n\\n[ ] cleaning pipeline'],\n [1.0046952962875366, 1.1417709589004517, 1.3817675113677979])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:41.819683800Z",
     "start_time": "2024-05-02T11:40:41.790995700Z"
    }
   },
   "id": "8eca2baf8a0d86c"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertScore scores:\n",
      "   Precision@3: 0.4171\n",
      "   Recall@3: 0.4959\n",
      "   F1@3: 0.5013\n"
     ]
    }
   ],
   "source": [
    "IRA_3.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:53.153154300Z",
     "start_time": "2024-05-02T11:40:45.781513800Z"
    }
   },
   "id": "f3fd0f01268c2587"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    Overview of All Tokens Used:\n",
      "                    Tokens used in the embedding section that were sent to the database: 1358\n",
      "                               \n"
     ]
    }
   ],
   "source": [
    "IRA_3.get_tokens_info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:40:53.153154300Z",
     "start_time": "2024-05-02T11:40:53.148969700Z"
    }
   },
   "id": "87c16ebbbffa852f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7465d95602cb8b8e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
