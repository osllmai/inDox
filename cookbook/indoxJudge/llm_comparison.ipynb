{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78d284f7aaa4b0e",
   "metadata": {},
   "source": [
    "# LLMComparison\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxJudge/llm_comparison.ipynb)\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Defining Model Data:** The `models` list contains dictionaries representing three different language models, each with a name, overall score, and detailed evaluation metrics. These metrics include Faithfulness, Answer Relevancy, Bias, Hallucination, Knowledge Retention, Toxicity, Precision, Recall, F1 Score, and BLEU. The `score` key represents the overall evaluation score for each model.\n",
    "\n",
    "- **Importing LLMComparison:** The `LLMComparison` class from the `indoxJudge.piplines` module is imported. This class is used to compare the performance of the different language models based on their metrics.\n",
    "\n",
    "- **Initializing the Comparison:** An instance of `LLMComparison` is created by passing the `models` list to it. This instance, `llm_comparison`, will handle the comparison of the models.\n",
    "\n",
    "- **Plotting the Comparison:** The `plot` method is called with `mode=\"inline\"` to generate and display a comparative visualization of the models' performance within the notebook. This is especially useful for users working in environments like Google Colab, where inline plotting is preferred for ease of use.\n",
    "\n",
    "This cell is designed to compare multiple language models visually, allowing for a detailed analysis of their respective strengths and weaknesses across various metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1566148beae1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install indoxJudge transformers dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf97b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check indoxJudge version\n",
    "import indoxJudge\n",
    "indoxJudge.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T05:02:47.320123Z",
     "start_time": "2024-09-10T05:02:47.313258Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [{'name': 'llama3',\n",
    "  'score': 0.50,\n",
    "  'metrics': {'Faithfulness': 0.55,\n",
    "   'AnswerRelevancy': 1.0,\n",
    "   'Bias': 0.45,\n",
    "   'Hallucination': 0.8,\n",
    "   'KnowledgeRetention': 0.0,\n",
    "   'Toxicity': 0.0,\n",
    "   'precision': 0.64,\n",
    "   'recall': 0.77,\n",
    "   'f1_score': 0.70,\n",
    "   'BLEU': 0.11}},\n",
    " {'name': 'OpenAi',\n",
    "  'score': 0.61,\n",
    "  'metrics': {'Faithfulness': 1.0,\n",
    "   'AnswerRelevancy': 1.0,\n",
    "   'Bias': 0.0,\n",
    "   'Hallucination': 0.8,\n",
    "   'KnowledgeRetention': 1.0,\n",
    "   'Toxicity': 0.0,\n",
    "   'precision': 0.667,\n",
    "   'recall': 0.77,\n",
    "   'f1_score': 0.71,\n",
    "   'BLEU': 0.14}},\n",
    " {'name': 'Gemini',\n",
    "  'score': 0.050,\n",
    "  'metrics': {'Faithfulness': 1.0,\n",
    "   'AnswerRelevancy': 1.0,\n",
    "   'Bias': 0.0,\n",
    "   'Hallucination': 0.83,\n",
    "   'KnowledgeRetention': 0.0,\n",
    "   'Toxicity': 0.0,\n",
    "   'precision': 0.64,\n",
    "   'recall': 0.76,\n",
    "   'f1_score': 0.70,\n",
    "   'BLEU': 0.10}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e4e907ed7e31dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T05:02:47.619039Z",
     "start_time": "2024-09-10T05:02:47.607835Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2e0d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAiModel' from 'indoxJudge.models' (c:\\Users\\llmserver\\.conda\\envs\\indox\\Lib\\site-packages\\indoxJudge\\models\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mindoxJudge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAiModel\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'OpenAiModel' from 'indoxJudge.models' (c:\\Users\\llmserver\\.conda\\envs\\indox\\Lib\\site-packages\\indoxJudge\\models\\__init__.py)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from indoxJudge.models import OpenAiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b3b7e31cb152107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T05:02:48.208781Z",
     "start_time": "2024-09-10T05:02:47.911991Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mindoxJudge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m openai\n\u001b[1;32m----> 2\u001b[0m judge \u001b[38;5;241m=\u001b[39m openai(api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "from indoxJudge.models import openai\n",
    "judge = openai(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b135f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T05:03:47.743260Z",
     "start_time": "2024-09-10T05:03:41.505104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"radar_chart\": \"OpenAi demonstrates the strongest overall performance, achieving perfect scores in Faithfulness, AnswerRelevancy, and KnowledgeRetention. In contrast, llama3 shows significant weaknesses in KnowledgeRetention and has a high Hallucination score. Gemini matches OpenAi in Faithfulness and AnswerRelevancy but has a slightly higher Hallucination score, indicating it also has areas needing improvement.\",\n",
      "\n",
      "    \"bar_chart\": \"OpenAi leads in precision (0.667) and f1_score (0.71), while both llama3 and Gemini lag slightly behind with f1_scores of 0.70. In recall, all models are closely matched, with llama3 and OpenAi both at 0.77. OpenAi clearly excels in precision and f1_score, marking it as the top performer in these metrics.\",\n",
      "\n",
      "    \"scatter_plot\": \"The scatter plot reveals that precision and recall are closely clustered for all models, indicating a balanced performance. OpenAi has a slight advantage in precision, while llama3 and Gemini maintain similar positions, suggesting no significant trade-offs between these two metrics for any model.\",\n",
      "\n",
      "    \"line_plot\": \"The line plot of BLEU scores indicates that OpenAi has the highest BLEU score (0.14), followed by llama3 (0.11) and Gemini (0.10). This trend shows that OpenAi consistently outperforms the others in translation-related metrics, while the other models show less variation.\",\n",
      "\n",
      "    \"heatmap\": \"The heatmap indicates a strong negative correlation between Faithfulness and Hallucination across all models. As Faithfulness improves, Hallucination tends to decrease. Additionally, there is a positive correlation between precision, f1_score, and recall, with OpenAi showing the strongest overall performance in these areas.\",\n",
      "\n",
      "    \"gauge_chart\": \"OpenAi achieves a perfect score in Faithfulness, indicating superior performance, while llama3 has a significantly lower score in this area. Gemini also scores perfectly in Faithfulness but has a higher Hallucination score, suggesting it has more room for improvement compared to OpenAi.\",\n",
      "\n",
      "    \"table\": \"Overall, OpenAi consistently outperforms the other models across most metrics, particularly in Faithfulness, AnswerRelevancy, and KnowledgeRetention. Llama3 struggles with KnowledgeRetention and has a high Hallucination score, while Gemini is more balanced but still has areas needing improvement. OpenAi emerges as the most reliable model across all metrics, making it the top-performing model based on the aggregated results.\"\n",
      "}\n",
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "source": [
    "from indoxJudge.pipelines import EvaluationAnalyzer\n",
    "llm_comparison = EvaluationAnalyzer(models=models)\n",
    "llm_comparison.plot(interpreter=judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91981dc938dc0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
