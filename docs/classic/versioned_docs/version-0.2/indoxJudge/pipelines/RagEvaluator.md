# Rag Evaluator

## Overview

The `RagEvaluator` class is designed to evaluate various aspects of language model outputs in the context of Retrieval-Augmented Generation (RAG) using specified metrics. It supports metrics such as Faithfulness, Answer Relevancy, Contextual Relevancy, GEval, Hallucination, Knowledge Retention, BertScore, and METEOR. This class provides a comprehensive assessment of different dimensions of RAG model performance, making it ideal for thorough evaluations of retrieval-based language models.

## Initialization

The `RagEvaluator` class is initialized with four main components:

- **llm_as_judge**: The language model acting as the judge for the evaluation.
- **llm_response**: The response generated by the language model.
- **retrieval_context**: The context retrieved for the query.
- **query**: The query or prompt provided to the language model.

### Example

```python
class RagEvaluator:
    def __init__(self, llm_as_judge, llm_response, retrieval_context, query):
        """
        Initializes the RagEvaluator with a language model and a list of metrics.

        Args:
            llm_as_judge: The language model acting as the judge for the evaluation.
            llm_response: The response generated by the language model.
            retrieval_context: The context retrieved for the query.
            query: The query or prompt provided to the language model.
        """
        self.metrics = [
            Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context),
            AnswerRelevancy(query=query, llm_response=llm_response),
            ContextualRelevancy(query=query, retrieval_context=retrieval_context),
            GEval(parameters="Rag Pipeline", llm_response=llm_response, query=query, retrieval_context=retrieval_context),
            Hallucination(llm_response=llm_response, retrieval_context=retrieval_context),
            KnowledgeRetention(messages=[{"query": query, "llm_response": llm_response}]),
            BertScore(llm_response=llm_response, retrieval_context=retrieval_context),
            METEOR(llm_response=llm_response, retrieval_context=retrieval_context),
        ]
```

## Setting the Model for Metrics

The `set_model_for_metrics` method ensures that the language model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model for evaluation purposes.

## Judging

The `judge` method evaluates the language model using the provided metrics and returns the results. It processes each metric individually, handling specific evaluation logic for different metric types.

## Usage Example

```python
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

from indoxJudge.pipelines import RagEvaluator
from indoxJudge.models import OpenAi

query = "What are the benefits of a Mediterranean diet?"
retrieval_context = [
    "The Mediterranean diet emphasizes eating primarily plant-based foods...",
    "Research has shown that the Mediterranean diet can reduce the risk of heart disease...",
    "A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases..."
]

response = "The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease..."

llm_as_judge = OpenAi(api_key=OPENAI_API_KEY, model="gpt-4o")

evaluator = RagEvaluator(llm_as_judge=llm_as_judge, llm_response=response, retrieval_context=retrieval_context, query=query)
rag_results = evaluator.judge()

evaluation_score = evaluator.evaluation_score
evaluation_metrics_score = evaluator.metrics_score

# For plot and visualize (choose inline for using in colab)
evaluator.plot(mode="external")
```

In this example, the `RagEvaluator` class method `judge` initializes an empty dictionary called `results`. It iterates through each metric in the `metrics` list, retrieves the metric's name, and logs the start of the evaluation process. For each metric, the method checks its type and calls the appropriate evaluation method. The results are stored in the `results` dictionary under the corresponding metric key.
If any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the evaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the evaluations for each metric.

## Visualization

The `plot` method allows for visualization of the evaluation results. It creates a graph representation of the RAG Evaluator's performance across different metrics.
