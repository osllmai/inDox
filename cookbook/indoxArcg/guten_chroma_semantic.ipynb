{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b8b50c8955b671b",
   "metadata": {},
   "source": [
    "# Using GutenbergReader for Accessing Project Gutenberg Books\n",
    "In this notebook, we will demonstrate how to use the `GutenbergReader` class for accessing books from Project Gutenberg. The `GutenbergReader` class interacts with the Project Gutenberg website to retrieve book content and metadata, which can be valuable for various applications, including research and question-answering systems.\n",
    "\n",
    "To get started, ensure you have your Project Gutenberg book IDs ready for fetching the content. This setup is crucial for accessing and processing book data effectively.\n",
    "\n",
    "To begin, ensure you have set up your environment variables and API keys in Python using the dotenv library. This is crucial for securely managing sensitive information, such as API keys, especially when using services like HuggingFace. Ensure your `HUGGINGFACE_API_KEY` is defined in the `.env` file to avoid hardcoding sensitive data into your codebase, thus enhancing security and maintainability.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxArcg/guten_chroma_semantic.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b1185e6566f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb beautifulsoup4 sentence_transformers semantic_text_splitter indoxArcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34666a88534518cf",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indoxArcg`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indoxArcg\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indoxArcg\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indoxArcg\n",
    "    ```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indoxArcg/bin/activate\n",
    "    ```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961379b71c5d3fc",
   "metadata": {},
   "source": [
    "## Import Essential Libraries\n",
    "\n",
    "Next, we import the essential libraries for our Indox question-answering system:\n",
    "\n",
    "- `IndoxRetrievalAugmentation`: Enhances the retrieval process by improving the relevance and quality of the documents retrieved, leading to better QA performance.\n",
    "- `MistralQA`: A powerful QA model provided by Indox, built on top of the Hugging Face model architecture. It leverages state-of-the-art language understanding to deliver precise answers.\n",
    "- `HuggingFaceEmbedding`: This library uses Hugging Face embeddings to enrich semantic understanding, making it easier to capture the contextual meaning of the text.\n",
    "- `SemanticTextSplitter`: utilizes a Hugging Face tokenizer to intelligently split text into chunks based on a specified maximum number of tokens, ensuring that each chunk maintains semantic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9daa14417b6e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:26:00.925099Z",
     "start_time": "2024-08-27T07:26:00.918099Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631854ff3240f6a",
   "metadata": {},
   "source": [
    "## Building the GutenbergReader System and Initializing Models\n",
    "Next, we will build our `GutenbergReader` system and initialize the necessary models for processing content from Project Gutenberg. This setup will enable us to effectively retrieve and handle texts from Gutenberg's collection, leveraging these models to support various research and question-answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9a7a29fc81465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:26:08.393142Z",
     "start_time": "2024-08-27T07:26:05.295511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing HuggingFaceAPIModel with model: mistralai/Mistral-7B-Instruct-v0.2\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mHuggingFaceAPIModel initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized HuggingFaceEmbedding with model: multi-qa-mpnet-base-cos-v1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.llms import HuggingFaceAPIModel\n",
    "from indoxArcg.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "mistral_qa = HuggingFaceAPIModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "embed = HuggingFaceEmbedding(api_key=HUGGINGFACE_API_KEY,model=\"multi-qa-mpnet-base-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29659851115c85",
   "metadata": {},
   "source": [
    "## Setting Up the GutenbergReader for Retrieving Book Content\n",
    "To demonstrate the capabilities of our `GutenbergReader` system and its integration with `Indox`, we will use a sample book from Project Gutenberg. This book will serve as our reference data, which we will use for testing and evaluation of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ba25cec5cb1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:28:15.003652Z",
     "start_time": "2024-08-27T07:28:12.627524Z"
    }
   },
   "outputs": [],
   "source": [
    "from indoxArcg.data_connectors import GutenbergReader\n",
    "\n",
    "reader = GutenbergReader()\n",
    "\n",
    "book_id = \"11\"  # Alice's Adventures in Wonderland\n",
    "content = reader.get_content(book_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b838e3518614d",
   "metadata": {},
   "source": [
    "## Splitting Content into Manageable Chunks\n",
    "We use the `SemanticTextSplitter` function from the `indox.splitter` module to divide the retrieved content into smaller, meaningful chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835fe1c42c4e31df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:28:18.404176Z",
     "start_time": "2024-08-27T07:28:16.353765Z"
    }
   },
   "outputs": [],
   "source": [
    "from indoxRag.splitter import SemanticTextSplitter\n",
    "splitter = SemanticTextSplitter(400)\n",
    "content_chunks = splitter.split_text(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4ec0584b71cc9",
   "metadata": {},
   "source": [
    "## Storing and Indexing Content with Chroma\n",
    "We use the `Chroma` vector store from the `indox.vector_stores` module to store and index the content chunks. By creating a collection named \"sample\" and applying an embedding function (`embed`), we convert each chunk into a vector for efficient retrieval. The `add` method then adds these vectors to the database, enabling scalable and effective search for question-answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636c3ad55b48d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:28:25.009706Z",
     "start_time": "2024-08-27T07:28:21.355330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mStoring documents in the vector store\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocument added successfully to the vector store.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocuments stored successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.vector_stores import Chroma\n",
    "db = Chroma(collection_name=\"sample\",embedding_function=embed)\n",
    "db.add(docs=content_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5aaa53fce659c4",
   "metadata": {},
   "source": [
    "## Querying Project Gutenberg Data with Indox\n",
    "With our `GutenbergReader` system and `Indox` fully set up, we are ready to test it using a sample query. This test will demonstrate how effectively our system can retrieve and process information from books in the Project Gutenberg collection.\n",
    "\n",
    "We’ll use the following sample query to evaluate our system:\n",
    "\n",
    "- Query: \"Who is the speaker talking to in the text?\"\n",
    "\n",
    "This question will be processed by the `GutenbergReader` and `Indox` system to retrieve relevant book content and generate an accurate response based on the text.\n",
    "\n",
    "Let’s put our setup to the test with this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9e7053437d60ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:28:33.660564Z",
     "start_time": "2024-08-27T07:28:33.649570Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"Who is the speaker talking to in the text?\"\n",
    "from indoxArcg.pipelines.rag import RAG\n",
    "retriever = RAG(llm=mistral_qa,vector_store=db,enable_web_fallback=False,top_k= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20611d34f3b8773",
   "metadata": {},
   "source": [
    "Now that our `GutenbergReader` system with `indoxArcg` is fully set up, we can test it with a sample query. We’ll use the invoke method to get a response from the system.\n",
    "\n",
    "The `infer` method processes the query using the connected QA model and retrieves relevant information from the book content.\n",
    "\n",
    "We’ll pass the query to the `invoke` method and print the response to evaluate how effectively the system retrieves and generates answers based on the Project Gutenberg content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cad8c3510343e8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:28:37.682341Z",
     "start_time": "2024-08-27T07:28:35.895681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving context and scores from the vector database\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating answer without document relevancy filter\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSending request to Hugging Face API\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mReceived successful response from Hugging Face API\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mQuery answered successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = retriever.infer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1edb80591b75581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:28:39.605873Z",
     "start_time": "2024-08-27T07:28:39.593877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The speaker is Alice, and she is talking to the crowd of animals that has gathered around Bill the Rabbit.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
