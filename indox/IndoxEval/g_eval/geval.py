import json

from indox.IndoxEval.g_eval.template import GEvalTemplate


class GEval:
    def __init__(self,parameters, query, llm_response, ground_truth, context, retrieval_context):
        """
        Initialize the GEval class with necessary inputs for evaluation.

        Parameters:
        query (str): The original query or input text.
        llm_response (str): The response generated by the language model.
        ground_truth (str): The expected or correct output.
        context (str): Additional context relevant to the query.
        retrieval_context (str): The context from which information was retrieved.
        """
        self.model = None
        self.query = query
        self.llm_response = llm_response
        self.ground_truth = ground_truth
        self.context = context
        self.retrieval_context = retrieval_context
        self.parameters = parameters
        self.criteria = """
        1. Retrieval Quality: The retrieved documents or snippets should be relevant and accurate.
        2. Integration: The retrieved information should be well integrated into the generated response.
        3. Coherence: The text should be logically structured and easy to follow.
        4. Relevance: The text should be relevant to the main topic and cover all key points.
        5. Accuracy: The text should be factually accurate and consistent with the source material.
        6. Fluency: The text should be easy to read and free from grammatical errors.
        7. Comprehensiveness: The text should cover all key points and provide a thorough response.
        8. Contextuality: The response should fit well within the context of the query.
        """

    def set_model(self, model):
        """
        Set the model to be used for evaluation.

        Parameters:
        model (str): The model to use.
        """
        self.model = model

    def generate_evaluation_steps(self):
        """
        Generate evaluation steps using the provided parameters and criteria.

        Parameters:
        parameters (str): The parameter to be evaluated (e.g., 'summary', 'dialogue').

        Returns:
        str: The prompt to generate evaluation steps.
        """
        eval_steps_prompt = GEvalTemplate.generate_evaluation_steps(self.parameters, self.criteria)
        return eval_steps_prompt

    def generate_evaluation_results(self, eval_steps):
        """
        Generate evaluation results based on the steps and the provided text.

        Parameters:
        eval_steps (list of str): List of evaluation steps.
        parameters (str): The parameter being evaluated.

        Returns:
        str: The prompt to generate evaluation results.
        """
        eval_results_prompt = GEvalTemplate.generate_evaluation_results(eval_steps, {
            "Query": self.query,
            "LLM response": self.llm_response,
            "Ground truth": self.ground_truth,
            "Context": self.context,
            "Retrieval Context": self.retrieval_context
        }, self.parameters)
        return eval_results_prompt

    def _call_language_model(self, prompt: str) -> str:
        response = self.model.generate_evaluation_response(prompt=prompt)
        return response

    def g_eval(self):
        """
        Evaluate the quality of NLG outputs using GPT-4 with the GEvalTemplate.

        Parameters:
        parameters (str): The parameter to be evaluated (e.g., 'summary', 'dialogue').

        Returns:
        list of dict: Evaluation scores and reasons for each text.
        """
        eval_steps_prompt = self.generate_evaluation_steps()
        eval_steps_response = self._call_language_model(eval_steps_prompt)
        eval_steps = json.loads(eval_steps_response)["steps"]

        eval_results_prompt = self.generate_evaluation_results(eval_steps)
        eval_result = self._call_language_model(eval_results_prompt)

        return eval_result
