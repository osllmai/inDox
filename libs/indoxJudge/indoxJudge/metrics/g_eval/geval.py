import json

from .template import GEvalTemplate
from loguru import logger
import sys

# Set up logging
logger.remove()  # Remove the default logger
logger.add(
    sys.stdout, format="<green>{level}</green>: <level>{message}</level>", level="INFO"
)

logger.add(
    sys.stdout, format="<red>{level}</red>: <level>{message}</level>", level="ERROR"
)


class GEval:
    def __init__(
        self,
        parameters,
        query,
        llm_response,
        retrieval_context,
        ground_truth=None,
        context=None,
    ):
        """
        Initialize the GEval class with necessary inputs for evaluation.

        Parameters:
        parameters (str): The parameters or aspects to evaluate (e.g., 'summary', 'dialogue').
        query (str): The original query or input text.
        llm_response (str): The response generated by the language model.
        ground_truth (str): The expected or correct output.
        context (str): Additional context relevant to the query.
        retrieval_context (str): The context from which information was retrieved.
        """
        self.model = None
        self.query = query
        self.llm_response = llm_response
        self.ground_truth = ground_truth if ground_truth is not None else ""
        self.context = context if context is not None else ""
        self.retrieval_context = retrieval_context
        self.parameters = parameters
        self.criteria = """
        1. Retrieval Quality: The retrieved documents or snippets should be relevant and accurate.
        2. Integration: The retrieved information should be well integrated into the generated response.
        3. Coherence: The text should be logically structured and easy to follow.
        4. Relevance: The text should be relevant to the main topic and cover all key points.
        5. Accuracy: The text should be factually accurate and consistent with the source material.
        6. Fluency: The text should be easy to read and free from grammatical errors.
        7. Comprehensiveness: The text should cover all key points and provide a thorough response.
        8. Contextuality: The response should fit well within the context of the query.
        """
        self.total_output_tokens = 0
        self.total_input_tokens = 0

    def set_model(self, model):
        """
        Set the model to be used for evaluation.

        Parameters:
        model (str): The model to use.
        """
        self.model = model

    def generate_evaluation_steps(self):
        """
        Generate evaluation steps using the provided parameters and criteria.

        Parameters:
        parameters (str): The parameter to be evaluated (e.g., 'summary', 'dialogue').

        Returns:
        str: The prompt to generate evaluation steps.
        """
        eval_steps_prompt = GEvalTemplate.generate_evaluation_steps(
            self.parameters, self.criteria
        )
        return eval_steps_prompt

    def generate_evaluation_results(self, eval_steps):
        """
        Generate evaluation results based on the steps and the provided text.

        Parameters:
        eval_steps (list of str): List of evaluation steps.
        parameters (str): The parameter being evaluated.

        Returns:
        str: The prompt to generate evaluation results.
        """
        eval_results_prompt = GEvalTemplate.generate_evaluation_results(
            eval_steps,
            {
                "Query": self.query,
                "LLM response": self.llm_response,
                "Ground truth": self.ground_truth,
                "Context": self.context,
                "Retrieval Context": self.retrieval_context,
            },
            self.parameters,
        )
        return eval_results_prompt

    def _clean_json_response(self, response: str) -> str:
        """
        Cleans the JSON response from the language model by removing markdown code blocks if present.

        :param response: Raw response from the language model
        :return: Cleaned JSON string
        """
        if response.startswith("```json") and response.endswith("```"):
            response = response[7:-3].strip()
        return response

    def _call_language_model(self, prompt: str) -> str:
        import tiktoken

        enc = tiktoken.get_encoding("cl100k_base")
        input_token_count = len(enc.encode(prompt))
        response = self.model.generate_evaluation_response(prompt=prompt)
        self.total_input_tokens += input_token_count

        if not response:
            raise ValueError("Received an empty response from the model.")

        clean_response = self._clean_json_response(response=response)
        output_token_count = len(enc.encode(response))
        self.total_output_tokens += output_token_count
        logger.info(
            f"Token Counts - Input: {input_token_count} | Output: {output_token_count}"
        )

        return clean_response

    def g_eval(self):
        """
        Evaluate the quality of natural language generation outputs using the GEvalTemplate.

        This function orchestrates the evaluation process by generating evaluation steps,
        calling the language model to evaluate those steps, and then gathering the evaluation
        results based on the provided query, LLM response, ground truth, and contexts.

        Returns:
        list of dict: Evaluation scores and reasons for each text.
        """
        eval_steps_prompt = self.generate_evaluation_steps()
        eval_steps_response = self._call_language_model(eval_steps_prompt)
        eval_steps = json.loads(eval_steps_response)["steps"]

        eval_results_prompt = self.generate_evaluation_results(eval_steps)
        eval_result = self._call_language_model(eval_results_prompt)
        logger.info(
            f"Token Usage Summary:\n Total Input: {self.total_input_tokens} | Total Output: {self.total_output_tokens} | Total: {self.total_input_tokens + self.total_output_tokens}"
        )
        return eval_result
