{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:53.132289Z",
     "start_time": "2024-07-24T08:49:53.119738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "INDOX_API_KEY = os.getenv(\"INDOX_API_KEY\")"
   ],
   "id": "ed950b5f2833500a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:53.620255Z",
     "start_time": "2024-07-24T08:49:53.133121Z"
    }
   },
   "source": [
    "from indox.IndoxEval.llms import IndoxApi\n",
    "llm = IndoxApi(api_key=INDOX_API_KEY)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:54.096687Z",
     "start_time": "2024-07-24T08:49:54.093084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_case = {\n",
    "                    \"messages\": [\n",
    "                        {\"query\": \"What are the best books for learning Python?\",\n",
    "                         \"llm_response\": \"Automate the Boring Stuff with Python, Python Crash Course, etc.\"},\n",
    "                        {\"query\": \"What is the capital of France?\", \"llm_response\": \"The capital of France is Paris.\"}\n",
    "                    ]\n",
    "                }"
   ],
   "id": "5e799a33363755e9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:54.427198Z",
     "start_time": "2024-07-24T08:49:54.423147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_messages = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"llm_response\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Can you tell me about the Eiffel Tower?\",\n",
    "        \"llm_response\": \"The Eiffel Tower, located in Paris, is one of the most iconic landmarks in the world.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are some famous foods in France?\",\n",
    "        \"llm_response\": \"France is known for its cuisine, including dishes like croissants, baguettes, and cheese.\"\n",
    "    }\n",
    "]"
   ],
   "id": "1cb70816e2109925",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:54.838628Z",
     "start_time": "2024-07-24T08:49:54.835267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What is the capital of Japan?\"\n",
    "llm_response = \"The capital of Japan is Tokyo.\"\n",
    "retrieval_context = [\"Tokyo is the most populous city in Japan and serves as the country's political and economic center.\",\n",
    "                     \"Tokyo has been the capital since 1868, after the Meiji Restoration moved the capital from Kyoto.\",\n",
    "                     \"The city is known for its mix of modern architecture and traditional temples, as well as its bustling districts like Shibuya and Shinjuku.\"]"
   ],
   "id": "983cd49670fe3394",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:55.227679Z",
     "start_time": "2024-07-24T08:49:55.223760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Faithfulness\n",
    "faithfulness_evaluator = Faithfulness(llm_response=llm_response,retrieval_context=retrieval_context)"
   ],
   "id": "741f1c522cf4a890",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:55.527497Z",
     "start_time": "2024-07-24T08:49:55.523784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import AnswerRelevancy\n",
    "answer_relevancy_metric = AnswerRelevancy(query=query,llm_response=llm_response)"
   ],
   "id": "383c8eb91a2c2532",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:55.827986Z",
     "start_time": "2024-07-24T08:49:55.824382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Bias\n",
    "bias_metric = Bias(llm_response=llm_response)"
   ],
   "id": "ce1de7123674cce6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.104118Z",
     "start_time": "2024-07-24T08:49:56.101015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import ContextualRelevancy\n",
    "contextual = ContextualRelevancy(query=query,retrieval_context=retrieval_context)"
   ],
   "id": "d3e25716ada96525",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.408440Z",
     "start_time": "2024-07-24T08:49:56.404515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import GEval\n",
    "geval = GEval(parameters=\"Rag application\",query=query,llm_response=llm_response,ground_truth=\"Tokyo\",context=\"geographic knowledge\",\n",
    "              retrieval_context=retrieval_context)"
   ],
   "id": "16c2d17a25f3a714",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.654507Z",
     "start_time": "2024-07-24T08:49:56.651101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import KnowledgeRetention\n",
    "knowledge_retention = KnowledgeRetention(messages=test_messages)"
   ],
   "id": "73cd7a7ecae55215",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.928481Z",
     "start_time": "2024-07-24T08:49:56.925478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Hallucination\n",
    "hallucination = Hallucination(llm_response=llm_response,retrieval_context=retrieval_context)"
   ],
   "id": "843c43180fbc887c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:57.229825Z",
     "start_time": "2024-07-24T08:49:57.226918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Toxicity\n",
    "toxicity = Toxicity(messages=test_messages)"
   ],
   "id": "9f34c622dbf50063",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:57.591474Z",
     "start_time": "2024-07-24T08:49:57.587061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.IndoxEval import Evaluator\n",
    "evaluator = Evaluator(model=llm,metrics=[contextual,faithfulness_evaluator,answer_relevancy_metric,\n",
    "                                         hallucination,toxicity,geval,knowledge_retention,bias_metric])"
   ],
   "id": "ddeacee6e56a6cc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluator initialized with model and metrics.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mModel set for all metrics.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:50:51.680754Z",
     "start_time": "2024-07-24T08:49:57.959790Z"
    }
   },
   "cell_type": "code",
   "source": "result = evaluator.evaluate()",
   "id": "4ff8285425030450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: ContextualRelevancy\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: ContextualRelevancy\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: Faithfulness\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: Faithfulness\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: AnswerRelevancy\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: AnswerRelevancy\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: Hallucination\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: Hallucination\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: Toxicity\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: Toxicity\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: GEval\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: GEval\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: KnowledgeRetention\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: KnowledgeRetention\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating metric: Bias\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for metric: Bias\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:50:51.688588Z",
     "start_time": "2024-07-24T08:50:51.681760Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "c1f3abd628e26a79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contextual_relevancy': {'verdicts': [{'verdict': 'yes',\n",
       "    'reason': 'No reason provided'},\n",
       "   {'verdict': 'yes', 'reason': 'No reason provided'},\n",
       "   {'verdict': 'no',\n",
       "    'reason': 'The context provided does not mention anything about the capital of Japan or provide any relevant information related to the question.'}],\n",
       "  'reason': {'reason': 'The score is 0 because the context provided does not contain any information about the capital of Japan, which is Tokyo.'}},\n",
       " 'faithfulness': {'claims': ['The capital of Japan is Tokyo.'],\n",
       "  'truths': ['The capital of Japan is Tokyo.'],\n",
       "  'verdicts': [{'verdict': 'yes', 'reason': None}],\n",
       "  'reason': 'The score is 1 because the actual output perfectly aligns with the retrieval context, stating that the capital of Japan is Tokyo.'},\n",
       " 'answer_relevancy': {'score': 1,\n",
       "  'reason': 'The score is 1.00 because the answer is perfectly relevant to the input provided. Great job!',\n",
       "  'statements': [],\n",
       "  'verdicts': []},\n",
       " 'hallucination': {'score': 0.6666666666666666,\n",
       "  'reason': 'The score is 0.67 because there are contradictions in the actual output compared to the provided context, indicating some level of hallucination in the generated content.',\n",
       "  'verdicts': [{'verdict': 'yes',\n",
       "    'reason': \"The actual output agrees with the provided context which states that Tokyo is the most populous city in Japan and serves as the country's political and economic center.\"},\n",
       "   {'verdict': 'no',\n",
       "    'reason': 'The actual output contradicts the provided context which states that Tokyo has been the capital since 1868, after the Meiji Restoration moved the capital from Kyoto.'},\n",
       "   {'verdict': 'no',\n",
       "    'reason': \"The actual output lacks detail and does not contradict the provided context which describes Tokyo's architecture and districts.\"}]},\n",
       " 'toxicity': {'score': 0,\n",
       "  'reason': 'The score is 0.00 because there are no reasons provided for toxicity, indicating a non-toxic output.',\n",
       "  'opinions': [],\n",
       "  'verdicts': []},\n",
       " 'geval': '{\\n    \"score\": 8,\\n    \"reason\": \"The response is relevant and accurate, but lacks comprehensiveness in covering all key points and contextuality from the Rag application.\"\\n}',\n",
       " 'knowledge_retention': {'score': 1.0,\n",
       "  'reason': 'The score is 1.00 because there are no attritions indicating forgetfulness, ensuring strong knowledge retention.',\n",
       "  'verdicts': [{'index': 0, 'verdict': 'no', 'reason': None},\n",
       "   {'index': 1, 'verdict': 'no', 'reason': None},\n",
       "   {'index': 2, 'verdict': 'no', 'reason': None}],\n",
       "  'knowledges': [{'Capital of France': 'Paris'},\n",
       "   {'Capital of France': 'Paris', 'Landmark': 'Eiffel Tower'},\n",
       "   {'Capital of France': 'Paris',\n",
       "    'Landmark': 'Eiffel Tower',\n",
       "    'Famous foods in France': []}]},\n",
       " 'bias': {'score': 0,\n",
       "  'reason': 'The score is 0.00 because there are no identified reasons for bias in the actual output.',\n",
       "  'opinions': [],\n",
       "  'verdicts': []}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{'contextual_relevancy': {'verdicts': [{'verdict': 'yes',\n",
    "    'reason': 'No reason provided'},\n",
    "   {'verdict': 'yes', 'reason': 'No reason provided'},\n",
    "   {'verdict': 'no',\n",
    "    'reason': 'The context provided does not mention anything about the capital of Japan or provide any relevant information related to the question.'}],\n",
    "  'reason': {'reason': 'The score is 0 because the context provided does not contain any information about the capital of Japan, which is Tokyo.'}},\n",
    " 'faithfulness': {'claims': ['The capital of Japan is Tokyo.'],\n",
    "  'truths': ['The capital of Japan is Tokyo.'],\n",
    "  'verdicts': [{'verdict': 'yes', 'reason': None}],\n",
    "  'reason': 'The score is 1 because the actual output perfectly aligns with the retrieval context, stating that the capital of Japan is Tokyo.'},\n",
    " 'answer_relevancy': {'score': 1,\n",
    "  'reason': 'The score is 1.00 because the answer is perfectly relevant to the input provided. Great job!',\n",
    "  'statements': [],\n",
    "  'verdicts': []},\n",
    " 'hallucination': {'score': 0.6666666666666666,\n",
    "  'reason': 'The score is 0.67 because there are contradictions in the actual output compared to the provided context, indicating some level of hallucination in the generated content.',\n",
    "  'verdicts': [{'verdict': 'yes',\n",
    "    'reason': \"The actual output agrees with the provided context which states that Tokyo is the most populous city in Japan and serves as the country's political and economic center.\"},\n",
    "   {'verdict': 'no',\n",
    "    'reason': 'The actual output contradicts the provided context which states that Tokyo has been the capital since 1868, after the Meiji Restoration moved the capital from Kyoto.'},\n",
    "   {'verdict': 'no',\n",
    "    'reason': \"The actual output lacks detail and does not contradict the provided context which describes Tokyo's architecture and districts.\"}]},\n",
    " 'toxicity': {'score': 0,\n",
    "  'reason': 'The score is 0.00 because there are no reasons provided for toxicity, indicating a non-toxic output.',\n",
    "  'opinions': [],\n",
    "  'verdicts': []},\n",
    " 'geval': '{\\n    \"score\": 8,\\n    \"reason\": \"The response is relevant and accurate, but lacks comprehensiveness in covering all key points and contextuality from the Rag application.\"\\n}',\n",
    " 'knowledge_retention': {'score': 1.0,\n",
    "  'reason': 'The score is 1.00 because there are no attritions indicating forgetfulness, ensuring strong knowledge retention.',\n",
    "  'verdicts': [{'index': 0, 'verdict': 'no', 'reason': None},\n",
    "   {'index': 1, 'verdict': 'no', 'reason': None},\n",
    "   {'index': 2, 'verdict': 'no', 'reason': None}],\n",
    "  'knowledges': [{'Capital of France': 'Paris'},\n",
    "   {'Capital of France': 'Paris', 'Landmark': 'Eiffel Tower'},\n",
    "   {'Capital of France': 'Paris',\n",
    "    'Landmark': 'Eiffel Tower',\n",
    "    'Famous foods in France': []}]},\n",
    " 'bias': {'score': 0,\n",
    "  'reason': 'The score is 0.00 because there are no identified reasons for bias in the actual output.',\n",
    "  'opinions': [],\n",
    "  'verdicts': []}}"
   ],
   "id": "6a10933510c81655"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
