{
 "cells": [
  {
   "metadata": {
    "id": "f03700dfc371b7f1"
   },
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/Demo/indoxJudge_evaluation.ipynb)",
   "id": "f03700dfc371b7f1"
  },
  {
   "metadata": {
    "id": "8fc93175e9cc1eb6"
   },
   "cell_type": "markdown",
   "source": [
    "## Installation of Required Libraries\n",
    "\n",
    "In this notebook, we will be installing the necessary packages for working with **Indox**, **IndoxJudge**, and other supporting libraries for a Retrieval-Augmented Generation (RAG) application. These packages include:\n",
    "\n",
    "- **Indox**: A library that supports large language models (LLMs) with retrieval-augmented generation functionality.\n",
    "- **IndoxJudge**: A library for evaluating LLMs using multiple metrics.\n",
    "- **OpenAI**: For interacting with OpenAI's API to use various GPT models.\n",
    "- **ChromaDB**: A vector database to manage and store embeddings.\n",
    "- **Semantic Text Splitter**: A tool for splitting text in a meaningful way, based on semantics, to ensure better chunking for retrieval-based applications.\n"
   ],
   "id": "8fc93175e9cc1eb6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "outputId": "7dcf40dd-5478-4d80-d759-fcd7d58e8d80",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "!pip install indox\n",
    "!pip install indoxJudge\n",
    "!pip install openai\n",
    "!pip install chromadb\n",
    "!pip install semantic_text_splitter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "c750e7b7543fc71e"
   },
   "cell_type": "markdown",
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indox\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indox_judge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indox\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indox/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ],
   "id": "c750e7b7543fc71e"
  },
  {
   "metadata": {
    "id": "6aaa1c40e8e4bb76"
   },
   "cell_type": "markdown",
   "source": [
    "## Setting Up API Keys for OpenAI\n",
    "\n",
    "In this section, we will set up the environment to securely load API keys for **OpenAI**. We will be using the **dotenv** library to manage environment variables, ensuring sensitive information like API keys is not hardcoded into the code. This approach enhances security and makes the project easier to manage across different environments.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Ensure you have a `.env` file in the root of your project directory.\n",
    "2. In the `.env` file, add the following:\n",
    "    ```\n",
    "    OPENAI_API_KEY=your_openai_api_key\n",
    "    ```\n",
    "3. The Python script will automatically load this environment variable and use it securely in the code.\n"
   ],
   "id": "6aaa1c40e8e4bb76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:12:53.416270Z",
     "start_time": "2024-09-07T15:12:53.378720Z"
    },
    "id": "3c99100dd86fd4a7"
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ],
   "id": "3c99100dd86fd4a7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:12:53.559090Z",
     "start_time": "2024-09-07T15:12:53.421279Z"
    },
    "id": "29e0431c9a58a672",
    "outputId": "e03b2826-60f6-4ed1-fc20-c288283444eb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox import IndoxRetrievalAugmentation\n",
    "\n",
    "indox = IndoxRetrievalAugmentation()"
   ],
   "id": "29e0431c9a58a672",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mIndoxRetrievalAugmentation initialized\u001B[0m\n",
      "\n",
      "            ██  ███    ██  ██████   ██████  ██       ██\n",
      "            ██  ████   ██  ██   ██ ██    ██   ██  ██\n",
      "            ██  ██ ██  ██  ██   ██ ██    ██     ██\n",
      "            ██  ██  ██ ██  ██   ██ ██    ██   ██   ██\n",
      "            ██  ██  █████  ██████   ██████  ██       ██\n",
      "            \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "1776f673ae465ae4"
   },
   "cell_type": "markdown",
   "source": [
    "## Setting Up and Using OpenAI and Azure Embeddings with Indox\n",
    "\n",
    "This section demonstrates how to integrate **OpenAI** models and **Azure** embeddings within the **Indox** framework. We will use **Chroma** as the vector store for storing embeddings, which can later be used for retrieval in various applications, including question answering or information retrieval.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **OpenAI LLM Setup**:\n",
    "   We will initialize a language model from OpenAI, specifying the API key and the model (`gpt-4o-mini` in this case).\n",
    "   \n",
    "2. **Azure Embeddings Setup**:\n",
    "   We will use **AzureOpenAIEmbeddings** to generate embeddings, specifying an API key and the embedding model (`text-embedding-3-small`).\n",
    "\n",
    "3. **Chroma Vector Store**:\n",
    "   The embeddings generated by **Azure** will be stored in **Chroma** under a collection named `sample`, which can be used for querying and retrieving relevant vectors.\n"
   ],
   "id": "1776f673ae465ae4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:27.689342Z",
     "start_time": "2024-09-07T15:13:24.799120Z"
    },
    "id": "230431b1af442de3",
    "outputId": "6b8a2eaa-eae9-4d1f-85ac-4664254a82b8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.llms import OpenAi  # Import OpenAI model class from Indox\n",
    "from indox.embeddings import AzureOpenAIEmbeddings  # Import Azure embeddings class\n",
    "from indox.vector_stores import Chroma  # Import Chroma vector store to handle embedding storage\n",
    "\n",
    "# Initialize the OpenAI language model using the API key and specifying the model \"gpt-4o-mini\"\n",
    "openai_llm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
    "\n",
    "# Initialize Azure embeddings using the same API key and specifying the model \"text-embedding-3-small\"\n",
    "azure_embed = AzureOpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create a Chroma vector store with a collection named 'sample' and set the embedding function to azure_embed\n",
    "db = Chroma(collection_name=\"sample\", embedding_function=azure_embed)\n",
    "\n",
    "# The vector store (db) is now ready to store embeddings generated by Azure and can be used for retrieval purposes\n"
   ],
   "id": "230431b1af442de3",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitializing OpenAi with model: gpt-4o-mini\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mOpenAi initialized successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitialized OpenAiEmbedding with model: text-embedding-3-small\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "86cece8de8415c5c"
   },
   "cell_type": "markdown",
   "source": [
    "## Loading Text Content from Project Gutenberg\n",
    "\n",
    "In this section, we demonstrate how to load the text of a book from **Project Gutenberg** using the **GutenbergReader** class provided by **Indox**. This reader allows us to fetch and read books by specifying their unique Project Gutenberg ID.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize GutenbergReader**:\n",
    "   We will create an instance of `GutenbergReader` to interface with Project Gutenberg.\n",
    "   \n",
    "2. **Specify the Book ID**:\n",
    "   Each book on Project Gutenberg has a unique ID. For example, the ID for *Alice's Adventures in Wonderland* is `\"11\"`.\n",
    "   \n",
    "3. **Fetch the Book Content**:\n",
    "   Using the `get_content()` method, we will fetch the book’s text content by passing the book ID.\n"
   ],
   "id": "86cece8de8415c5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:05.770407Z",
     "start_time": "2024-09-07T15:13:02.142922Z"
    },
    "id": "4a2f82f4a2ca9287"
   },
   "cell_type": "code",
   "source": [
    "from indox.data_connectors import GutenbergReader  # Import GutenbergReader to load books from Project Gutenberg\n",
    "\n",
    "# Initialize the GutenbergReader to access and fetch book content from Project Gutenberg\n",
    "reader = GutenbergReader()\n",
    "\n",
    "# Specify the Project Gutenberg book ID for \"Alice's Adventures in Wonderland\" (ID: 11)\n",
    "book_id = \"11\"\n",
    "\n",
    "# Fetch the content of the book using the get_content method, passing the book_id\n",
    "content = reader.get_content(book_id)\n",
    "\n",
    "# Now, 'content' contains the entire text of \"Alice's Adventures in Wonderland\" as fetched from Project Gutenberg\n"
   ],
   "id": "4a2f82f4a2ca9287",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "ef10e7833ef0d026"
   },
   "cell_type": "markdown",
   "source": [
    "## Splitting Text with SemanticTextSplitter\n",
    "\n",
    "In this section, we show how to split long text into smaller, semantically meaningful chunks using the **SemanticTextSplitter** from **Indox**. This is particularly useful for processing large texts (such as books or articles) into smaller units for tasks like retrieval, summarization, or further analysis.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize the SemanticTextSplitter**:\n",
    "   We will instantiate the `SemanticTextSplitter` with a specified chunk size (e.g., 400 tokens or characters).\n",
    "   \n",
    "2. **Split the Text**:\n",
    "   After fetching the text (from sources like Project Gutenberg), we will pass the content to `split_text()` to break it into smaller, semantically coherent chunks.\n"
   ],
   "id": "ef10e7833ef0d026"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:09.494758Z",
     "start_time": "2024-09-07T15:13:05.771416Z"
    },
    "id": "4795ceae531b32b4"
   },
   "cell_type": "code",
   "source": [
    "from indox.splitter import SemanticTextSplitter  # Import SemanticTextSplitter to split long text into chunks\n",
    "\n",
    "# Initialize the SemanticTextSplitter with a chunk size of 400 (tokens/characters)\n",
    "splitter = SemanticTextSplitter(400)\n",
    "\n",
    "# Split the book content into smaller, semantically meaningful chunks using the splitter\n",
    "content_chunks = splitter.split_text(content)\n",
    "\n",
    "# 'content_chunks' now contains the text broken into smaller segments, useful for retrieval or further processing\n"
   ],
   "id": "4795ceae531b32b4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:09.511466Z",
     "start_time": "2024-09-07T15:13:09.496765Z"
    },
    "id": "a2bd8734a7e9bd46",
    "outputId": "13717b91-3c23-47e6-fdfc-6d0dc6da9f42",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "content_chunks[2:5]"
   ],
   "id": "a2bd8734a7e9bd46",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Alice was beginning to get very tired of sitting by her sister on the\\r\\nbank, and of having nothing to do: once or twice she had peeped into\\r\\nthe book her sister was reading, but it had no pictures or\\r\\nconversations in it, â\\x80\\x9cand what is the use of a book,â\\x80\\x9d thought Alice\\r\\nâ\\x80\\x9cwithout pictures or conversations?â\\x80\\x9d\\r\\n\\r\\nSo she was considering in her own mind (as well as she could, for the\\r\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\r\\nmaking a daisy-chain would be worth the trouble of getting up and\\r\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\r\\nclose by her.\\r\\n\\r\\nThere was nothing so _very_ remarkable in that; nor did Alice think it\\r\\nso _very_ much out of the way to hear the Rabbit say to itself, â\\x80\\x9cOh\\r\\ndear! Oh dear! I shall be late!â\\x80\\x9d (when she thought it over afterwards,\\r\\nit occurred to her that she ought to have wondered at this, but at the\\r\\ntime it all seemed quite natural); but when the Rabbit actually _took a\\r\\nwatch out of its waistcoat-pocket_, and looked at it, and then hurried\\r\\non, Alice started to her feet, for it flashed across her mind that she\\r\\nhad never before seen a rabbit with either a waistcoat-pocket, or a\\r\\nwatch to take out of it, and burning with curiosity, she ran across the\\r\\nfield after it, and fortunately was just in time to see it pop down a\\r\\nlarge rabbit-hole under the hedge.\\r\\n\\r\\nIn another moment down went Alice after it, never once considering how\\r\\nin the world she was to get out again.\\r\\n\\r\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\r\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\r\\nabout stopping herself before she found herself falling down a very\\r\\ndeep well.',\n",
       " 'Either the well was very deep, or she fell very slowly, for she had\\r\\nplenty of time as she went down to look about her and to wonder what\\r\\nwas going to happen next. First, she tried to look down and make out\\r\\nwhat she was coming to, but it was too dark to see anything; then she\\r\\nlooked at the sides of the well, and noticed that they were filled with\\r\\ncupboards and book-shelves; here and there she saw maps and pictures\\r\\nhung upon pegs. She took down a jar from one of the shelves as she\\r\\npassed; it was labelled â\\x80\\x9cORANGE MARMALADEâ\\x80\\x9d, but to her great\\r\\ndisappointment it was empty: she did not like to drop the jar for fear\\r\\nof killing somebody underneath, so managed to put it into one of the\\r\\ncupboards as she fell past it.\\r\\n\\r\\nâ\\x80\\x9cWell!â\\x80\\x9d thought Alice to herself, â\\x80\\x9cafter such a fall as this, I shall\\r\\nthink nothing of tumbling down stairs! How brave theyâ\\x80\\x99ll all think me\\r\\nat home! Why, I wouldnâ\\x80\\x99t say anything about it, even if I fell off the\\r\\ntop of the house!â\\x80\\x9d (Which was very likely true.)\\r\\n\\r\\nDown, down, down. Would the fall _never_ come to an end? â\\x80\\x9cI wonder how\\r\\nmany miles Iâ\\x80\\x99ve fallen by this time?â\\x80\\x9d she said aloud. â\\x80\\x9cI must be\\r\\ngetting somewhere near the centre of the earth. Let me see: that would\\r\\nbe four thousand miles down, I thinkâ\\x80\\x94â\\x80\\x9d (for, you see, Alice had learnt\\r\\nseveral things of this sort in her lessons in the schoolroom, and\\r\\nthough this was not a _very_ good opportunity for showing off her\\r\\nknowledge, as there was no one to listen to her, still it was good\\r\\npractice to say it over) â\\x80\\x9câ\\x80\\x94yes, thatâ\\x80\\x99s about the right distanceâ\\x80\\x94but\\r\\nthen I wonder what Latitude or Longitude Iâ\\x80\\x99ve got to?â\\x80\\x9d (Alice had no\\r\\nidea what Latitude was, or Longitude either, but thought they were nice\\r\\ngrand words to say.)',\n",
       " 'Presently she began again. â\\x80\\x9cI wonder if I shall fall right _through_\\r\\nthe earth! How funny itâ\\x80\\x99ll seem to come out among the people that walk\\r\\nwith their heads downward! The Antipathies, I thinkâ\\x80\\x94â\\x80\\x9d (she was rather\\r\\nglad there _was_ no one listening, this time, as it didnâ\\x80\\x99t sound at all\\r\\nthe right word) â\\x80\\x9câ\\x80\\x94but I shall have to ask them what the name of the\\r\\ncountry is, you know. Please, Maâ\\x80\\x99am, is this New Zealand or Australia?â\\x80\\x9d\\r\\n(and she tried to curtsey as she spokeâ\\x80\\x94fancy _curtseying_ as youâ\\x80\\x99re\\r\\nfalling through the air! Do you think you could manage it?) â\\x80\\x9cAnd what\\r\\nan ignorant little girl sheâ\\x80\\x99ll think me for asking! No, itâ\\x80\\x99ll never do\\r\\nto ask: perhaps I shall see it written up somewhere.â\\x80\\x9d']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:14:42.339580Z",
     "start_time": "2024-09-07T15:13:39.166135Z"
    },
    "id": "5d375184690646f4",
    "outputId": "d13870b3-2119-4cda-d386-ecccf4d327d8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "db.add(docs=content_chunks) # Add chunks to vector database"
   ],
   "id": "5d375184690646f4",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mStoring documents in the vector store\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEmbedding documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting to fetch embeddings for texts using engine: text-embedding-3-small\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocument added successfully to the vector store.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocuments stored successfully\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "a1e7845dc60fb4dd"
   },
   "cell_type": "markdown",
   "source": [
    "## Advanced Setup for the QuestionAnswer Retriever\n",
    "\n",
    "This section explains the additional hyperparameters available when configuring the **QuestionAnswer** retriever in **Indox**. The retriever uses a vector database and language model to fetch relevant content and provide answers to queries. We will cover the following hyperparameters:\n",
    "\n",
    "### Key Hyperparameters:\n",
    "\n",
    "- **llm**: The language model used to generate answers (e.g., OpenAI GPT models).\n",
    "- **vector_database**: The vector database used for embedding retrieval (e.g., Chroma).\n",
    "- **top_k**: Determines how many of the most relevant documents to retrieve (default is 5).\n",
    "- **document_relevancy_filter**: If set to `True`, only the most relevant documents are retrieved based on relevancy filtering.\n",
    "- **generate_clustered_prompts**: If set to `True`, this enables the retriever to cluster the retrieved documents and generate summaries for each cluster. The summary is added to the retrieval context, helping improve the overall response quality by providing a more structured context.\n"
   ],
   "id": "a1e7845dc60fb4dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:16:03.812077Z",
     "start_time": "2024-09-07T15:16:03.808632Z"
    },
    "id": "767293552560adcf"
   },
   "cell_type": "code",
   "source": [
    "# Import the QuestionAnswer retriever from Indox\n",
    "# This version includes all available hyperparameters with explanations\n",
    "\n",
    "retriever = indox.QuestionAnswer(\n",
    "    llm=openai_llm,  # Language model to generate answers (e.g., OpenAI GPT-4)\n",
    "    vector_database=db,  # Chroma vector store for embedding retrieval\n",
    "    top_k=3,  # Number of top relevant documents to retrieve, default is 3\n",
    "    document_relevancy_filter=False,  # Set to True to filter results based on document relevancy\n",
    "    generate_clustered_prompts=False  # Set to True to enable clustering of retrieval results and generate summaries for each cluster\n",
    ")\n",
    "\n",
    "# If 'generate_clustered_prompts' is True:\n",
    "# - The retriever will group the retrieved documents into clusters.\n",
    "# - It will summarize each cluster and add the summary to the initial retrieval context,\n",
    "#   which can help the language model produce a more comprehensive and coherent answer.\n"
   ],
   "id": "767293552560adcf",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:16:25.020023Z",
     "start_time": "2024-09-07T15:16:20.412055Z"
    },
    "id": "a3d7c943139564ae",
    "outputId": "56195864-af5b-4b6a-a147-ced41cfae6bd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    }
   },
   "cell_type": "code",
   "source": [
    "retriever.invoke(\"Who is the speaker talking to in the text?\")"
   ],
   "id": "a3d7c943139564ae",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mRetrieving context and scores from the vector database\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEmbedding documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting to fetch embeddings for texts using engine: text-embedding-3-small\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating answer without document relevancy filter\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mAnswering question\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating response\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mResponse generated successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mQuery answered successfully\u001B[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The speaker is talking to the Caterpillar in the text. Alice engages in a conversation with the Caterpillar, responding to its questions and expressing her feelings about her changing identity and size.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:18:22.891580Z",
     "start_time": "2024-09-07T15:18:19.287157Z"
    },
    "id": "5755848d3e313a07",
    "outputId": "a700c941-0c9f-4a68-e231-7450d6a3bbf7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    }
   },
   "cell_type": "code",
   "source": [
    "retriever.invoke(\"tell me about alice's sister book?\")"
   ],
   "id": "5755848d3e313a07",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mRetrieving context and scores from the vector database\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEmbedding documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting to fetch embeddings for texts using engine: text-embedding-3-small\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating answer without document relevancy filter\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mAnswering question\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating response\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mResponse generated successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mQuery answered successfully\u001B[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Alice's sister's book is described as having no pictures or conversations in it. Alice finds it uninteresting and questions the usefulness of a book without these elements. She feels tired and sleepy while sitting by her sister on the bank, and her lack of engagement with the book contributes to her restlessness. This leads her to consider making a daisy-chain, but her attention is soon captured by the appearance of the White Rabbit, which ultimately leads her into her own adventures in Wonderland.\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:19:15.946343Z",
     "start_time": "2024-09-07T15:19:11.696496Z"
    },
    "id": "d4f3f85d28d99d14",
    "outputId": "9f592e6a-6a70-4659-9375-dc68285dae90",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    }
   },
   "cell_type": "code",
   "source": [
    "retriever.invoke(\"how alice story ends?\")"
   ],
   "id": "d4f3f85d28d99d14",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mRetrieving context and scores from the vector database\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEmbedding documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting to fetch embeddings for texts using engine: text-embedding-3-small\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating answer without document relevancy filter\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mAnswering question\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating response\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mResponse generated successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mQuery answered successfully\u001B[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The story of Alice\\'s Adventures in Wonderland ends with Alice sitting with her eyes closed, half believing she is in Wonderland, while knowing that opening her eyes would return her to dull reality. She imagines her little sister growing up and keeping the loving heart of her childhood, sharing stories and memories of Wonderland with her own children. The narrative concludes with the phrase \"THE END,\" indicating the end of Alice\\'s adventures.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:19:44.781911Z",
     "start_time": "2024-09-07T15:19:44.777999Z"
    },
    "id": "c8dfd6cfbec4f50c"
   },
   "cell_type": "code",
   "source": [
    "chat_history = retriever.chat_history"
   ],
   "id": "c8dfd6cfbec4f50c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "id": "48dacd67cfe53dac"
   },
   "cell_type": "markdown",
   "source": [
    "## Setting Up the RagEvaluator with OpenAI LLM as Judge\n",
    "\n",
    "In this section, we demonstrate how to set up the **RagEvaluator** from **IndoxJudge**, which evaluates the quality of responses generated by a language model in a Retrieval-Augmented Generation (RAG) system. We use **OpenAI** as the language model (LLM) that acts as a judge for the evaluation process.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize the OpenAI Model**:\n",
    "   We first create an instance of `OpenAi` with an API key and a specified model (e.g., `\"gpt-4o-mini\"`).\n",
    "   \n",
    "2. **Set Up the RagEvaluator**:\n",
    "   The `RagEvaluator` will use the LLM to evaluate the quality of the conversation entries (represented by `chat_history`), judging their accuracy and relevancy.\n",
    "   \n",
    "### Key Parameters:\n",
    "- **llm_as_judge**: The language model acting as the judge, responsible for evaluating the retrieved content.\n",
    "- **entries**: The conversation or content history to be evaluated.\n"
   ],
   "id": "48dacd67cfe53dac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:18:22.909887Z",
     "start_time": "2024-09-07T15:18:22.892586Z"
    },
    "id": "5e894ae02427a1aa",
    "outputId": "5f734027-9b59-4e5e-a3ee-c3f958f79c5b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "from indoxJudge.pipelines import RagEvaluator  # Import RagEvaluator for evaluating the quality of RAG systems\n",
    "from indoxJudge.models import OpenAi  # Import OpenAi model from IndoxJudge to use it as a judge\n",
    "\n",
    "# Initialize the OpenAI model with API key and model name (gpt-4o-mini)\n",
    "model = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create a RagEvaluator to evaluate the conversation history\n",
    "# - llm_as_judge: The OpenAI model used to evaluate the responses\n",
    "# - entries: The conversation history or entries to be judged\n",
    "evaluator = RagEvaluator(llm_as_judge=model, entries=chat_history)\n",
    "\n",
    "# Now, the RagEvaluator is set to evaluate the provided chat history based on the responses retrieved from the RAG system.\n"
   ],
   "id": "5e894ae02427a1aa",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitializing OpenAi with model: gpt-4o-mini\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "id": "53963dfb24b32b7b"
   },
   "cell_type": "markdown",
   "source": [
    "## Reviewing Evaluation Results with RagEvaluator\n",
    "\n",
    "After running the `judge()` method, the **RagEvaluator** provides detailed feedback on the quality of the responses. In this section, we explore how to access and visualize the evaluation results and metric scores.\n",
    "\n",
    "### Key Methods and Attributes:\n",
    "\n",
    "1. **evaluator.results**:\n",
    "   This attribute returns the detailed results and verdicts for each evaluation metric applied to the conversation or content entries. You can inspect the specific feedback for each metric (e.g., relevance, accuracy).\n",
    "\n",
    "2. **evaluator.metrics_score**:\n",
    "   This attribute provides the numeric scores for each metric used during the evaluation process, giving you a clear understanding of the performance of the retrieved responses.\n",
    "\n",
    "3. **evaluator.plot()**:\n",
    "   This method generates a visualization (using Matplotlib) of the evaluation scores for each metric. This chart helps you visually interpret the strengths and weaknesses of the model’s responses.\n"
   ],
   "id": "53963dfb24b32b7b"
  },
  {
   "metadata": {
    "id": "9c0d471c927bc165",
    "outputId": "2e499b09-ea7a-4ecb-a894-f0c6954a222c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mModel set for all metrics.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mRagEvaluator initialized with model and metrics.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluation Began...\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating entry: 0\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for entry: 0\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating entry: 1\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for entry: 1\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating entry: 2\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for entry: 2\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluating entry: 3\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted evaluation for entry: 3\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mEvaluation Completed, Check out the results\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "evaluator.judge()"
   ],
   "id": "9c0d471c927bc165"
  },
  {
   "metadata": {
    "id": "a22ddde569b43f35",
    "outputId": "81185424-2584-4768-94f3-a58d2f3a0daa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Faithfulness': 0.91,\n",
       " 'AnswerRelevancy': 1.0,\n",
       " 'ContextualRelevancy': 0.67,\n",
       " 'GEval': 0.83,\n",
       " 'Hallucination': 0.5,\n",
       " 'KnowledgeRetention': 0.0,\n",
       " 'precision': 0.58,\n",
       " 'recall': 0.47,\n",
       " 'f1_score': 0.52,\n",
       " 'METEOR': 0.81,\n",
       " 'evaluation_score': 0.71}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "execution_count": 17,
   "source": [
    "evaluator.metrics_score"
   ],
   "id": "a22ddde569b43f35"
  },
  {
   "metadata": {
    "id": "709f1cc49ac45a35",
    "outputId": "8a385863-6b51-4449-bb79-0d27381c5e1b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "(async (port, path, width, height, cache, element) => {\n",
       "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
       "      return;\n",
       "    }\n",
       "    element.appendChild(document.createTextNode(''));\n",
       "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
       "    const iframe = document.createElement('iframe');\n",
       "    iframe.src = new URL(path, url).toString();\n",
       "    iframe.height = height;\n",
       "    iframe.width = width;\n",
       "    iframe.style.border = 0;\n",
       "    iframe.allow = [\n",
       "        'accelerometer',\n",
       "        'autoplay',\n",
       "        'camera',\n",
       "        'clipboard-read',\n",
       "        'clipboard-write',\n",
       "        'gyroscope',\n",
       "        'magnetometer',\n",
       "        'microphone',\n",
       "        'serial',\n",
       "        'usb',\n",
       "        'xr-spatial-tracking',\n",
       "    ].join('; ');\n",
       "    element.appendChild(iframe);\n",
       "  })(8050, \"/\", \"100%\", 650, false, window.element)"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 19,
   "source": [
    "evaluator.plot(mode=\"inline\")"
   ],
   "id": "709f1cc49ac45a35"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "q3fJv0oRL0F9"
   },
   "id": "q3fJv0oRL0F9",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
