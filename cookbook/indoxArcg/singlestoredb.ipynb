{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deafd62eace5fad4",
   "metadata": {},
   "source": [
    "# SingleStoreDB\n",
    "In this notebook, we will demonstrate how to use SingleStoreDB, for accessing and querying data efficiently. SingleStoreDB is designed to work seamlessly with modern analytical workloads, making it a powerful tool for data analysis, research, and question-answering systems.\n",
    "\n",
    "To begin, ensure you have singlestoredb installed in your Python environment. You can easily install it using `pip install singlestoredb`. \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxArcg/singlestoredb.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354cdb0ba4254b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install indoxArcg\n",
    "!pip install singlestoredb\n",
    "!pip install semantic_text_splitter\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43747bd79eb98e9a",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indoxArcg`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indoxArcg\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indoxArcg\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indoxArcg\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indoxArcg/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca675f19f961a233",
   "metadata": {},
   "source": [
    "### Load Hugging face API key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a24e485afc020d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:47:36.142082Z",
     "start_time": "2024-09-08T10:47:36.137104Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('api.env')\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76889f4768e83877",
   "metadata": {},
   "source": [
    "Initialize a language model and an embedding model using the indox library with Hugging Face. The HuggingFaceModel class is used to create an instance of the Mistral-7B-Instruct model for tasks like question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104548072f255cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:47:43.651887Z",
     "start_time": "2024-09-08T10:47:40.154894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing HuggingFaceModel with model: mistralai/Mistral-7B-Instruct-v0.2\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mHuggingFaceModel initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 14:17:40,160 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda\n",
      "2024-09-08 14:17:40,161 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: multi-qa-mpnet-base-cos-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized HuggingFaceEmbedding with model: multi-qa-mpnet-base-cos-v1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.llms import HuggingFaceModel\n",
    "from indoxArcg.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "mistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "embed = HuggingFaceEmbedding(api_key=HUGGINGFACE_API_KEY,model=\"multi-qa-mpnet-base-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931615abf33be582",
   "metadata": {},
   "source": [
    "### Load Sample text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb1c1e58e04e3ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:47:49.706996Z",
     "start_time": "2024-09-08T10:47:47.690979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-08 14:17:47--  https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185.199.110.133, 185.199.109.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 14025 (14K) [text/plain]\r\n",
      "Saving to: ‘sample.txt.7’\r\n",
      "\r\n",
      "sample.txt.7        100%[===================>]  13.70K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2024-09-08 14:17:49 (121 MB/s) - ‘sample.txt.7’ saved [14025/14025]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af177c8fb50f6859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:47:59.756180Z",
     "start_time": "2024-09-08T10:47:59.751339Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"sample.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ee6b3a502fe7",
   "metadata": {},
   "source": [
    "use the `RecursiveCharacterTextSplitter` class from the indox library to divide a large text into smaller, manageable chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e969fe0ee474430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:48:01.747075Z",
     "start_time": "2024-09-08T10:48:01.742595Z"
    }
   },
   "outputs": [],
   "source": [
    "from indoxArcg.splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(400,20)\n",
    "content_chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ba3c5890bcca9e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:48:04.231500Z",
     "start_time": "2024-09-08T10:48:04.224599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The wife of a rich man fell sick, and as she felt that her end\\n\\nwas drawing near, she called her only daughter to her bedside and\\n\\nsaid, dear child, be good and pious, and then the\\n\\ngood God will always protect you, and I will look down on you\\n\\nfrom heaven and be near you.  Thereupon she closed her eyes and\\n\\ndeparted.  Every day the maiden went out to her mother's grave,\",\n",
       " 'and wept, and she remained pious and good.  When winter came\\n\\nthe snow spread a white sheet over the grave, and by the time the\\n\\nspring sun had drawn it off again, the man had taken another wife.\\n\\nThe woman had brought with her into the house two daughters,\\n\\nwho were beautiful and fair of face, but vile and black of heart.\\n\\nNow began a bad time for the poor step-child.  Is the stupid goose',\n",
       " 'to sit in the parlor with us, they said.  He who wants to eat bread\\n\\nmust earn it.  Out with the kitchen-wench.  They took her pretty\\n\\nclothes away from her, put an old grey bedgown on her, and gave\\n\\nher wooden shoes.  Just look at the proud princess, how decked\\n\\nout she is, they cried, and laughed, and led her into the kitchen.\\n\\nThere she had to do hard work from morning till night, get up']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_chunks[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c15bab1d6f0612e",
   "metadata": {},
   "source": [
    "### Set up vector store\n",
    "Set up a vector store using the `SinlgeStoreDB` class from the indox library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9b87ac6b212d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:48:11.004025Z",
     "start_time": "2024-09-08T10:48:06.679258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector index 'idx_embeddings_vector' already exists.\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.vector_stores import SingleStoreVectorDB\n",
    "\n",
    "connection_params = {\n",
    "    \"host\": \"host\",\n",
    "    \"port\": port,\n",
    "    \"user\": \"user\",\n",
    "    \"password\": \"password\",\n",
    "    \"database\": \"databasename\"\n",
    "}\n",
    "\n",
    "db = SingleStoreVectorDB(connection_params=connection_params,embedding_function=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730369204b63e1d4",
   "metadata": {},
   "source": [
    "### Storing Data in the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "796b85ec513639d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:48:24.008898Z",
     "start_time": "2024-09-08T10:48:11.448365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 11.63it/s]\n"
     ]
    }
   ],
   "source": [
    "db.add_texts(content_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3caacdb855a0740",
   "metadata": {},
   "source": [
    "### Answering query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf21be5bc8d4c2c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:48:26.981772Z",
     "start_time": "2024-09-08T10:48:24.128852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving context and scores from the vector database\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating answer without document relevancy filter\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSending request to Hugging Face API\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mReceived successful response from Hugging Face API\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mQuery answered successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.pipelines.rag import RAG\n",
    "\n",
    "\n",
    "query = \"How cinderella reach her happy ending?\"\n",
    "retriever = RAG(llm=mistral_qa,vector_store=db)\n",
    "answer = retriever.infer(question=query,top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28b584d4be0c1800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T10:48:27.003035Z",
     "start_time": "2024-09-08T10:48:26.997572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cinderella reached her happy ending by escaping from her wicked stepmother and stepsisters and attending the royal ball in disguise. When the prince saw her there, he was instantly attracted to her and identified her as the mysterious maiden he had met earlier. After recognizing each other, they rode away together and live happily ever after. However, due to the wickedness of the stepmother and stepsisters, they tried to prevent Cinderella from attending the ball by'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
