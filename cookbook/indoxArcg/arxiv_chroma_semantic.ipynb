{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c833fa16351fe",
   "metadata": {},
   "source": [
    "# How to Use ArxivReader for Retrieving Papers from arXiv\n",
    "In this notebook, we will demonstrate how to use the `ArxivReader` class for accessing papers from the arXiv repository. The `ArxivReader` class is designed to interact with the arXiv API and retrieve paper content and metadata, which can be utilized in various research and question-answering systems. We'll be leveraging open-source models available on the internet, such as Mistral, to process the retrieved data.\n",
    "\n",
    "To begin, ensure you have set up your environment variables and API keys in Python using the dotenv library. This is crucial for securely managing sensitive information, such as API keys, especially when using services like HuggingFace. Ensure your `HUGGINGFACE_API_KEY` is defined in the `.env` file to avoid hardcoding sensitive data into your codebase, thus enhancing security and maintainability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Platform |\n",
    "|----------|\n",
    "| [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxArcg/arxiv_chroma_semantic.ipynb) |\n",
    "| [![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/osllmai/inDox/blob/master/cookbook/indoxArcg/arxiv_chroma_semantic.ipynb) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c1731b1cfd21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install indoxarcg chromadb arxiv semantic_text_splitter sentence_transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd774b4",
   "metadata": {},
   "source": [
    "if you have an issue to install torch in windows, please use below \n",
    "\n",
    "```python\n",
    "pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822f39ef478838e",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indoxArcg`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "\n",
    "```bash\n",
    "python -m venv indoxArcg\n",
    "\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "\n",
    "```bash\n",
    "indoxArcg\\Scripts\\activate\n",
    "\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "\n",
    "```bash\n",
    "python3 -m venv indoxArcg\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "\n",
    "```bash\n",
    "source indoxArcg/bin/activate\n",
    "```\n",
    "   \n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620c6d0c74b5fbe",
   "metadata": {},
   "source": [
    "## Import Essential Libraries\n",
    "\n",
    "Next, we import the essential libraries for our indoxrag question-answering system:\n",
    "\n",
    "- `IndoxRetrievalAugmentation`: Enhances the retrieval process by improving the relevance and quality of the documents retrieved, leading to better QA performance.\n",
    "- `MistralQA`: A powerful QA model provided by indoxrag, built on top of the Hugging Face model architecture. It leverages state-of-the-art language understanding to deliver precise answers.\n",
    "- `HuggingFaceEmbedding`: This library uses Hugging Face embeddings to enrich semantic understanding, making it easier to capture the contextual meaning of the text.\n",
    "- `SemanticTextSplitter`: utilizes a Hugging Face tokenizer to intelligently split text into chunks based on a specified maximum number of tokens, ensuring that each chunk maintains semantic coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1869873",
   "metadata": {},
   "source": [
    "## How to Obtain a Hugging Face API Key\n",
    "\n",
    "To access the Hugging Face API for our arXiv paper retrieval system, you'll need to generate an API key. Follow these steps:\n",
    "\n",
    "### Step 1: Create a Hugging Face Account\n",
    "If you don't already have one, visit [Hugging Face](https://huggingface.co/) and sign up for an account.\n",
    "\n",
    "### Step 2: Access Your Profile Settings\n",
    "- Log in to your Hugging Face account\n",
    "- Click on your profile picture in the top-right corner\n",
    "- Select \"Settings\" from the dropdown menu\n",
    "\n",
    "### Step 3: Generate a New API Token\n",
    "- Navigate to the \"Access Tokens\" section in the left sidebar\n",
    "- Click on \"New token\" to create a new user access token\n",
    "- Select an appropriate role (Read for inference, Write for model uploads)\n",
    "- Give your token a descriptive name (e.g., \"ArXiv Reader Project\")\n",
    "- Copy the newly generated API token immediately (it won't be shown again)\n",
    "\n",
    "### Step 4: Store Your API Token Securely\n",
    "- Store your API token in your `.env` file as shown in the code\n",
    "- Never hardcode tokens directly in your application\n",
    "- Keep your token confidential as it grants access to your Hugging Face account\n",
    "\n",
    "### Step 5: Use the API Token\n",
    "Once configured in your environment, the token authenticates your requests to:\n",
    "- Access the Hugging Face Inference API\n",
    "- Download models and embeddings\n",
    "- Push or pull models from the Hub\n",
    "\n",
    "> **Note**: The token shown in this notebook is for demonstration purposes only. You should generate your own token for actual use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc9daa14417b6e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T13:36:05.951160Z",
     "start_time": "2024-09-05T13:36:05.943943Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014e14383ad4fbd",
   "metadata": {},
   "source": [
    "## Building the ArxivReader System and Initializing Models\n",
    "\n",
    "Next, we will build our ArxivReader system and initialize the MistralQA model along with the HuggingFaceEmbedding model. This setup will enable us to effectively retrieve and process arXiv papers, leveraging the advanced capabilities of these models for our question-answering tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8b0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad9a7a29fc81465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T13:36:25.897567Z",
     "start_time": "2024-09-05T13:36:15.357657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nemat\\.conda\\envs\\indox\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing HuggingFaceAPIModel with model: mistralai/Mistral-7B-Instruct-v0.2\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mHuggingFaceAPIModel initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nemat\\.conda\\envs\\indox\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:196: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized HuggingFaceEmbedding with model: multi-qa-mpnet-base-cos-v1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.llms import HuggingFaceAPIModel\n",
    "from indoxArcg.embeddings import HuggingFaceEmbedding\n",
    "mistral_qa = HuggingFaceAPIModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "embed = HuggingFaceEmbedding(api_key=HUGGINGFACE_API_KEY,model=\"multi-qa-mpnet-base-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0032f294a4379f2",
   "metadata": {},
   "source": [
    "## Setting Up the ArxivReader for Retrieving Papers\n",
    "To demonstrate the capabilities of our `ArxivReader` system and `Indox` question-answering model, we will use a sample paper ID. This paper will contain arXiv paper, which we will use for testing and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d377a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2ba25cec5cb1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:04.837027Z",
     "start_time": "2024-08-27T07:30:02.598380Z"
    }
   },
   "outputs": [],
   "source": [
    "from indoxArcg.data_connectors import ArxivReader\n",
    "\n",
    "reader = ArxivReader()\n",
    "\n",
    "paper_ids = [\"2201.08239\"]\n",
    "documents = reader.load_content(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43358891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Title: LaMDA: Language Models for Dialog Applications\\n\\nAuthors: Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le\\n\\nAbstract: We present LaMDA: Language Models for Dialog Applications. LaMDA is a family\\nof Transformer-based neural language models specialized for dialog, which have\\nup to 137B parameters and are pre-trained on 1.56T words of public dialog data\\nand web text. While model scaling alone can improve quality, it shows less\\nimprovements on safety and factual grounding. We demonstrate that fine-tuning\\nwith annotated data and enabling the model to consult external knowledge\\nsources can lead to significant improvements towards the two key challenges of\\nsafety and factual grounding. The first challenge, safety, involves ensuring\\nthat the model's responses are consistent with a set of human values, such as\\npreventing harmful suggestions and unfair bias. We quantify safety using a\\nmetric based on an illustrative set of human values, and we find that filtering\\ncandidate responses using a LaMDA classifier fine-tuned with a small amount of\\ncrowdworker-annotated data offers a promising approach to improving model\\nsafety. The second challenge, factual grounding, involves enabling the model to\\nconsult external knowledge sources, such as an information retrieval system, a\\nlanguage translator, and a calculator. We quantify factuality using a\\ngroundedness metric, and we find that our approach enables the model to\\ngenerate responses grounded in known sources, rather than responses that merely\\nsound plausible. Finally, we explore the use of LaMDA in the domains of\\neducation and content recommendations, and analyze their helpfulness and role\\nconsistency.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d3e6113f3f5e1ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:07.057854Z",
     "start_time": "2024-08-27T07:30:07.049857Z"
    }
   },
   "outputs": [],
   "source": [
    "content = documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139a5ed0a1e734b",
   "metadata": {},
   "source": [
    "## Splitting Content into Manageable Chunks\n",
    "We use the `SemanticTextSplitter` function from the `indoxArcg.splitter` module to divide the retrieved content into smaller, meaningful chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8805ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install semantic_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835fe1c42c4e31df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:09.592094Z",
     "start_time": "2024-08-27T07:30:09.214442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: LaMDA: Language Models for Dialog Applications\\n\\nAuthors: Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indoxArcg.splitter import SemanticTextSplitter\n",
    "splitter = SemanticTextSplitter(400)\n",
    "content_chunks = splitter.split_text(content)\n",
    "\n",
    "content_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b882f2c8f21fec0",
   "metadata": {},
   "source": [
    "## Storing and Indexing Content with Chroma\n",
    "We use the `Chroma` vector store from the `indoxArcg.vector_stores` module to store and index the content chunks. By creating a collection named \"sample\" and applying an embedding function (`embed`), we convert each chunk into a vector for efficient retrieval. The `add` method then adds these vectors to the database, enabling scalable and effective search for question-answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f73b4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4636c3ad55b48d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:14.477310Z",
     "start_time": "2024-08-27T07:30:12.967248Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 18:49:27,254 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mStoring documents in the vector store\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocument added successfully to the vector store.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocuments stored successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.vector_stores import Chroma\n",
    "db = Chroma(collection_name=\"sample\",embedding_function=embed)\n",
    "db.add(docs=content_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c8870e944d72a5",
   "metadata": {},
   "source": [
    "## Querying the Arxiv Data with Indox\n",
    "With our `ArxivReader` system and `indoxArcg` setup complete, we are ready to test it using a sample query. This test will show how well our system can retrieve and generate accurate answers based on the arXiv papers stored in the vector store.\n",
    "\n",
    "We’ll use a sample query to evaluate our system:\n",
    "- **Query**: \"what are challenges?\"\n",
    "\n",
    "This question will be processed by the `ArxivReader` and `indoxArcg` system to retrieve relevant papers and generate a precise response based on the information.\n",
    "\n",
    "Let’s test our setup with this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dedec112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indoxArcg.pipelines.rag import RAG\n",
    "indox = RAG(llm=mistral_qa,vector_store=db) #  WebSearchFallback=True has removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e7053437d60ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T07:30:18.173362Z",
     "start_time": "2024-08-27T07:30:18.167379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.26it/s]\n",
      "2025-03-28 18:49:36,976 - chromadb.segment.impl.vector.local_hnsw - WARNING - Number of requested results 5 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSending request to Hugging Face API\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mReceived successful response from Hugging Face API\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"what are challenges?\"\n",
    "response = indox.infer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ff9f0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key challenges addressed in the study for the LaMDA (Language Models for Dialog Applications) are safety and factual grounding. Safety involves ensuring the model's responses align with human values, preventing harmful suggestions and unfair biases. Factual grounding enables the model to consult external knowledge sources, providing responses rooted in known facts, rather than merely plausible responses.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f301240",
   "metadata": {},
   "source": [
    "## Join Us\n",
    "\n",
    "Join us in exploring how Indox can revolutionize your document processing workflow, bringing clarity and organization to your data retrieval needs. Connect with us and become part of our growing community through the platforms below:\n",
    "\n",
    "## Community\n",
    "\n",
    "- [Discord](https://discord.com/invite/xGz5tQYaeq)\n",
    "- [X (Twitter)](https://x.com/osllmai)\n",
    "- [LinkedIn](https://www.linkedin.com/company/osllmai/)\n",
    "- [YouTube](https://www.youtube.com/@osllm-rb9pr)\n",
    "- [Telegram](https://t.me/osllmai)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Reviewed by: Ali Nemati - March, 27, 2025*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
