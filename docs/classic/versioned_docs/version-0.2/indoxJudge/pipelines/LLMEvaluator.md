# LLM Evaluator

## Overview

The `LLMEvaluator` class is designed to evaluate various aspects of language model outputs using specified metrics. It supports metrics such as Faithfulness, Answer Relevancy, Bias, Contextual Relevancy, GEval, Hallucination, Knowledge Retention, Toxicity, BertScore, BLEU, Rouge, and METEOR. This class provides a comprehensive assessment of different dimensions of model performance, making it ideal for thorough evaluations of language models.

## Initialization

The `LLMEvaluator` class is initialized with four main components:

- **llm_as_judge**: The language model acting as the judge for the evaluation.
- **llm_response**: The response generated by the language model.
- **retrieval_context**: The context used for retrieval-based metrics.
- **query**: The query or prompt provided to the language model.

### Example

```python
class LLMEvaluator:
    def __init__(self, llm_as_judge, llm_response, retrieval_context, query):
        """
        Initializes the Evaluator with a language model and a list of metrics.

        Args:
            llm_as_judge: The language model acting as the judge for the evaluation.
            llm_response: The response generated by the language model.
            retrieval_context: The context used for retrieval-based metrics.
            query: The query or prompt provided to the language model.
        """
        self.model = llm_as_judge
        self.metrics = [
            Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context),
            AnswerRelevancy(query=query, llm_response=llm_response),
            Bias(llm_response=llm_response),
            Hallucination(llm_response=llm_response, retrieval_context=retrieval_context),
            KnowledgeRetention(messages=[{"query": query, "llm_response": llm_response}]),
            Toxicity(messages=[{"query": query, "llm_response": llm_response}]),
            BertScore(llm_response=llm_response, retrieval_context=retrieval_context),
            BLEU(llm_response=llm_response, retrieval_context=retrieval_context),
        ]
```

## Setting the Model for Metrics

The `set_model_for_metrics` method ensures that the language model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model for evaluation purposes.

### Usage Example

```python
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

from indoxJudge.pipelines import LLMEvaluator
from indoxJudge.models import OpenAi

query = "What are the benefits of a Mediterranean diet?"
retrieval_context = [
    "The Mediterranean diet emphasizes eating primarily plant-based foods, such as fruits and vegetables, whole grains, legumes, and nuts. It also includes moderate amounts of fish and poultry, and low consumption of red meat. Olive oil is the main source of fat, providing monounsaturated fats which are beneficial for heart health.",
    "Research has shown that the Mediterranean diet can reduce the risk of heart disease, stroke, and type 2 diabetes. It is also associated with improved cognitive function and a lower risk of Alzheimer's disease. The diet's high content of fiber, antioxidants, and healthy fats contributes to its numerous health benefits.",
    "A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases. It promotes healthy aging and weight management due to its emphasis on whole, unprocessed foods and balanced nutrition."
]

# Obtain the model's response
response = "The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease, stroke, and diabetes. It encourages the consumption of fruits, vegetables, whole grains, nuts, and olive oil, while limiting red meat. Additionally, this diet has been associated with better cognitive function and a reduced risk of Alzheimer's disease, promoting longevity and overall well-being."


llm_as_judge = OpenAi(api_key=OPENAI_API_KEY,model="gpt-4o")

evaluator = LLMEvaluator(llm_as_judge=llm_as_judge,llm_response=response,retrieval_context=retrieval_context,query=query)
llm_results = evaluator.judge()

evaluation_score = evaluator.evaluation_score
evaluation_metrics_score = evaluator.metrics_score

# For plot and visualize (choose inline for using in colab)
evaluator.plot(mode="external")
```

In this example, the LLMEvaluator class method judge initializes an empty dictionary called results. It iterates through each metric in the metrics list, retrieves the metric's name, and logs the start of the evaluation process. For each metric, the method checks its type and calls the appropriate evaluation method. For instance, the Faithfulness metric involves evaluating claims, truths, and verdicts, which are then stored in the results dictionary under the 'faithfulness' key.

If any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the evaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the evaluations for each metric.