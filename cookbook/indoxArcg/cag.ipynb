{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tenacity loguru huggingface_hub semantic-text-splitter openai\n",
    "!pip install indoxarcg torch docling torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install the required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking version of indoxArcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import indoxArcg \n",
    "indoxArcg.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding your Api Key to the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "HUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_huggingface_token' with your actual Hugging Face token\n",
    "login(token=HUGGINGFACE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indoxArcg.llms import OpenAi\n",
    "from indoxArcg.embeddings import OpenAiEmbedding\n",
    "from indoxArcg.data_loaders import Txt,DoclingReader\n",
    "from indoxArcg.splitter import RecursiveCharacterTextSplitter,SemanticTextSplitter\n",
    "from indoxArcg.pipelines.cag import CAG, KVCache\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing OpenAi with model: gpt-4o-mini\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mOpenAi initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized OpenAiEmbedding with model: text-embedding-3-small\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAi(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
    "embed_model = OpenAiEmbedding(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.txt exists\n"
     ]
    }
   ],
   "source": [
    "# check if the sample.txt file exists\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"sample.txt\"):\n",
    "    print(\"sample.txt exists\")\n",
    "else:\n",
    "    !wget https://raw.githubusercontent.com/osllmai/inDox/refs/heads/master/cookbook/indoxArcg/sample.txt\n",
    "    print(\"sample.txt does not exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "The wife of a rich man fell sick, and as she felt that her end\n",
      "\n",
      "was drawing near, she called her only daughter to her bedside and\n",
      "\n",
      "said, dear child, be good and pious, and then the\n",
      "\n",
      "good God will always protect you, and I will look down on you\n",
      "\n",
      "from heaven and be near you.  Thereupon she closed her eyes and\n",
      "\n",
      "departed.  Every day the maiden went out to her mother's grave,\n"
     ]
    }
   ],
   "source": [
    "# Load the text file using Txt loader\n",
    "txt_loader = Txt(txt_path=\"sample.txt\")\n",
    "\n",
    "# Initialize a RecursiveCharacterTextSplitter to split the text into smaller chunks\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "# Load the content of the text file\n",
    "docs = txt_loader.load()\n",
    "\n",
    "# Split the loaded text into smaller chunks using the splitter\n",
    "split_docs = splitter.split_text(text=docs)\n",
    "\n",
    "# Print the number of chunks created\n",
    "print(len(split_docs ))\n",
    "\n",
    "# Print the first chunk of the split text\n",
    "print(split_docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output verification.pdf  exists\n"
     ]
    }
   ],
   "source": [
    "# check if the sample.txt file exists\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"LLM output verification.pdf\"):\n",
    "    print(\"LLM output verification.pdf  exists\")\n",
    "else:\n",
    "    !wget \"https://raw.githubusercontent.com/osllmai/inDox/refs/heads/master/cookbook/indoxArcg/LLM output verification.pdf\"\n",
    "    print(\"sample.txt does not exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = \"LLM output verification.pdf\"\n",
    "docling_reader = DoclingReader(file_path=pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 19:28:08,486 - docling.document_converter - INFO - Going to convert document batch...\n",
      "2025-03-22 19:28:08,512 - docling.models.factories.base_factory - INFO - Loading plugin 'docling_defaults'\n",
      "2025-03-22 19:28:08,512 - docling.models.factories - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-03-22 19:28:08,801 - docling.utils.accelerator_utils - INFO - Accelerator device: 'mps'\n",
      "2025-03-22 19:28:11,591 - docling.utils.accelerator_utils - INFO - Accelerator device: 'mps'\n",
      "2025-03-22 19:28:39,101 - docling.utils.accelerator_utils - INFO - Accelerator device: 'mps'\n",
      "2025-03-22 19:28:39,853 - docling.models.factories.base_factory - INFO - Loading plugin 'docling_defaults'\n",
      "2025-03-22 19:28:39,853 - docling.models.factories - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-03-22 19:28:39,855 - docling.pipeline.base_pipeline - INFO - Processing document LLM output verification.pdf\n",
      "2025-03-22 19:28:52,869 - docling.document_converter - INFO - Finished converting document LLM output verification.pdf in 44.43 sec.\n"
     ]
    }
   ],
   "source": [
    "pdf_doc = docling_reader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 19:28:53,022 - docling_core.types.doc.document - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    }
   ],
   "source": [
    "text_docs = pdf_doc.document.export_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zkLLM is a cryptographic framework designed to ensure verifiable execution '\n",
      " 'of large language models (LLMs) using Zero-Knowledge Proofs (ZKPs) . The key '\n",
      " 'idea is that a third-party executor can prove they used the specified model '\n",
      " 'to compute the given input and produce the output without revealing the '\n",
      " 'underlying model parameters or the input data. zkLLM achieves this through '\n",
      " 'efficient protocols tailored for LLM operations, such as transformer '\n",
      " 'attention mechanisms, enabling secure and scalable verification.\\n'\n",
      " '\\n'\n",
      " 'This innovative approach is detailed in the paper, zkLLM: Zero Knowledge '\n",
      " 'Proofs for Large Language Models , which introduces core components like '\n",
      " 'tlookup for non-arithmetic operations and zkAttn for attention mechanisms. '\n",
      " 'The official implementation is available on: '\n",
      " 'https://github.com/jvhs0706/zkllm-ccs2024\\n'\n",
      " '\\n'\n",
      " '## Detailed Explanation of zkLLM\\n'\n",
      " '\\n'\n",
      " 'The zkLLM framework introduces an innovative method for verifiable '\n",
      " 'computations of large language models (LLMs) using Zero-Knowledge Proofs '\n",
      " '(ZKPs) . This solution allows third-party executors to prove that they have '\n",
      " 'correctly executed a specified LLM on a given input without revealing '\n",
      " 'sensitive model parameters or input data.\\n'\n",
      " '\\n'\n",
      " '## How zkLLM Works\\n'\n",
      " '\\n'\n",
      " '## 1. Model Commitment :\\n'\n",
      " '\\n'\n",
      " '- ○ The model parameters are hashed into a cryptographic commitment, '\n",
      " 'creating a \"fingerprint\" of the LLM. This ensures that any tampering with '\n",
      " 'the model parameters can be detected.\\n'\n",
      " '\\n'\n",
      " '## 2. Input Execution :\\n'\n",
      " '\\n'\n",
      " '- ○ The LLM processes the input query to generate an output. While doing so, '\n",
      " 'the zkLLM framework captures intermediate computations for proof '\n",
      " 'construction.\\n'\n",
      " '\\n'\n",
      " '## 3. Proof Generation :')\n"
     ]
    }
   ],
   "source": [
    "# Initialize a SemanticTextSplitter instance to split the text into meaningful semantic chunks\n",
    "semantic_splitter = SemanticTextSplitter()\n",
    "\n",
    "# Use the semantic splitter to divide the extracted text from the PDF into smaller, coherent sections\n",
    "pdf_doc_split = semantic_splitter.split_text(text_docs)\n",
    "\n",
    "# Print the second chunk of the split text to verify the output\n",
    "pprint(pdf_doc_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mPrecomputing KV cache for 38 document chunks...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mKV cache saved: kv_cache/no_embed_cache.pkl\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPreloaded 38 document chunks into KV cache\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cache_no_embed_key = \"no_embed_cache\"\n",
    "cag_without_embedding = CAG(llm=llm, cache=KVCache())\n",
    "cag_without_embedding.preload_documents(split_docs, cache_no_embed_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_doc_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m cache_no_embed_key_pdf = \u001b[33m\"\u001b[39m\u001b[33mno_embed_cache_pdf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m cag_without_embedding = CAG(llm=llm, cache=KVCache())\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m cag_without_embedding.preload_documents(\u001b[43mpdf_doc_split\u001b[49m, cache_no_embed_key_pdf)\n",
      "\u001b[31mNameError\u001b[39m: name 'pdf_doc_split' is not defined"
     ]
    }
   ],
   "source": [
    "cache_no_embed_key_pdf = \"no_embed_cache_pdf\"\n",
    "cag_without_embedding = CAG(llm=llm, cache=KVCache())\n",
    "cag_without_embedding.preload_documents(pdf_doc_split, cache_no_embed_key_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_cinderella = \"How Cinderella reach her happy ending?\"\n",
    "query_pdf = \"how users could earn tokens?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(cag_with_embedding.infer(query,cache_key=cache_embed_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mSelected 0 relevant chunks from cache\u001b[0m\n",
      "<generator object OpenAi._generate_response at 0x368cc5640>\n"
     ]
    }
   ],
   "source": [
    "response_tfidf = cag_without_embedding.infer(query_pdf,cache_key=cache_no_embed_key_pdf,similarity_search_type=\"tfidf\",similarity_threshold=0.3)\n",
    "pprint(response_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mSelected 5 relevant chunks from cache\u001b[0m\n",
      "<generator object OpenAi._generate_response at 0x368cc6140>\n"
     ]
    }
   ],
   "source": [
    "response_bm25 = cag_without_embedding.infer(query_pdf,cache_key=cache_no_embed_key,similarity_search_type=\"bm25\",similarity_threshold=0.7)\n",
    "pprint(response_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mSelected 0 relevant chunks from cache\u001b[0m\n",
      "<generator object OpenAi._generate_response at 0x368cc5fe0>\n"
     ]
    }
   ],
   "source": [
    "response_jaccard = cag_without_embedding.infer(query_pdf,cache_key=cache_no_embed_key,similarity_search_type=\"jaccard\",similarity_threshold=0.1)\n",
    "pprint(response_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mUsing smart retrieval with validation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mUsing smart retrieval\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSelected 0 relevant chunks from cache\u001b[0m\n",
      "\u001b[32mWARNING\u001b[0m: \u001b[33m\u001b[1mNo relevant context found in cache\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPerforming web search for additional context\u001b[0m\n",
      "\u001b[32mERROR\u001b[0m: \u001b[31m\u001b[1mWeb fallback failed: No module named 'duckduckgo_search'\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m: \u001b[31m\u001b[1mWeb fallback failed: No module named 'duckduckgo_search'\u001b[0m\n",
      "<generator object OpenAi._generate_response at 0x368cc5e80>\n"
     ]
    }
   ],
   "source": [
    "response_jaccard = cag_without_embedding.infer(query_pdf,cache_key=cache_no_embed_key,similarity_search_type=\"jaccard\",similarity_threshold=0.1,smart_retrieval=True)\n",
    "pprint(response_jaccard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
