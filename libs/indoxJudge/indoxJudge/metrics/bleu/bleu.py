import numpy as np
import math
from collections import Counter
from typing import List, Union
from nltk.tokenize import word_tokenize


class BLEU:
    def __init__(
            self,
            llm_response: str,
            retrieval_context: Union[str, List[str]],
            n: int = 2,
            remove_repeating_ngrams: bool = False,
            chunk_size: int = 500
    ):
        """
        Initialize the BLEU evaluator with the desired n-gram size and option to remove repeating n-grams.

        Parameters:
        llm_response (str): The response generated by a language model.
        retrieval_context (Union[str, List[str]]): The expected response(s) to compare against the actual response.
        n (int): The maximum size of the n-grams to use for evaluation (default is 2).
        remove_repeating_ngrams (bool): Whether to remove repeating n-grams (default is False).
        chunk_size (int): Size of each chunk when splitting a long context.
        """
        self.llm_response = llm_response
        self.retrieval_context = retrieval_context
        self.n = n
        self.remove_repeating_ngrams = remove_repeating_ngrams
        self.chunk_size = chunk_size

    def measure(self) -> float:
        """
        Compute the BLEU score between the actual response and the expected response(s).

        Returns:
        float: The computed BLEU score.
        """
        score = self._calculate_score(
            llm_answer=self.llm_response, context=self.retrieval_context
        )
        return score

    def preprocess_text(self, text: str) -> str:
        """
        Preprocess the given text by applying various text preprocessing methods.

        Parameters:
        text (str): The text to preprocess.

        Returns:
        str: The preprocessed text.
        """
        return text.lower()

    def tokenize(self, text: str) -> List[str]:
        """
        Tokenize the given text into a list of words.

        Parameters:
        text (str): The text to tokenize.

        Returns:
        List[str]: The list of tokens.
        """
        tokens = word_tokenize(text)
        return tokens

    def get_ngrams(self, text: str, n: int) -> List[str]:
        tokens = self.tokenize(text)
        ngrams = [" ".join(tokens[i: i + n]) for i in range(len(tokens) - n + 1)]
        if self.remove_repeating_ngrams:
            ngrams = list(set(ngrams))
        return ngrams

    def calculate_bp(self, context_length: int, llm_answer_length: int) -> float:
        if llm_answer_length == 0:
            return 0  # If the answer is empty, the BP should be 0
        if llm_answer_length > context_length:
            return 1
        else:
            penalty = 1 - (context_length / llm_answer_length)
            bp = np.exp(min(0, penalty))  # Avoid excessive penalization
            return bp

    def calculate_clipped_precision(
            self, context_ngrams: Counter, llm_answer_ngrams: Counter
    ) -> float:
        """
        Calculate the clipped precision for the BLEU score.

        Parameters:
        context_ngrams (Counter): The n-grams from the context text.
        llm_answer_ngrams (Counter): The n-grams from the language model's response.

        Returns:
        float: The clipped precision.
        """
        clipped_count = 0
        total_count = sum(llm_answer_ngrams.values())

        for ngram in llm_answer_ngrams:
            clipped_count += min(llm_answer_ngrams[ngram], context_ngrams.get(ngram, 0))

        return clipped_count / total_count if total_count > 0 else 0

    def calculate_bleu(self, context: str, llm_answer: str) -> float:
        context = self.preprocess_text(context)
        llm_answer = self.preprocess_text(llm_answer)

        context_length = len(self.tokenize(context))
        llm_answer_length = len(self.tokenize(llm_answer))

        BP = self.calculate_bp(context_length, llm_answer_length)

        clipped_precision_scores = []
        for i in range(1, self.n + 1):
            context_ngrams = Counter(self.get_ngrams(context, i))
            llm_answer_ngrams = Counter(self.get_ngrams(llm_answer, i))

            clipped_precision = self.calculate_clipped_precision(
                context_ngrams, llm_answer_ngrams
            )

            clipped_precision_scores.append(
                clipped_precision if clipped_precision > 0 else 1e-9
            )

        # Use a small smoothing constant
        smoothing = 1e-2
        weights = [1 / self.n] * self.n
        s = (w_i * math.log(p_i + smoothing) for w_i, p_i in zip(weights, clipped_precision_scores))

        # Ensure BLEU score does not exceed 1.0
        score = BP * math.exp(math.fsum(s))

        return score

    def chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """
        Split the text into smaller chunks of a specified size.

        Parameters:
        text (str): The input text to split.
        chunk_size (int): The number of tokens per chunk.

        Returns:
        List[str]: A list of text chunks.
        """
        tokens = self.tokenize(text)
        chunks = [' '.join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]
        return chunks

    def _calculate_score(
            self, context: Union[str, List[str]], llm_answer: Union[str, List[str]]
    ) -> float:
        """
        Calculate the average BLEU score for the given context(s) and language model response.
        If the context is too long, it will be chunked.

        Parameters:
        context (Union[str, List[str]]): The context text(s).
        llm_answer (Union[str, List[str]]): The language model's response.

        Returns:
        float: The average BLEU score.
        """
        if isinstance(context, str):
            context = self.chunk_text(context)

        if isinstance(llm_answer, list):
            llm_answer = " ".join(llm_answer)

        scores = []
        for ctx in context:
            scores.append(self.calculate_bleu(ctx, llm_answer))

        average_score = np.mean(scores)
        return round(average_score, 2)
