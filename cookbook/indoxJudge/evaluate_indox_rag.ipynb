{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc93175e9cc1eb6",
   "metadata": {
    "id": "8fc93175e9cc1eb6"
   },
   "source": [
    "## Installation of Required Libraries\n",
    "\n",
    "| Platform |\n",
    "|----------|\n",
    "| [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxJudge/evaluate_indox_rag.ipynb)|\n",
    "| [![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/osllmai/inDox/blob/master/cookbook/indoxJudge/evaluate_indox_rag.ipynb) |\n",
    "\n",
    "\n",
    "In this notebook, we will be installing the necessary packages for working with **Indox**, **IndoxJudge**, and other supporting libraries for a Retrieval-Augmented Generation (RAG) application. These packages include:\n",
    "\n",
    "- **Indox**: A library that supports large language models (LLMs) with retrieval-augmented generation functionality.\n",
    "- **IndoxJudge**: A library for evaluating LLMs using multiple metrics.\n",
    "- **OpenAI**: For interacting with OpenAI's API to use various GPT models.\n",
    "- **ChromaDB**: A vector database to manage and store embeddings.\n",
    "- **Semantic Text Splitter**: A tool for splitting text in a meaningful way, based on semantics, to ensure better chunking for retrieval-based applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "initial_id",
    "outputId": "7dcf40dd-5478-4d80-d759-fcd7d58e8d80"
   },
   "outputs": [],
   "source": [
    "#!pip install indox indoxJudge openai chromadb semantic_text_splitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750e7b7543fc71e",
   "metadata": {
    "id": "c750e7b7543fc71e"
   },
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "\n",
    "```bash\n",
    "python -m venv indox\n",
    "```\n",
    "1. **Activate the virtual environment:**\n",
    "\n",
    "```bash\n",
    "indox_judge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "\n",
    "```bash\n",
    "   python3 -m venv indox\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "\n",
    "```bash\n",
    "   source indox/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa1c40e8e4bb76",
   "metadata": {
    "id": "6aaa1c40e8e4bb76"
   },
   "source": [
    "## Setting Up API Keys for OpenAI\n",
    "\n",
    "In this section, we will set up the environment to securely load API keys for **OpenAI**. We will be using the **dotenv** library to manage environment variables, ensuring sensitive information like API keys is not hardcoded into the code. This approach enhances security and makes the project easier to manage across different environments.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Ensure you have a `.env` file in the root of your project directory.\n",
    "2. In the `.env` file, add the following:\n",
    "    ```\n",
    "    OPENAI_API_KEY=your_openai_api_key\n",
    "    ```\n",
    "3. The Python script will automatically load this environment variable and use it securely in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c99100dd86fd4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:12:53.416270Z",
     "start_time": "2024-09-07T15:12:53.378720Z"
    },
    "id": "3c99100dd86fd4a7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e0431c9a58a672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:12:53.559090Z",
     "start_time": "2024-09-07T15:12:53.421279Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29e0431c9a58a672",
    "outputId": "e03b2826-60f6-4ed1-fc20-c288283444eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mIndoxRetrievalAugmentation initialized\u001b[0m\n",
      "\n",
      "            ██  ███    ██  ██████   ██████  ██       ██\n",
      "            ██  ████   ██  ██   ██ ██    ██   ██  ██\n",
      "            ██  ██ ██  ██  ██   ██ ██    ██     ██\n",
      "            ██  ██  ██ ██  ██   ██ ██    ██   ██   ██\n",
      "            ██  ██  █████  ██████   ██████  ██       ██\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "from indox import IndoxRetrievalAugmentation\n",
    "\n",
    "indox = IndoxRetrievalAugmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776f673ae465ae4",
   "metadata": {
    "id": "1776f673ae465ae4"
   },
   "source": [
    "## Setting Up and Using OpenAI and Azure Embeddings with Indox\n",
    "\n",
    "This section demonstrates how to integrate **OpenAI** models and **Azure** embeddings within the **Indox** framework. We will use **Chroma** as the vector store for storing embeddings, which can later be used for retrieval in various applications, including question answering or information retrieval.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **OpenAI LLM Setup**:\n",
    "   We will initialize a language model from OpenAI, specifying the API key and the model (`gpt-4o-mini` in this case).\n",
    "   \n",
    "2. **Azure Embeddings Setup**:\n",
    "   We will use **AzureOpenAIEmbeddings** to generate embeddings, specifying an API key and the embedding model (`text-embedding-3-small`).\n",
    "\n",
    "3. **Chroma Vector Store**:\n",
    "   The embeddings generated by **Azure** will be stored in **Chroma** under a collection named `sample`, which can be used for querying and retrieving relevant vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "230431b1af442de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:27.689342Z",
     "start_time": "2024-09-07T15:13:24.799120Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "230431b1af442de3",
    "outputId": "6b8a2eaa-eae9-4d1f-85ac-4664254a82b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:33:53,724 - numexpr.utils - INFO - Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "2025-03-23 16:33:53,724 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing OpenAi with model: gpt-4o-mini\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mOpenAi initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized OpenAiEmbedding with model: text-embedding-3-small\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:33:55,913 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "from indox.llms import OpenAi  # Import OpenAI model class from Indox\n",
    "from indox.embeddings import AzureOpenAIEmbeddings  # Import Azure embeddings class\n",
    "from indox.vector_stores import Chroma  # Import Chroma vector store to handle embedding storage\n",
    "\n",
    "# Initialize the OpenAI language model using the API key and specifying the model \"gpt-4o-mini\"\n",
    "openai_llm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
    "\n",
    "# Initialize Azure embeddings using the same API key and specifying the model \"text-embedding-3-small\"\n",
    "azure_embed = AzureOpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create a Chroma vector store with a collection named 'sample' and set the embedding function to azure_embed\n",
    "db = Chroma(collection_name=\"sample\", embedding_function=azure_embed)\n",
    "\n",
    "# The vector store (db) is now ready to store embeddings generated by Azure and can be used for retrieval purposes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cece8de8415c5c",
   "metadata": {
    "id": "86cece8de8415c5c"
   },
   "source": [
    "## Loading Text Content from Project Gutenberg\n",
    "\n",
    "In this section, we demonstrate how to load the text of a book from **Project Gutenberg** using the **GutenbergReader** class provided by **Indox**. This reader allows us to fetch and read books by specifying their unique Project Gutenberg ID.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize GutenbergReader**:\n",
    "   We will create an instance of `GutenbergReader` to interface with Project Gutenberg.\n",
    "   \n",
    "2. **Specify the Book ID**:\n",
    "   Each book on Project Gutenberg has a unique ID. For example, the ID for *Alice's Adventures in Wonderland* is `\"11\"`.\n",
    "   \n",
    "3. **Fetch the Book Content**:\n",
    "   Using the `get_content()` method, we will fetch the book’s text content by passing the book ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2f82f4a2ca9287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:05.770407Z",
     "start_time": "2024-09-07T15:13:02.142922Z"
    },
    "id": "4a2f82f4a2ca9287"
   },
   "outputs": [],
   "source": [
    "from indox.data_connectors import GutenbergReader  # Import GutenbergReader to load books from Project Gutenberg\n",
    "\n",
    "# Initialize the GutenbergReader to access and fetch book content from Project Gutenberg\n",
    "reader = GutenbergReader()\n",
    "\n",
    "# Specify the Project Gutenberg book ID for \"Alice's Adventures in Wonderland\" (ID: 11)\n",
    "book_id = \"11\"\n",
    "\n",
    "# Fetch the content of the book using the get_content method, passing the book_id\n",
    "content = reader.get_content(book_id)\n",
    "\n",
    "# Now, 'content' contains the entire text of \"Alice's Adventures in Wonderland\" as fetched from Project Gutenberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10e7833ef0d026",
   "metadata": {
    "id": "ef10e7833ef0d026"
   },
   "source": [
    "## Splitting Text with SemanticTextSplitter\n",
    "\n",
    "In this section, we show how to split long text into smaller, semantically meaningful chunks using the **SemanticTextSplitter** from **Indox**. This is particularly useful for processing large texts (such as books or articles) into smaller units for tasks like retrieval, summarization, or further analysis.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize the SemanticTextSplitter**:\n",
    "   We will instantiate the `SemanticTextSplitter` with a specified chunk size (e.g., 400 tokens or characters).\n",
    "   \n",
    "2. **Split the Text**:\n",
    "   After fetching the text (from sources like Project Gutenberg), we will pass the content to `split_text()` to break it into smaller, semantically coherent chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4795ceae531b32b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:09.494758Z",
     "start_time": "2024-09-07T15:13:05.771416Z"
    },
    "id": "4795ceae531b32b4"
   },
   "outputs": [],
   "source": [
    "from indox.splitter import SemanticTextSplitter  # Import SemanticTextSplitter to split long text into chunks\n",
    "\n",
    "# Initialize the SemanticTextSplitter with a chunk size of 400 (tokens/characters)\n",
    "splitter = SemanticTextSplitter(400)\n",
    "\n",
    "# Split the book content into smaller, semantically meaningful chunks using the splitter\n",
    "content_chunks = splitter.split_text(content)\n",
    "\n",
    "# 'content_chunks' now contains the text broken into smaller segments, useful for retrieval or further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2bd8734a7e9bd46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:09.511466Z",
     "start_time": "2024-09-07T15:13:09.496765Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2bd8734a7e9bd46",
    "outputId": "13717b91-3c23-47e6-fdfc-6d0dc6da9f42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice was beginning to get very tired of sitting by her sister on the\\r\\nbank, and of having nothing to do: once or twice she had peeped into\\r\\nthe book her sister was reading, but it had no pictures or\\r\\nconversations in it, “and what is the use of a book,” thought Alice\\r\\n“without pictures or conversations?”\\r\\n\\r\\nSo she was considering in her own mind (as well as she could, for the\\r\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\r\\nmaking a daisy-chain would be worth the trouble of getting up and\\r\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\r\\nclose by her.\\r\\n\\r\\nThere was nothing so _very_ remarkable in that; nor did Alice think it\\r\\nso _very_ much out of the way to hear the Rabbit say to itself, “Oh\\r\\ndear! Oh dear! I shall be late!” (when she thought it over afterwards,\\r\\nit occurred to her that she ought to have wondered at this, but at the\\r\\ntime it all seemed quite natural); but when the Rabbit actually _took a\\r\\nwatch out of its waistcoat-pocket_, and looked at it, and then hurried\\r\\non, Alice started to her feet, for it flashed across her mind that she\\r\\nhad never before seen a rabbit with either a waistcoat-pocket, or a\\r\\nwatch to take out of it, and burning with curiosity, she ran across the\\r\\nfield after it, and fortunately was just in time to see it pop down a\\r\\nlarge rabbit-hole under the hedge.\\r\\n\\r\\nIn another moment down went Alice after it, never once considering how\\r\\nin the world she was to get out again.\\r\\n\\r\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\r\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\r\\nabout stopping herself before she found herself falling down a very\\r\\ndeep well.',\n",
       " 'Either the well was very deep, or she fell very slowly, for she had\\r\\nplenty of time as she went down to look about her and to wonder what\\r\\nwas going to happen next. First, she tried to look down and make out\\r\\nwhat she was coming to, but it was too dark to see anything; then she\\r\\nlooked at the sides of the well, and noticed that they were filled with\\r\\ncupboards and book-shelves; here and there she saw maps and pictures\\r\\nhung upon pegs. She took down a jar from one of the shelves as she\\r\\npassed; it was labelled “ORANGE MARMALADE”, but to her great\\r\\ndisappointment it was empty: she did not like to drop the jar for fear\\r\\nof killing somebody underneath, so managed to put it into one of the\\r\\ncupboards as she fell past it.\\r\\n\\r\\n“Well!” thought Alice to herself, “after such a fall as this, I shall\\r\\nthink nothing of tumbling down stairs! How brave they’ll all think me\\r\\nat home! Why, I wouldn’t say anything about it, even if I fell off the\\r\\ntop of the house!” (Which was very likely true.)',\n",
       " 'Down, down, down. Would the fall _never_ come to an end? “I wonder how\\r\\nmany miles I’ve fallen by this time?” she said aloud. “I must be\\r\\ngetting somewhere near the centre of the earth. Let me see: that would\\r\\nbe four thousand miles down, I think—” (for, you see, Alice had learnt\\r\\nseveral things of this sort in her lessons in the schoolroom, and\\r\\nthough this was not a _very_ good opportunity for showing off her\\r\\nknowledge, as there was no one to listen to her, still it was good\\r\\npractice to say it over) “—yes, that’s about the right distance—but\\r\\nthen I wonder what Latitude or Longitude I’ve got to?” (Alice had no\\r\\nidea what Latitude was, or Longitude either, but thought they were nice\\r\\ngrand words to say.)\\r\\n\\r\\nPresently she began again. “I wonder if I shall fall right _through_\\r\\nthe earth! How funny it’ll seem to come out among the people that walk\\r\\nwith their heads downward! The Antipathies, I think—” (she was rather\\r\\nglad there _was_ no one listening, this time, as it didn’t sound at all\\r\\nthe right word) “—but I shall have to ask them what the name of the\\r\\ncountry is, you know. Please, Ma’am, is this New Zealand or Australia?”\\r\\n(and she tried to curtsey as she spoke—fancy _curtseying_ as you’re\\r\\nfalling through the air! Do you think you could manage it?) “And what\\r\\nan ignorant little girl she’ll think me for asking! No, it’ll never do\\r\\nto ask: perhaps I shall see it written up somewhere.”']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_chunks[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d375184690646f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:14:42.339580Z",
     "start_time": "2024-09-07T15:13:39.166135Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d375184690646f4",
    "outputId": "d13870b3-2119-4cda-d386-ecccf4d327d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mStoring documents in the vector store\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: text-embedding-3-small\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:34:16,373 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:17,062 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:17,710 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:18,383 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:19,032 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:19,485 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:20,053 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:20,631 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:20,822 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:21,319 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:21,981 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:22,480 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:22,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:24,474 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:25,054 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:25,257 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:25,828 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:25,987 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:26,449 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:26,660 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:26,859 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:27,153 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:27,383 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:27,649 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:28,215 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:28,602 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:28,844 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:29,253 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:29,725 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:29,896 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:30,186 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:32,944 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:33,317 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:33,956 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:34,172 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:34,446 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:34,759 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:34,973 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:35,192 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:35,442 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:35,630 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:36,083 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:36,262 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:36,397 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:38,007 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:38,193 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:38,421 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:38,640 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:39,021 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:41,510 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:41,740 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:41,917 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:42,176 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:42,410 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:42,773 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:42,974 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:43,184 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:43,390 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:43,603 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:43,882 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:44,054 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:44,388 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:44,579 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:44,752 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:45,009 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:45,245 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:45,434 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:45,697 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:45,875 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:46,151 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:46,352 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:46,520 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:46,721 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:46,929 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:47,120 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:47,360 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:47,529 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:47,781 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:48,069 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:48,347 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:48,561 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:48,787 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:49,080 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:49,293 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:49,595 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:49,955 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:50,195 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:50,430 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:50,647 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:50,877 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:51,325 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:51,649 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:51,842 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:52,093 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:52,313 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:52,536 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:52,727 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:52,936 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:53,129 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:53,447 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:53,620 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:53,861 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:54,047 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:54,267 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:54,507 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:54,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:55,176 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:55,522 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:55,758 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:56,055 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:56,240 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:56,667 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:56,885 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:57,092 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:57,321 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:57,468 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:57,621 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:57,817 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:57,958 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:58,103 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:58,371 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:58,567 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 16:34:58,724 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocument added successfully to the vector store.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocuments stored successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db.add(docs=content_chunks) # Add chunks to vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7845dc60fb4dd",
   "metadata": {
    "id": "a1e7845dc60fb4dd"
   },
   "source": [
    "## Advanced Setup for the QuestionAnswer Retriever\n",
    "\n",
    "This section explains the additional hyperparameters available when configuring the **QuestionAnswer** retriever in **Indox**. The retriever uses a vector database and language model to fetch relevant content and provide answers to queries. We will cover the following hyperparameters:\n",
    "\n",
    "### Key Hyperparameters:\n",
    "\n",
    "- **llm**: The language model used to generate answers (e.g., OpenAI GPT models).\n",
    "- **vector_database**: The vector database used for embedding retrieval (e.g., Chroma).\n",
    "- **top_k**: Determines how many of the most relevant documents to retrieve (default is 5).\n",
    "- **document_relevancy_filter**: If set to `True`, only the most relevant documents are retrieved based on relevancy filtering.\n",
    "- **generate_clustered_prompts**: If set to `True`, this enables the retriever to cluster the retrieved documents and generate summaries for each cluster. The summary is added to the retrieval context, helping improve the overall response quality by providing a more structured context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "767293552560adcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:16:03.812077Z",
     "start_time": "2024-09-07T15:16:03.808632Z"
    },
    "id": "767293552560adcf"
   },
   "outputs": [],
   "source": [
    "# Import the QuestionAnswer retriever from Indox\n",
    "# This version includes all available hyperparameters with explanations\n",
    "\n",
    "retriever = indox.QuestionAnswer(\n",
    "    llm=openai_llm,  # Language model to generate answers (e.g., OpenAI GPT-4)\n",
    "    vector_database=db,  # Chroma vector store for embedding retrieval\n",
    "    top_k=3,  # Number of top relevant documents to retrieve, default is 3\n",
    "    document_relevancy_filter=False,  # Set to True to filter results based on document relevancy\n",
    "    generate_clustered_prompts=False  # Set to True to enable clustering of retrieval results and generate summaries for each cluster\n",
    ")\n",
    "\n",
    "# If 'generate_clustered_prompts' is True:\n",
    "# - The retriever will group the retrieved documents into clusters.\n",
    "# - It will summarize each cluster and add the summary to the initial retrieval context,\n",
    "#   which can help the language model produce a more comprehensive and coherent answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d7c943139564ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:16:25.020023Z",
     "start_time": "2024-09-07T15:16:20.412055Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "a3d7c943139564ae",
    "outputId": "56195864-af5b-4b6a-a147-ced41cfae6bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving context and scores from the vector database\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: text-embedding-3-small\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:35:09,203 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating answer without document relevancy filter\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating response\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:35:10,438 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mResponse generated successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mQuery answered successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The speaker is talking to the Caterpillar. In the context provided, Alice is engaged in a conversation with the Caterpillar, responding to its questions and remarks about her identity and experiences of change.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Who is the speaker talking to in the text?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5755848d3e313a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:18:22.891580Z",
     "start_time": "2024-09-07T15:18:19.287157Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "5755848d3e313a07",
    "outputId": "a700c941-0c9f-4a68-e231-7450d6a3bbf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving context and scores from the vector database\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: text-embedding-3-small\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:35:14,548 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating answer without document relevancy filter\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating response\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:35:15,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mResponse generated successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mQuery answered successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Alice's sister's book did not have any pictures or conversations in it, which made Alice feel bored and disinterested. She thought to herself, “and what is the use of a book, without pictures or conversations?” This lack of engaging content contributed to Alice's feelings of tiredness and her desire for something more stimulating to do while sitting by her sister on the bank.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"tell me about alice's sister book?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f3f85d28d99d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:19:15.946343Z",
     "start_time": "2024-09-07T15:19:11.696496Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "d4f3f85d28d99d14",
    "outputId": "9f592e6a-6a70-4659-9375-dc68285dae90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving context and scores from the vector database\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: text-embedding-3-small\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:35:20,312 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating answer without document relevancy filter\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating response\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:35:22,567 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mResponse generated successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mQuery answered successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The story of Alice in Wonderland does not have a definitive ending in the traditional sense, as it is more of a series of whimsical adventures rather than a linear narrative. However, the story concludes with Alice waking up from her dream. After her many encounters and experiences in Wonderland, she finds herself back on the bank where she initially fell asleep. She recounts her adventures to her sister, who reflects on the curious nature of Alice's dream. The story ends with Alice's sister pondering the imaginative world Alice has experienced, suggesting that the adventures may continue in Alice's mind as she grows older.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"how alice story ends?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8dfd6cfbec4f50c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:19:44.781911Z",
     "start_time": "2024-09-07T15:19:44.777999Z"
    },
    "id": "c8dfd6cfbec4f50c"
   },
   "outputs": [],
   "source": [
    "chat_history = retriever.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b38c8c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'query': 'Who is the speaker talking to in the text?',\n",
       "  'llm_response': 'The speaker is talking to the Caterpillar. In the context provided, Alice is engaged in a conversation with the Caterpillar, responding to its questions and remarks about her identity and experiences of change.',\n",
       "  'retrieval_context': ['“’Tis the voice of the Lobster; I heard him declare,\\r\\n“You have baked me too brown, I must sugar my hair.”\\r\\nAs a duck with its eyelids, so he with his nose\\r\\nTrims his belt and his buttons, and turns out his toes.”\\r\\n\\r\\n[later editions continued as follows\\r\\nWhen the sands are all dry, he is gay as a lark,\\r\\nAnd will talk in contemptuous tones of the Shark,\\r\\nBut, when the tide rises and sharks are around,\\r\\nHis voice has a timid and tremulous sound.]',\n",
       "   'The Caterpillar and Alice looked at each other for some time in\\r\\nsilence: at last the Caterpillar took the hookah out of its mouth, and\\r\\naddressed her in a languid, sleepy voice.\\r\\n\\r\\n“Who are _you?_” said the Caterpillar.\\r\\n\\r\\nThis was not an encouraging opening for a conversation. Alice replied,\\r\\nrather shyly, “I—I hardly know, sir, just at present—at least I know\\r\\nwho I _was_ when I got up this morning, but I think I must have been\\r\\nchanged several times since then.”\\r\\n\\r\\n“What do you mean by that?” said the Caterpillar sternly. “Explain\\r\\nyourself!”\\r\\n\\r\\n“I can’t explain _myself_, I’m afraid, sir,” said Alice, “because I’m\\r\\nnot myself, you see.”\\r\\n\\r\\n“I don’t see,” said the Caterpillar.\\r\\n\\r\\n“I’m afraid I can’t put it more clearly,” Alice replied very politely,\\r\\n“for I can’t understand it myself to begin with; and being so many\\r\\ndifferent sizes in a day is very confusing.”\\r\\n\\r\\n“It isn’t,” said the Caterpillar.\\r\\n\\r\\n“Well, perhaps you haven’t found it so yet,” said Alice; “but when you\\r\\nhave to turn into a chrysalis—you will some day, you know—and then\\r\\nafter that into a butterfly, I should think you’ll feel it a little\\r\\nqueer, won’t you?”\\r\\n\\r\\n“Not a bit,” said the Caterpillar.\\r\\n\\r\\n“Well, perhaps your feelings may be different,” said Alice; “all I know\\r\\nis, it would feel very queer to _me_.”\\r\\n\\r\\n“You!” said the Caterpillar contemptuously. “Who are _you?_”',\n",
       "   'Which brought them back again to the beginning of the conversation.\\r\\nAlice felt a little irritated at the Caterpillar’s making such _very_\\r\\nshort remarks, and she drew herself up and said, very gravely, “I\\r\\nthink, you ought to tell me who _you_ are, first.”\\r\\n\\r\\n“Why?” said the Caterpillar.\\r\\n\\r\\nHere was another puzzling question; and as Alice could not think of any\\r\\ngood reason, and as the Caterpillar seemed to be in a _very_ unpleasant\\r\\nstate of mind, she turned away.\\r\\n\\r\\n“Come back!” the Caterpillar called after her. “I’ve something\\r\\nimportant to say!”\\r\\n\\r\\nThis sounded promising, certainly: Alice turned and came back again.\\r\\n\\r\\n“Keep your temper,” said the Caterpillar.\\r\\n\\r\\n“Is that all?” said Alice, swallowing down her anger as well as she\\r\\ncould.\\r\\n\\r\\n“No,” said the Caterpillar.\\r\\n\\r\\nAlice thought she might as well wait, as she had nothing else to do,\\r\\nand perhaps after all it might tell her something worth hearing. For\\r\\nsome minutes it puffed away without speaking, but at last it unfolded\\r\\nits arms, took the hookah out of its mouth again, and said, “So you\\r\\nthink you’re changed, do you?”\\r\\n\\r\\n“I’m afraid I am, sir,” said Alice; “I can’t remember things as I\\r\\nused—and I don’t keep the same size for ten minutes together!”\\r\\n\\r\\n“Can’t remember _what_ things?” said the Caterpillar.\\r\\n\\r\\n“Well, I’ve tried to say “How doth the little busy bee,” but it all\\r\\ncame different!” Alice replied in a very melancholy voice.\\r\\n\\r\\n“Repeat, “_You are old, Father William_,’” said the Caterpillar.\\r\\n\\r\\nAlice folded her hands, and began:—']},\n",
       " 1: {'query': \"tell me about alice's sister book?\",\n",
       "  'llm_response': \"Alice's sister's book did not have any pictures or conversations in it, which made Alice feel bored and disinterested. She thought to herself, “and what is the use of a book, without pictures or conversations?” This lack of engaging content contributed to Alice's feelings of tiredness and her desire for something more stimulating to do while sitting by her sister on the bank.\",\n",
       "  'retrieval_context': ['But her sister sat still just as she left her, leaning her head on her\\r\\nhand, watching the setting sun, and thinking of little Alice and all\\r\\nher wonderful Adventures, till she too began dreaming after a fashion,\\r\\nand this was her dream:—\\r\\n\\r\\nFirst, she dreamed of little Alice herself, and once again the tiny\\r\\nhands were clasped upon her knee, and the bright eager eyes were\\r\\nlooking up into hers—she could hear the very tones of her voice, and\\r\\nsee that queer little toss of her head to keep back the wandering hair\\r\\nthat _would_ always get into her eyes—and still as she listened, or\\r\\nseemed to listen, the whole place around her became alive with the\\r\\nstrange creatures of her little sister’s dream.\\r\\n\\r\\nThe long grass rustled at her feet as the White Rabbit hurried by—the\\r\\nfrightened Mouse splashed his way through the neighbouring pool—she\\r\\ncould hear the rattle of the teacups as the March Hare and his friends\\r\\nshared their never-ending meal, and the shrill voice of the Queen\\r\\nordering off her unfortunate guests to execution—once more the pig-baby\\r\\nwas sneezing on the Duchess’s knee, while plates and dishes crashed\\r\\naround it—once more the shriek of the Gryphon, the squeaking of the\\r\\nLizard’s slate-pencil, and the choking of the suppressed guinea-pigs,\\r\\nfilled the air, mixed up with the distant sobs of the miserable Mock\\r\\nTurtle.',\n",
       "   'Alice was beginning to get very tired of sitting by her sister on the\\r\\nbank, and of having nothing to do: once or twice she had peeped into\\r\\nthe book her sister was reading, but it had no pictures or\\r\\nconversations in it, “and what is the use of a book,” thought Alice\\r\\n“without pictures or conversations?”\\r\\n\\r\\nSo she was considering in her own mind (as well as she could, for the\\r\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\r\\nmaking a daisy-chain would be worth the trouble of getting up and\\r\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\r\\nclose by her.\\r\\n\\r\\nThere was nothing so _very_ remarkable in that; nor did Alice think it\\r\\nso _very_ much out of the way to hear the Rabbit say to itself, “Oh\\r\\ndear! Oh dear! I shall be late!” (when she thought it over afterwards,\\r\\nit occurred to her that she ought to have wondered at this, but at the\\r\\ntime it all seemed quite natural); but when the Rabbit actually _took a\\r\\nwatch out of its waistcoat-pocket_, and looked at it, and then hurried\\r\\non, Alice started to her feet, for it flashed across her mind that she\\r\\nhad never before seen a rabbit with either a waistcoat-pocket, or a\\r\\nwatch to take out of it, and burning with curiosity, she ran across the\\r\\nfield after it, and fortunately was just in time to see it pop down a\\r\\nlarge rabbit-hole under the hedge.\\r\\n\\r\\nIn another moment down went Alice after it, never once considering how\\r\\nin the world she was to get out again.\\r\\n\\r\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\r\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\r\\nabout stopping herself before she found herself falling down a very\\r\\ndeep well.',\n",
       "   '“Wake up, Alice dear!” said her sister; “Why, what a long sleep you’ve\\r\\nhad!”\\r\\n\\r\\n“Oh, I’ve had such a curious dream!” said Alice, and she told her\\r\\nsister, as well as she could remember them, all these strange\\r\\nAdventures of hers that you have just been reading about; and when she\\r\\nhad finished, her sister kissed her, and said, “It _was_ a curious\\r\\ndream, dear, certainly: but now run in to your tea; it’s getting late.”\\r\\nSo Alice got up and ran off, thinking while she ran, as well she might,\\r\\nwhat a wonderful dream it had been.']},\n",
       " 2: {'query': 'how alice story ends?',\n",
       "  'llm_response': \"The story of Alice in Wonderland does not have a definitive ending in the traditional sense, as it is more of a series of whimsical adventures rather than a linear narrative. However, the story concludes with Alice waking up from her dream. After her many encounters and experiences in Wonderland, she finds herself back on the bank where she initially fell asleep. She recounts her adventures to her sister, who reflects on the curious nature of Alice's dream. The story ends with Alice's sister pondering the imaginative world Alice has experienced, suggesting that the adventures may continue in Alice's mind as she grows older.\",\n",
       "  'retrieval_context': ['Alice was not a bit hurt, and she jumped up on to her feet in a moment:\\r\\nshe looked up, but it was all dark overhead; before her was another\\r\\nlong passage, and the White Rabbit was still in sight, hurrying down\\r\\nit. There was not a moment to be lost: away went Alice like the wind,\\r\\nand was just in time to hear it say, as it turned a corner, “Oh my ears\\r\\nand whiskers, how late it’s getting!” She was close behind it when she\\r\\nturned the corner, but the Rabbit was no longer to be seen: she found\\r\\nherself in a long, low hall, which was lit up by a row of lamps hanging\\r\\nfrom the roof.\\r\\n\\r\\nThere were doors all round the hall, but they were all locked; and when\\r\\nAlice had been all the way down one side and up the other, trying every\\r\\ndoor, she walked sadly down the middle, wondering how she was ever to\\r\\nget out again.\\r\\n\\r\\nSuddenly she came upon a little three-legged table, all made of solid\\r\\nglass; there was nothing on it except a tiny golden key, and Alice’s\\r\\nfirst thought was that it might belong to one of the doors of the hall;\\r\\nbut, alas! either the locks were too large, or the key was too small,\\r\\nbut at any rate it would not open any of them. However, on the second\\r\\ntime round, she came upon a low curtain she had not noticed before, and\\r\\nbehind it was a little door about fifteen inches high: she tried the\\r\\nlittle golden key in the lock, and to her great delight it fitted!',\n",
       "   'Alice thought she might as well go back, and see how the game was going\\r\\non, as she heard the Queen’s voice in the distance, screaming with\\r\\npassion. She had already heard her sentence three of the players to be\\r\\nexecuted for having missed their turns, and she did not like the look\\r\\nof things at all, as the game was in such confusion that she never knew\\r\\nwhether it was her turn or not. So she went in search of her hedgehog.\\r\\n\\r\\nThe hedgehog was engaged in a fight with another hedgehog, which seemed\\r\\nto Alice an excellent opportunity for croqueting one of them with the\\r\\nother: the only difficulty was, that her flamingo was gone across to\\r\\nthe other side of the garden, where Alice could see it trying in a\\r\\nhelpless sort of way to fly up into a tree.\\r\\n\\r\\nBy the time she had caught the flamingo and brought it back, the fight\\r\\nwas over, and both the hedgehogs were out of sight: “but it doesn’t\\r\\nmatter much,” thought Alice, “as all the arches are gone from this side\\r\\nof the ground.” So she tucked it away under her arm, that it might not\\r\\nescape again, and went back for a little more conversation with her\\r\\nfriend.\\r\\n\\r\\nWhen she got back to the Cheshire Cat, she was surprised to find quite\\r\\na large crowd collected round it: there was a dispute going on between\\r\\nthe executioner, the King, and the Queen, who were all talking at once,\\r\\nwhile all the rest were quite silent, and looked very uncomfortable.\\r\\n\\r\\nThe moment Alice appeared, she was appealed to by all three to settle\\r\\nthe question, and they repeated their arguments to her, though, as they\\r\\nall spoke at once, she found it very hard indeed to make out exactly\\r\\nwhat they said.',\n",
       "   'Alas! it was too late to wish that! She went on growing, and growing,\\r\\nand very soon had to kneel down on the floor: in another minute there\\r\\nwas not even room for this, and she tried the effect of lying down with\\r\\none elbow against the door, and the other arm curled round her head.\\r\\nStill she went on growing, and, as a last resource, she put one arm out\\r\\nof the window, and one foot up the chimney, and said to herself “Now I\\r\\ncan do no more, whatever happens. What _will_ become of me?”\\r\\n\\r\\nLuckily for Alice, the little magic bottle had now had its full effect,\\r\\nand she grew no larger: still it was very uncomfortable, and, as there\\r\\nseemed to be no sort of chance of her ever getting out of the room\\r\\nagain, no wonder she felt unhappy.\\r\\n\\r\\n“It was much pleasanter at home,” thought poor Alice, “when one wasn’t\\r\\nalways growing larger and smaller, and being ordered about by mice and\\r\\nrabbits. I almost wish I hadn’t gone down that rabbit-hole—and yet—and\\r\\nyet—it’s rather curious, you know, this sort of life! I do wonder what\\r\\n_can_ have happened to me! When I used to read fairy-tales, I fancied\\r\\nthat kind of thing never happened, and now here I am in the middle of\\r\\none! There ought to be a book written about me, that there ought! And\\r\\nwhen I grow up, I’ll write one—but I’m grown up now,” she added in a\\r\\nsorrowful tone; “at least there’s no room to grow up any more _here_.”']}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dacd67cfe53dac",
   "metadata": {
    "id": "48dacd67cfe53dac"
   },
   "source": [
    "## Setting Up the RagEvaluator with OpenAI LLM as Judge\n",
    "\n",
    "In this section, we demonstrate how to set up the **RagEvaluator** from **IndoxJudge**, which evaluates the quality of responses generated by a language model in a Retrieval-Augmented Generation (RAG) system. We use **OpenAI** as the language model (LLM) that acts as a judge for the evaluation process.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize the OpenAI Model**:\n",
    "   We first create an instance of `OpenAi` with an API key and a specified model (e.g., `\"gpt-4o-mini\"`).\n",
    "   \n",
    "2. **Set Up the RagEvaluator**:\n",
    "   The `RagEvaluator` will use the LLM to evaluate the quality of the conversation entries (represented by `chat_history`), judging their accuracy and relevancy.\n",
    "   \n",
    "### Key Parameters:\n",
    "- **llm_as_judge**: The language model acting as the judge, responsible for evaluating the retrieved content.\n",
    "- **entries**: The conversation or content history to be evaluated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e894ae02427a1aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:18:22.909887Z",
     "start_time": "2024-09-07T15:18:22.892586Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e894ae02427a1aa",
    "outputId": "5f734027-9b59-4e5e-a3ee-c3f958f79c5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:36:15,824 - matplotlib.font_manager - INFO - generated new fontManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing OpenAi with model: gpt-4o-mini and max_tokens: 2048\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxJudge.pipelines import RagEvaluator  # Import RagEvaluator for evaluating the quality of RAG systems\n",
    "from indoxJudge.models import OpenAi  # Import OpenAi model from IndoxJudge to use it as a judge\n",
    "\n",
    "# Initialize the OpenAI model with API key and model name (gpt-4o-mini)\n",
    "model = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create a RagEvaluator to evaluate the conversation history\n",
    "# - llm_as_judge: The OpenAI model used to evaluate the responses\n",
    "# - entries: The conversation history or entries to be judged\n",
    "evaluator = RagEvaluator(llm_as_judge=model, entries=chat_history)\n",
    "\n",
    "# Now, the RagEvaluator is set to evaluate the provided chat history based on the responses retrieved from the RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53963dfb24b32b7b",
   "metadata": {
    "id": "53963dfb24b32b7b"
   },
   "source": [
    "## Reviewing Evaluation Results with RagEvaluator\n",
    "\n",
    "After running the `judge()` method, the **RagEvaluator** provides detailed feedback on the quality of the responses. In this section, we explore how to access and visualize the evaluation results and metric scores.\n",
    "\n",
    "### Key Methods and Attributes:\n",
    "\n",
    "1. **evaluator.results**:\n",
    "   This attribute returns the detailed results and verdicts for each evaluation metric applied to the conversation or content entries. You can inspect the specific feedback for each metric (e.g., relevance, accuracy).\n",
    "\n",
    "2. **evaluator.metrics_score**:\n",
    "   This attribute provides the numeric scores for each metric used during the evaluation process, giving you a clear understanding of the performance of the retrieved responses.\n",
    "\n",
    "3. **evaluator.plot()**:\n",
    "   This method generates a visualization (using Matplotlib) of the evaluation scores for each metric. This chart helps you visually interpret the strengths and weaknesses of the model’s responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d53caa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\llmserver\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\llmserver\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c0d471c927bc165",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c0d471c927bc165",
    "outputId": "2e499b09-ea7a-4ecb-a894-f0c6954a222c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mModel set for all metrics.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mRagEvaluator initialized with model and metrics.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating entry: 0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Faithfulness\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:11,374 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 223 | Output: 67\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:13,731 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 188 | Output: 70\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:15,591 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 1534 | Output: 60\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:16,393 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 235 | Output: 34\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 2180 | Total Output: 231 | Total: 2411\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Faithfulness, score: 1.44\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: AnswerRelevancy\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:17,589 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 164 | Output: 54\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:18,741 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 445 | Output: 60\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:19,736 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 199 | Output: 31\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 808 | Total Output: 145 | Total: 953\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: AnswerRelevancy, score: 1.44\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: ContextualRelevancy\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:20,839 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 406 | Output: 59\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:22,039 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 668 | Output: 57\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:23,474 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 691 | Output: 48\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:24,517 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 406 | Output: 55\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:25,701 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 668 | Output: 61\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:26,675 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 691 | Output: 52\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:28,964 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 257 | Output: 61\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 3787 | Total Output: 393 | Total: 4180\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: ContextualRelevancy, score: 0.97\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: GEval\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:31,919 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 270 | Output: 100\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:33,849 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 1267 | Output: 58\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1537 | Total Output: 158 | Total: 1695\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: GEval, score: 1.3\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Hallucination\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:35,846 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 1352 | Output: 129\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:36,737 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 210 | Output: 34\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1562 | Total Output: 163 | Total: 1725\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Hallucination, score: 0.0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: KnowledgeRetention\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:37,246 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 313 | Output: 17\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:38,238 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 527 | Output: 48\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:39,193 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 177 | Output: 45\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1017 | Total Output: 110 | Total: 1127\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: KnowledgeRetention, score: 0.0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: BertScore\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1m\n",
      "Completed evaluation for metric: BertScore, scores: \n",
      "precision: 0.84,\n",
      "recall: 0.64,\n",
      "f1_score: 0.72,\n",
      "                        \u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: METEOR\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: METEOR, score: 0.79\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for entry: 0\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluation Completed, Check out the results\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating entry: 1\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Faithfulness\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:47,362 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 259 | Output: 99\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:49,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 224 | Output: 100\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:50,956 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 1509 | Output: 72\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:51,719 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 235 | Output: 34\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 2227 | Total Output: 305 | Total: 2532\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Faithfulness, score: 1.48\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: AnswerRelevancy\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:53,844 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 200 | Output: 81\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:54,999 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 470 | Output: 73\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:56,485 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 197 | Output: 31\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 867 | Total Output: 185 | Total: 1052\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: AnswerRelevancy, score: 1.48\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: ContextualRelevancy\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:57,489 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 594 | Output: 52\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:58,799 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 685 | Output: 83\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:43:59,788 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 424 | Output: 59\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:00,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 594 | Output: 52\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:01,952 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 685 | Output: 66\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:02,912 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 424 | Output: 59\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:03,841 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 205 | Output: 34\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 3611 | Total Output: 405 | Total: 4016\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: ContextualRelevancy, score: 1.32\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: GEval\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:06,331 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 270 | Output: 95\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:07,857 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 1229 | Output: 61\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1499 | Total Output: 156 | Total: 1655\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: GEval, score: 0.8300000000000001\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Hallucination\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:10,714 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 1322 | Output: 185\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:12,109 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 266 | Output: 67\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1588 | Total Output: 252 | Total: 1840\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Hallucination, score: 1.0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: KnowledgeRetention\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:12,613 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 311 | Output: 11\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:14,078 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 558 | Output: 66\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:15,585 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 195 | Output: 54\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1064 | Total Output: 131 | Total: 1195\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: KnowledgeRetention, score: 0.0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: BertScore\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1m\n",
      "Completed evaluation for metric: BertScore, scores: \n",
      "precision: 0.86,\n",
      "recall: 0.71,\n",
      "f1_score: 0.77,\n",
      "                        \u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: METEOR\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: METEOR, score: 1.05\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for entry: 1\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluation Completed, Check out the results\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating entry: 2\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Faithfulness\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:18,960 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 303 | Output: 133\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:21,468 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 268 | Output: 125\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:25,247 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 1777 | Output: 170\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:26,598 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 287 | Output: 52\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 2635 | Total Output: 480 | Total: 3115\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Faithfulness, score: 0.61\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: AnswerRelevancy\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:28,680 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 244 | Output: 136\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:30,383 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 511 | Output: 109\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:31,286 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 194 | Output: 32\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 949 | Total Output: 277 | Total: 1226\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: AnswerRelevancy, score: 1.49\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: ContextualRelevancy\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:32,215 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 624 | Output: 45\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:33,321 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 669 | Output: 57\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:34,595 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 635 | Output: 57\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:35,554 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 624 | Output: 50\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:36,576 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 669 | Output: 62\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:37,646 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 635 | Output: 55\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:38,737 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 202 | Output: 33\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 4058 | Total Output: 359 | Total: 4417\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: ContextualRelevancy, score: 1.44\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: GEval\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:41,091 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 270 | Output: 93\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:42,574 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 1500 | Output: 56\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1770 | Total Output: 149 | Total: 1919\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: GEval, score: 1.08\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Hallucination\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:45,330 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 1597 | Output: 173\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:46,294 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 254 | Output: 50\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1851 | Total Output: 223 | Total: 2074\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Hallucination, score: 1.33\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: KnowledgeRetention\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:46,812 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 308 | Output: 14\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:48,368 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 606 | Output: 55\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:44:49,327 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Counts - Input: 184 | Output: 53\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mToken Usage Summary:\n",
      " Total Input: 1098 | Total Output: 122 | Total: 1220\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: KnowledgeRetention, score: 0.0\u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: BertScore\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1m\n",
      "Completed evaluation for metric: BertScore, scores: \n",
      "precision: 0.8400000000000001,\n",
      "recall: 0.72,\n",
      "f1_score: 0.77,\n",
      "                        \u001b[0m\n",
      "\n",
      "==================================================\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: METEOR\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: METEOR, score: 1.1099999999999999\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for entry: 2\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluation Completed, Check out the results\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluator.judge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a22ddde569b43f35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a22ddde569b43f35",
    "outputId": "81185424-2584-4768-94f3-a58d2f3a0daa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Faithfulness': 0.2,\n",
       " 'AnswerRelevancy': 0.5,\n",
       " 'ContextualRelevancy': 0.48,\n",
       " 'GEval': 0.36,\n",
       " 'Hallucination': 0.44,\n",
       " 'KnowledgeRetention': 0.0,\n",
       " 'precision': 0.28,\n",
       " 'recall': 0.24,\n",
       " 'f1_score': 0.26,\n",
       " 'METEOR': 0.37,\n",
       " 'evaluation_score': 0.36}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.metrics_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "709f1cc49ac45a35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "709f1cc49ac45a35",
    "outputId": "8a385863-6b51-4449-bb79-0d27381c5e1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c88a7ad090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator.plot(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fae30",
   "metadata": {},
   "source": [
    "\n",
    "## Join Us\n",
    "\n",
    "Join us in exploring how Indox can revolutionize your document processing workflow, bringing clarity and organization to your data retrieval needs. Connect with us and become part of our growing community through the platforms below:\n",
    "\n",
    "## Community\n",
    "\n",
    "- [Discord](https://discord.com/invite/xGz5tQYaeq)\n",
    "- [X (Twitter)](https://x.com/osllmai)\n",
    "- [LinkedIn](https://www.linkedin.com/company/osllmai/)\n",
    "- [YouTube](https://www.youtube.com/@osllm-rb9pr)\n",
    "- [Telegram](https://t.me/osllmai)\n",
    "\n",
    "\n",
    "*Reviewed by: Ali Nemati - March, 23, 2025*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "indox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
